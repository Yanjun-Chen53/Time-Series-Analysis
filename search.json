[
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "Data Visulization",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(tidyverse) \nlibrary(dplyr)"
  },
  {
    "objectID": "visualization.html#long-run-evolution-of-power-generation-mix",
    "href": "visualization.html#long-run-evolution-of-power-generation-mix",
    "title": "Data Visulization",
    "section": "Long-Run Evolution of Power Generation Mix",
    "text": "Long-Run Evolution of Power Generation Mix\nThe data, sourced from the International Energy Agency (IEA), highlights a clear structural shift in the global energy system. While fossil fuels and hydro power historically dominated capacity additions, the past two decades have been characterized by the rapid acceleration of solar and wind energy. In particular, solar capacity has exhibited exponential growth since the early 2010s, outpacing all other sources, while wind energy has demonstrated steady, sustained expansion. By contrast, hydro capacity additions have remained relatively stable, reflecting geographical and environmental constraints. Nuclear additions appear limited in scale, indicating stagnation compared to renewables. Overall, the figure underscores the transition towards low-carbon energy systems, with solar and wind emerging as the primary drivers of global renewable capacity growth.\n\n\nCode\ndata &lt;- read.csv(\"data/renewable-capacity.csv\")\np &lt;- ggplot(data, aes(x = Year, y = Capacity, color = Entity)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Global Renewable Energy Capacity by Source\",\n    subtitle = \"Comparison across Solar, Wind, Hydro, etc.\",\n    x = \"Year\",\n    y = \"Capacity (GW)\",\n    color = \"Energy Source\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\ninteractive_plot &lt;- ggplotly(p, tooltip = c(\"x\", \"y\", \"Entity\"))\ninteractive_plot\n\n\n\n\n\n\n\n\nCode\ndf &lt;- read.csv(\"data/electric.csv\")\ndf &lt;- df %&gt;%\n  mutate(\n    Description = str_extract(Description, \"(?&lt;=From ).*?(?=,)\"),\n    YYYYMM = as.character(YYYYMM),\n    Year = as.integer(substr(YYYYMM, 1, 4)),\n    Month = as.integer(substr(YYYYMM, 5, 6))\n  ) %&gt;%\n  filter(Month &lt;= 12) %&gt;%\n  filter(Year &gt;= 2000) %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))\n\ndf_clean &lt;- df %&gt;%\n  mutate(\n    source = case_when(\n      Description == \"Coal\" ~ \"coal\",\n      Description == \"Natural Gas\" ~ \"natural_gas\",\n      Description == \"Conventional Hydroelectric Power\" ~ \"hydro\",\n      Description == \"Wind\" ~ \"wind\",\n      Description == \"Solar\" ~ \"solar\",\n      Description == \"Nuclear Electric Power\" ~ \"nuclear\",\n      TRUE ~ NA_character_\n    )\n  ) %&gt;%\n  filter(!is.na(source))\n\ndf_wide &lt;- df_clean %&gt;%\n  select(Date, source, Value) %&gt;%\n  pivot_wider(names_from = source, values_from = Value)\n\ndf_wide &lt;- df_wide %&gt;%\n  arrange(Date)\n\ndf_wide_clean &lt;- df_wide %&gt;%\n  mutate(\n    Year = year(Date),\n    Month = month(Date),\n    coal = parse_number(coal),\n    natural_gas = parse_number(natural_gas),\n    nuclear = parse_number(nuclear),\n    hydro = parse_number(hydro),\n    solar = parse_number(solar),\n    wind = parse_number(wind)\n  )\n\n\n\n\nCode\nlibrary(tidyr)\n\ndf_long &lt;- df_wide_clean %&gt;%\n  pivot_longer(\n    cols = coal:wind,\n    names_to = \"Energy\",\n    values_to = \"Value\"\n  )\n\nlibrary(ggplot2)\nlibrary(scales)\n\ndf_yearly &lt;- df_long %&gt;%\n  group_by(Year, Energy) %&gt;%\n  summarise(Value = sum(Value, na.rm = TRUE), .groups = \"drop\")\ndf_yearly &lt;- df_yearly %&gt;% filter(Year &lt; 2025)\nggplot(df_yearly, aes(x = Year, y = Value, color = Energy)) +\n  geom_line(linewidth = 1.2) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    title = \"Long-Run Evolution of U.S. Electricity Generation\",\n    subtitle = \"Annual generation by energy source (Million kWh)\",\n    x = \"Year\",\n    y = \"Generation (Million kWh)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_share &lt;- df_yearly %&gt;%\n  group_by(Year) %&gt;%\n  mutate(Share = Value / sum(Value))\n\nggplot(df_share, aes(x = Year, y = Share, fill = Energy)) +\n  geom_area(alpha = 0.9) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    title = \"Electricity Generation Mix Over Time\",\n    subtitle = \"Share of total U.S. electricity generation\",\n    x = \"Year\",\n    y = \"Share\"\n  ) +\n  theme_minimal(base_size = 14)"
  },
  {
    "objectID": "visualization.html#seasonality-climate-drivers",
    "href": "visualization.html#seasonality-climate-drivers",
    "title": "Data Visulization",
    "section": "Seasonality & Climate Drivers",
    "text": "Seasonality & Climate Drivers\nThe graph shows a positive correlation between wind power generation and temperature anomalies, due to the long-term upward trend of both. As global warming intensifies, temperature anomalies are gradually increasing; simultaneously, wind power generation is also showing a continuous upward trend due to renewable energy policies and sustained growth in installed capacity. Therefore, this correlation primarily reflects a shared temporal trend and should not be interpreted as a direct driver of wind power output by temperature anomalies.\n\n\nCode\ndf &lt;- read.csv(\"data/Global_Temperature_2000onwards.csv\")\ndf &lt;- df[order(df$Year, df$Month), ]\ndf_temp &lt;- df %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))\n\nlibrary(dplyr)\nlibrary(tidyr)\n\nselected_energies &lt;- c(\"coal\", \"natural_gas\", \"solar\", \"wind\")\n\ndf_energy &lt;- df_wide_clean %&gt;%\n  select(Date, all_of(selected_energies))\n\ndf_combined &lt;- df_energy %&gt;%\n  left_join(df_temp %&gt;% select(Date, Monthly_Anomaly), by = \"Date\")\n\ndf_long &lt;- df_combined %&gt;%\n  pivot_longer(cols = all_of(selected_energies),\n               names_to = \"Energy\",\n               values_to = \"Generation\")\n\nggplot(df_combined, aes(x = Monthly_Anomaly, y = solar)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Solar Generation vs Temperature Anomaly\",\n    x = \"Temperature Anomaly (°C)\",\n    y = \"Solar Generation (Million kWh)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df_combined, aes(x = Monthly_Anomaly, y = wind)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Wind Generation vs Temperature Anomaly\",\n    x = \"Temperature Anomaly (°C)\",\n    y = \"Wind Generation (Million kWh)\"\n  ) +\n  theme_minimal(base_size = 14)"
  },
  {
    "objectID": "visualization.html#linkages-and-causality-in-energy-prices",
    "href": "visualization.html#linkages-and-causality-in-energy-prices",
    "title": "Data Visulization",
    "section": "Linkages and Causality in Energy Prices",
    "text": "Linkages and Causality in Energy Prices\nFrom the figure, one can observe several key patterns. Periods of sharp volatility in oil prices—such as the downturn around 2008 and the collapse in early 2020—are mirrored by fluctuations in energy-sector equities, though the magnitude and persistence of these responses differ across renewable and traditional ETFs. Notably, while XLE tends to correlate more closely with oil price movements, ICLN demonstrates partial decoupling, reflecting the structural shift toward renewable energy sources whose valuation drivers are less directly dependent on crude oil prices.\n\n\nCode\nwti &lt;- read.csv(\"data/WTIprice.csv\")\n\nwti &lt;- wti %&gt;%\n  rename(date = observation_date) %&gt;%\n  mutate(date = as.Date(date)) %&gt;%\n  rename(WTI = POILWTIUSDM) %&gt;%\n  filter(!is.na(WTI))\n\ndf_n &lt;- read.csv(\"data/naturalgasprice.csv\")\ndf_n &lt;- df_n %&gt;%\n  mutate(\n    YYYYMM = as.character(YYYYMM),\n    Year = as.integer(substr(YYYYMM, 1, 4)),\n    Month = as.integer(substr(YYYYMM, 5, 6))\n  ) %&gt;%\n  filter(Month &lt;= 12) %&gt;%\n  filter(Year &gt;= 2000) %&gt;%\n  mutate(Value = as.numeric(Value)) %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))%&gt;%\n  arrange(Date)\n\nfig &lt;- plot_ly() %&gt;%\n  \n  add_lines(data = wti, x = ~date, y = ~WTI,\n            name = \"WTI Crude Oil\", yaxis = \"y1\",\n            line = list(color = \"green\")) %&gt;%\n  \n  add_lines(data = df_n, x = ~Date, y = ~Value,\n            name = \"Natural gas price\", yaxis = \"y1\",\n            line = list(color = \"blue\")) %&gt;%\n  \n  layout(\n    title = \"WTI crude oil vs Henry Hub natural gas\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Price (USD)\"),\n    legend = list(x = 0.05, y = 0.95)\n  )\n\nfig"
  },
  {
    "objectID": "visualization.html#financial-dynamics-of-clean-energy-assets-return-volatility-behavior",
    "href": "visualization.html#financial-dynamics-of-clean-energy-assets-return-volatility-behavior",
    "title": "Data Visulization",
    "section": "Financial Dynamics of Clean Energy Assets (Return & Volatility Behavior)",
    "text": "Financial Dynamics of Clean Energy Assets (Return & Volatility Behavior)\nFrom the figure, one can observe several key patterns. Periods of sharp volatility in oil prices—such as the downturn around 2008 and the collapse in early 2020—are mirrored by fluctuations in energy-sector equities, though the magnitude and persistence of these responses differ across renewable and traditional ETFs. Notably, while XLE tends to correlate more closely with oil price movements, ICLN demonstrates partial decoupling, reflecting the structural shift toward renewable energy sources whose valuation drivers are less directly dependent on crude oil prices.\n\n\nCode\nlibrary(plotly)\n\netf_symbols &lt;- c(\"ICLN\", \"XLE\")\n\netf_data &lt;- tq_get(etf_symbols,\n                   from = \"2005-01-01\",\n                   to = Sys.Date(),\n                   get = \"stock.prices\")\n\netf_data &lt;- etf_data %&gt;%\n  select(symbol, date, adjusted)\n\nfig &lt;- plot_ly() %&gt;%\n  # ICLN\n  add_lines(data = subset(etf_data, symbol == \"ICLN\"),\n            x = ~date, y = ~adjusted,\n            name = \"ICLN (Renewable ETF)\",\n            line = list(color = \"green\")) %&gt;%\n  \n  # XLE\n  add_lines(data = subset(etf_data, symbol == \"XLE\"),\n            x = ~date, y = ~adjusted,\n            name = \"XLE (Energy ETF)\",\n            line = list(color = \"blue\")) %&gt;%\n  \n  layout(\n    title = \"Clean energy (ICLN) vs traditional energy (XLE)\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"ETF Price (USD)\"),\n    legend = list(x = 0.05, y = 0.95)\n  )\n\nfig"
  },
  {
    "objectID": "source.html",
    "href": "source.html",
    "title": "Data Souces",
    "section": "",
    "text": "The International Energy Agency (IEA) provides comprehensive, high-quality datasets on global energy production and consumption. Specifically, their renewable energy statistics track solar, wind, hydro, and bioenergy generation across different countries and regions. This dataset enables analysis of seasonality, structural growth, and geographic distribution of renewable adoption over time. The data is primarily available in annual and monthly time-series formats and is widely used in policy and academic research. For this project, we will focus on renewable generation from 1990 to the present. Data can be accessed through the IEA Data Browser"
  },
  {
    "objectID": "source.html#data-source-1-international-energy-agency-iea",
    "href": "source.html#data-source-1-international-energy-agency-iea",
    "title": "Data Souces",
    "section": "",
    "text": "The International Energy Agency (IEA) provides comprehensive, high-quality datasets on global energy production and consumption. Specifically, their renewable energy statistics track solar, wind, hydro, and bioenergy generation across different countries and regions. This dataset enables analysis of seasonality, structural growth, and geographic distribution of renewable adoption over time. The data is primarily available in annual and monthly time-series formats and is widely used in policy and academic research. For this project, we will focus on renewable generation from 1990 to the present. Data can be accessed through the IEA Data Browser"
  },
  {
    "objectID": "source.html#data-source-2-u.s.-energy-information-administration-eia",
    "href": "source.html#data-source-2-u.s.-energy-information-administration-eia",
    "title": "Data Souces",
    "section": "Data Source 2 – U.S. Energy Information Administration (EIA)",
    "text": "Data Source 2 – U.S. Energy Information Administration (EIA)\n\n\n\n\n\nThe U.S. Energy Information Administration (EIA) publishes official statistics on global energy markets, including fossil fuel consumption, production, and pricing. This dataset includes historical oil prices (e.g., Brent crude), natural gas benchmarks, and coal usage, which provide insight into the decline of fossil fuels and volatility in energy markets. The data is updated regularly and made available via the EIA Open Data API, which allows time-series extraction of daily, monthly, and annual series. For this project, fossil fuel consumption and pricing trends from 1980 onward will be analyzed to compare against renewable energy adoption."
  },
  {
    "objectID": "source.html#data-source-3-yahoo-finance",
    "href": "source.html#data-source-3-yahoo-finance",
    "title": "Data Souces",
    "section": "Data Source 3 – Yahoo Finance",
    "text": "Data Source 3 – Yahoo Finance\n\n\n\n\n\nYahoo Finance provides historical financial market data, including stock prices, indices, and exchange rates. For this project, we will collect stock price information for major clean energy companies (e.g., NextEra Energy, Tesla, Ørsted, Enphase) and fossil fuel companies (e.g., ExxonMobil, Chevron). This allows us to compare market valuation trends and volatility between renewable and traditional energy firms. We will also track clean energy ETFs (e.g., ICLN, TAN) to measure sector-level performance. You can find the data at Yahoo Finance"
  },
  {
    "objectID": "source.html#data-source-4-federal-reserve-economic-data-fred",
    "href": "source.html#data-source-4-federal-reserve-economic-data-fred",
    "title": "Data Souces",
    "section": "Data Source 4 – Federal Reserve Economic Data (FRED)",
    "text": "Data Source 4 – Federal Reserve Economic Data (FRED)\n\n\n\n\n\nFRED, maintained by the Federal Reserve Bank of St. Louis, provides macroeconomic time-series data such as energy consumption, electricity generation, and commodity prices. For this project, we will extract U.S. time-series on renewable electricity production, natural gas prices, and West Texas Intermediate (WTI) crude oil prices. This dataset will be used to study energy price cycles, structural breaks, and relationships between fossil fuel prices and renewable adoption. You can find the data at FRED API"
  },
  {
    "objectID": "source.html#data-source-5-world-bank-carbon-pricing-dashboard",
    "href": "source.html#data-source-5-world-bank-carbon-pricing-dashboard",
    "title": "Data Souces",
    "section": "Data Source 5 – World Bank Carbon Pricing Dashboard",
    "text": "Data Source 5 – World Bank Carbon Pricing Dashboard\n\n\n\n\n\nThe World Bank Carbon Pricing Dashboard provides global data on carbon taxes and emissions trading systems. These time-series are valuable for analyzing policy shocks and their impact on renewable adoption and energy investment flows. For example, we can test whether countries that introduced carbon pricing mechanisms show faster growth in renewable energy capacity. Data is freely available from the World Bank."
  },
  {
    "objectID": "multivariate.html",
    "href": "multivariate.html",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "",
    "text": "Energy transition is both a result of technological evolution and a dynamic process driven by the combined effects of economics, climate, and policy. Numerous studies have shown that fossil fuel prices (such as crude oil and natural gas) have a significant impact on the development of renewable energy: when oil or natural gas prices rise, the relative cost of fossil fuels increases, enhancing the relative competitiveness of clean energy, thereby driving growth in renewable power generation or related investment. Simultaneously, climate and weather variables (temperature, irradiance, degree-days) directly affect the short-term output of renewable power generation: solar power is affected by solar irradiance and temperature (high temperatures may slightly reduce photovoltaic efficiency), while wind power is affected by seasonal wind speeds and extreme weather. Finally, policy (subsidies, feed-in tariffs, carbon prices) is a structural driver: policy shocks typically produce lasting trend changes or step responses.\n\n\n\nLiterature divides price transmission into supply-side and demand-side mechanisms: Price increases make renewable energy more attractive by raising the cost of fossil fuel power generation (or the marginal cost of fuel power generation) (substitution effect); another channel is the financial market: oil and gas prices affect the earnings and stock performance of related energy companies, which in turn affects ETFs or capital flows (ICLE, XLE, etc.). These financial variables then feed back into investment and project construction decisions.\n\n\n\nIn empirical methods, researchers widely use ARIMAX/SARIMAX (modeling a single response variable under exogenous shocks) to characterize the short-term impact of exogenous variables such as weather/prices on power generation or prices; while VAR/VECM/VARMA are used to study the dynamic interactions between several endogenous variables (e.g., how wind, solar, natural gas, and coal power generation interact), and to conduct impact response analysis (IRFs) and variance decomposition to identify shock sources and propagation paths. Model selection typically considers the seasonality, trend, and cointegration of time series data.\n\n\n\nETFs (such as clean energy ETFs vs. traditional energy ETFs) are often used as market signals for the energy transition. Empirical research shows that energy ETFs are sensitive to oil and gas prices, macroeconomic and policy news, and can reflect investors’ expectations for the transition prospects. Therefore, using ETFs as financial indicators can supplement the dimension of market expectations that cannot be directly captured by physical electricity generation sequences."
  },
  {
    "objectID": "multivariate.html#literature-review",
    "href": "multivariate.html#literature-review",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "",
    "text": "Energy transition is both a result of technological evolution and a dynamic process driven by the combined effects of economics, climate, and policy. Numerous studies have shown that fossil fuel prices (such as crude oil and natural gas) have a significant impact on the development of renewable energy: when oil or natural gas prices rise, the relative cost of fossil fuels increases, enhancing the relative competitiveness of clean energy, thereby driving growth in renewable power generation or related investment. Simultaneously, climate and weather variables (temperature, irradiance, degree-days) directly affect the short-term output of renewable power generation: solar power is affected by solar irradiance and temperature (high temperatures may slightly reduce photovoltaic efficiency), while wind power is affected by seasonal wind speeds and extreme weather. Finally, policy (subsidies, feed-in tariffs, carbon prices) is a structural driver: policy shocks typically produce lasting trend changes or step responses.\n\n\n\nLiterature divides price transmission into supply-side and demand-side mechanisms: Price increases make renewable energy more attractive by raising the cost of fossil fuel power generation (or the marginal cost of fuel power generation) (substitution effect); another channel is the financial market: oil and gas prices affect the earnings and stock performance of related energy companies, which in turn affects ETFs or capital flows (ICLE, XLE, etc.). These financial variables then feed back into investment and project construction decisions.\n\n\n\nIn empirical methods, researchers widely use ARIMAX/SARIMAX (modeling a single response variable under exogenous shocks) to characterize the short-term impact of exogenous variables such as weather/prices on power generation or prices; while VAR/VECM/VARMA are used to study the dynamic interactions between several endogenous variables (e.g., how wind, solar, natural gas, and coal power generation interact), and to conduct impact response analysis (IRFs) and variance decomposition to identify shock sources and propagation paths. Model selection typically considers the seasonality, trend, and cointegration of time series data.\n\n\n\nETFs (such as clean energy ETFs vs. traditional energy ETFs) are often used as market signals for the energy transition. Empirical research shows that energy ETFs are sensitive to oil and gas prices, macroeconomic and policy news, and can reflect investors’ expectations for the transition prospects. Therefore, using ETFs as financial indicators can supplement the dimension of market expectations that cannot be directly captured by physical electricity generation sequences."
  },
  {
    "objectID": "multivariate.html#key-research-questions",
    "href": "multivariate.html#key-research-questions",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "Key Research Questions",
    "text": "Key Research Questions\n\nDoes the price of fossil fuels drive the growth of solar power generation through substitution effects?\n\nWhat are the immediate or lagged responses of solar power if WTI or HH rises? (Examine the β parameter and impulse response)\n\nWhat are the short-term and seasonal effects of weather variables (temp or degree-days) on solar output? Are there nonlinearities (e.g., excessively high temperatures reduce efficiency)?\n\nIt is necessary to examine the linear/quadratic terms of temp, as well as the seasonal interaction term.\n\nAre there substitution or complementarity relationships in the power generation structure (wind, solar, coal, natural gas)? How do shocks (e.g., rising natural gas prices) propagate between different energy sources?\n\nVAR can be used to obtain the impulse response function (IRF) and variance decomposition to answer these questions.\n\nHow do financial markets (ICLE vs XLE) reflect expectations of energy transition? Are these ETFs leading/lagging relationships with actual power generation data or fuel prices?\n\nVAR can be used to test Granger causality and interaction.\n\nHave policies or structural events altered the long-term relationships between energy variables (e.g., trend abrupt changes or changes in cointegration structures)?\n\nExamine structural breakpoints and incorporate policy dummy variables to quantify policy effects.\n\nForecasting: How does the ARIMAX model perform in short-term solar forecasting, considering the uncertainty of exogenous variables? Is it a significant improvement compared to using an autoregressive model alone?\n\n\nTest model stability using time series cross-validation (tsCV) and RMSE comparisons."
  },
  {
    "objectID": "multivariate.html#arimax-solar-power-generation-temperature-oil-prices-gas-prices",
    "href": "multivariate.html#arimax-solar-power-generation-temperature-oil-prices-gas-prices",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "(1) ARIMAX: Solar power generation ~ temperature + oil prices + gas prices",
    "text": "(1) ARIMAX: Solar power generation ~ temperature + oil prices + gas prices\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(seasonal)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(gridExtra)\nlibrary(fpp2)\nlibrary(prophet)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(patchwork)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tsibble)\nlibrary(knitr)\n\ndf &lt;- read.csv(\"data/electric.csv\")\ndf &lt;- df %&gt;%\n  mutate(\n    Description = str_extract(Description, \"(?&lt;=From ).*?(?=,)\"),\n    YYYYMM = as.character(YYYYMM),\n    Year = as.integer(substr(YYYYMM, 1, 4)),\n    Month = as.integer(substr(YYYYMM, 5, 6))\n  ) %&gt;%\n  filter(Month &lt;= 12) %&gt;%\n  filter(Year &gt;= 2000) %&gt;%\n  filter(Year &lt;= 2024) %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))\n\n\n\n\nCode\nsolar_df &lt;- df %&gt;%\n  filter(Description == \"Solar\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  rename(Solar = Value) %&gt;%\n  mutate(Solar = log(pmax(Solar, .Machine$double.eps))) %&gt;%\n  arrange(Date)\n\nts_solar &lt;- ts(solar_df$Solar,\n              start = c(min(solar_df$Year), min(solar_df$Month)),\n              frequency = 12)\n\nwti &lt;- read.csv(\"data/WTIprice.csv\")\nwti &lt;- wti %&gt;%\n  rename(Date = observation_date) %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  filter(Date &gt;= as.Date(\"2000-01-01\")) %&gt;%\n  filter(Date &lt;= as.Date(\"2024-12-01\")) %&gt;%\n  rename(WTI = POILWTIUSDM) %&gt;%\n  mutate(WTI = as.numeric(WTI))%&gt;%\n  arrange(Date)\n\nts_wti &lt;- ts(wti$WTI,\n              start = c(2000, 1),\n              frequency = 12)\n\ndf_n &lt;- read.csv(\"data/naturalgasprice.csv\")\ndf_n &lt;- df_n %&gt;%\n  mutate(\n    YYYYMM = as.character(YYYYMM),\n    Year = as.integer(substr(YYYYMM, 1, 4)),\n    Month = as.integer(substr(YYYYMM, 5, 6))\n  ) %&gt;%\n  filter(Month &lt;= 12) %&gt;%\n  filter(Year &gt;= 2000) %&gt;%\n  filter(Year &lt;= 2024) %&gt;%\n  mutate(HH = as.numeric(Value)) %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))%&gt;%\n  mutate(HH = log(pmax(HH, .Machine$double.eps))) %&gt;%\n  arrange(Date)\n\nts_np &lt;- ts(df_n$HH,\n              start = c(min(df_n$Year), min(df_n$Month)),\n              frequency = 12)\n\ndf &lt;- read.csv(\"data/Global_Temperature_2000onwards.csv\")\ndf &lt;- df[order(df$Year, df$Month), ]\ndf_temp &lt;- df %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))\n\nts_anomaly &lt;- ts(df_temp$Monthly_Anomaly,\n                 start = c(2000, 1),   \n                 frequency = 12)     \n\n\n\nVisualizationauto.arima()Manual ModelModel DiagnosticsCVFinal Model FitForecasting\n\n\n\nThe relationship between temperature (temp) and solar power generation: Solar power generation is closely related to temperature and irradiance, but extreme high temperatures can reduce the efficiency of photovoltaic panels.\nThe substitution effect of fossil fuel prices on clean energy: When oil prices (WTI) or natural gas prices (HH) rise, clean energy (especially wind and solar power) becomes more economically attractive.\n\n\n\nCode\ndata_all &lt;- solar_df %&gt;%\n  full_join(df_temp, by = \"Date\") %&gt;%\n  full_join(wti, by = \"Date\") %&gt;%\n  full_join(df_n, by = \"Date\") %&gt;%\n  arrange(Date)\n\n# Create individual plots for each variable with y-axis labels\np1 &lt;- plot_ly(data_all, x = ~Date, y = ~Solar, type = 'scatter', mode = 'lines', name = \"Solar Electricity Generation\") %&gt;%\n  layout(yaxis = list(title = \"Value\"))\n\np2 &lt;- plot_ly(data_all, x = ~Date, y = ~Monthly_Anomaly, type = 'scatter', mode = 'lines', name = \"Global Average Temperature Anomaly\") %&gt;%\n  layout(yaxis = list(title = \"Temperature (C)\"))\n\np3 &lt;- plot_ly(data_all, x = ~Date, y = ~WTI, type = 'scatter', mode = 'lines', name = \"WTI Oil price\") %&gt;%\n  layout(yaxis = list(title = \"Price\"))\n\np4 &lt;- plot_ly(data_all, x = ~Date, y = ~HH, type = 'scatter', mode = 'lines', name = \"Henry Hub Natural Gas price\") %&gt;%\n  layout(yaxis = list(title = \"Price\"))\n\n# Combine the individual plots into a single figure with subplots and y-axis labels\nfinal_plot &lt;- subplot(p1, p2, p3, p4, nrows = 4, shareX = TRUE, titleX = TRUE) %&gt;%\n  layout(\n    title = \"Time Series of Solar power generation, temperature, and oil and gas prices\",\n    yaxis = list(title = \"Value\"), \n    yaxis2 = list(title = \"Temperature (C)\"), \n    yaxis3 = list(title = \"Price\"),\n    yaxis4 = list(title = \"Price\"),\n    xaxis = list(title = \"Year\")\n  )\n\nfinal_plot\n\n\n\n\n\n\n\n\n\n\nCode\nfit_auto &lt;- auto.arima(\n  ts_solar,\n  xreg = cbind(ts_anomaly, ts_wti, ts_np))\n\nsummary(fit_auto)\n\n\nSeries: ts_solar \nRegression with ARIMA(0,1,0)(2,0,0)[12] errors \n\nCoefficients:\n        sar1    sar2  ts_anomaly   ts_wti   ts_np\n      0.7384  0.0343     -0.2005  -0.0035  0.1222\ns.e.  0.0623  0.0655      0.1178   0.0024  0.2867\n\nsigma^2 = 0.0832:  log likelihood = -55.29\nAIC=122.58   AICc=122.87   BIC=144.78\n\nTraining set error measures:\n                      ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set 0.007005674 0.2855417 0.1738749 -0.5963215 5.174797 0.5206869\n                   ACF1\nTraining set -0.3249882\n\n\nCode\ncheckresiduals(fit_auto)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0)(2,0,0)[12] errors\nQ* = 75.679, df = 22, p-value = 8.212e-08\n\nModel df: 2.   Total lags used: 24\n\n\nThe auto.arima() fits a Regression with (0,1,0)(2,0,0)[12] errors. The ACF plot and Ljung–Box test all indicate that the model does not fully capture the seasonal structure.\n\n\n\n\nCode\nlm_fit &lt;- lm(Solar ~ Monthly_Anomaly + WTI + HH, data = data_all)\nsummary(lm_fit)\n\n\n\nCall:\nlm(formula = Solar ~ Monthly_Anomaly + WTI + HH, data = data_all)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4115 -1.0282  0.0462  1.2208  4.0037 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -4.819000   0.880666  -5.472 9.49e-08 ***\nMonthly_Anomaly  7.017950   0.397215  17.668  &lt; 2e-16 ***\nWTI              0.009648   0.004100   2.353   0.0193 *  \nHH               1.839612   0.389946   4.718 3.68e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.585 on 296 degrees of freedom\nMultiple R-squared:  0.5961,    Adjusted R-squared:  0.592 \nF-statistic: 145.6 on 3 and 296 DF,  p-value: &lt; 2.2e-16\n\n\nFrom the regression model, all variables are statistically significant, indicating that WTI oil price, natural gas price and temperature have a meaningful impact on total solar electricity generation. This suggests that changes in these factors directly influence solar electricity generation.\n\n\nCode\nacf(residuals(lm_fit))\n\n\n\n\n\n\n\n\n\nMoreover, the residuals exhibit a high correlation, indicating significant serial correlation that remains unexplained. This suggests that traditional machine learning models may struggle to fully capture the underlying temporal patterns in these time series variables.\n\n\nCode\nres.fit &lt;- ts(residuals(lm_fit), frequency = 12)\np1 &lt;- ggAcf(res.fit)\np2 &lt;- ggPacf(res.fit)\n(p1 | p2)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(patchwork)\nfirst_diff &lt;- diff(res.fit)  \n\nseasonal_diff &lt;- diff(first_diff, lag = 12)\n\n###### Plot ACF and PACF for the Differenced Series ######\np1 &lt;- ggAcf(seasonal_diff) +\n  ggtitle(\"ACF of Seasonally Differenced Residuals\") +\n  theme_minimal()\n\np2 &lt;- ggPacf(seasonal_diff) +\n  ggtitle(\"PACF of Seasonally Differenced Residuals\") +\n  theme_minimal()\n\n# Display plots\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\nThis series appears closer to weak stationarity. The ACF plot shows clear autocorrelations at lags q = 0,1,2,3 and Q = 1,2.\nAdditionally, both first-order differencing (d = 1) and seasonal differencing (D = 1) have been applied to the data.\nThe PACF plot shows clear autocorrelations at lags p = 0,1,2,3 and P = 0,1,2.\n\n\nCode\n###### Define SARIMA Model Comparison Function ######\nSARIMA.c &lt;- function(p_range, d_range, q_range, P_range, D_range, Q_range, data) {\n  \n  # Set seasonal period\n  s &lt;- 12\n  \n  # Initialize results storage\n  results_list &lt;- list()\n  \n  # Iterate over parameter combinations\n  for (p in p_range) {\n    for (d in d_range) {\n      for (q in q_range) {\n        for (P in P_range) {\n          for (D in D_range) {\n            for (Q in Q_range) {\n              \n              if (p + d + q + P + D + Q &lt;= 10) {\n                result &lt;- tryCatch({\n                  model &lt;- Arima(\n                    data,\n                    order = c(p, d, q),\n                    seasonal = list(order = c(P, D, Q), period = s)\n                  )\n                  \n                  c(p, d, q, P, D, Q,\n                    AIC(model),\n                    BIC(model),\n                    model$aicc)\n                  \n                }, error = function(e) {\n                  c(p, d, q, P, D, Q, NA, NA, NA)\n                })\n                \n                results_list[[length(results_list) + 1]] &lt;- result\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  \n  # Convert results to a tidy data frame\n  results_df &lt;- as.data.frame(do.call(rbind, results_list))\n  colnames(results_df) &lt;- c(\"p\", \"d\", \"q\", \"P\", \"D\", \"Q\", \"AIC\", \"BIC\", \"AICc\")\n  \n  return(results_df)\n}\n\n###### Run SARIMA Model Comparison ######\noutput &lt;- SARIMA.c(\n  p_range = 0:3, q_range = 0:3, \n  d_range = 1, D_range = 1, \n  P_range = 0:2, Q_range = 0:2, \n  data = res.fit\n)\n\n###### Identify Models with Minimum AIC and BIC ######\nminaic &lt;- output[which.min(output$AIC), ]\nminbic &lt;- output[which.min(output$BIC), ]\n\n###### Display Best Models Based on AIC and BIC ######\nprint(minaic)\n\n\n   p d q P D Q      AIC      BIC     AICc\n96 2 1 2 1 1 2 729.6844 758.9602 730.2023\n\n\n\n\nThe model ARIMA(2,1,2)x(1,1,2)[12] works siginificantly better than ARIMA(0,1,0)x(2,0,0)[12] due to the lower AIC, BIC, and AICc numbers.\n\n\nCode\nmodel_output_1 &lt;- capture.output(sarima(res.fit, 2, 1, 2, 1, 1, 2, 12))\n\n\n\n\n\n\n\n\n\nCode\n###### Function to Extract Model Diagnostics ######\nextract_model_diagnostics &lt;- function(model_output) {\n  start_line &lt;- grep(\"Coefficients\", model_output)  # Find where coefficients start\n  end_line &lt;- length(model_output)  # Capture till the last line\n  if (length(start_line) &gt; 0) {\n    cat(model_output[start_line:end_line], sep = \"\\n\")  # Print coefficient section\n  } else {\n    cat(\"No coefficient details found.\\n\")  # Handle cases where output format changes\n  }\n}\n\n###### Extract and Print Model Diagnostics ######\ncat(\"### ARIMA(2,1,2)x(1,1,2)[12] ###\\n\")\n\n\n### ARIMA(2,1,2)x(1,1,2)[12] ###\n\n\nCode\nextract_model_diagnostics(model_output_1)\n\n\nCoefficients: \n     Estimate     SE  t.value p.value\nar1   -0.0857 0.1575  -0.5444  0.5866\nar2    0.4865 0.0843   5.7725  0.0000\nma1   -0.3514 0.1644  -2.1372  0.0335\nma2   -0.4945 0.1338  -3.6948  0.0003\nsar1  -0.9712 0.0467 -20.7861  0.0000\nsma1   0.1310 0.0839   1.5613  0.1196\nsma2  -0.7351 0.0832  -8.8318  0.0000\n\nsigma^2 estimated as 0.6651609 on 280 degrees of freedom \n \nAIC = 2.542454  AICc = 2.543853  BIC = 2.644461 \n \n\n\n\n\nCode\nmodel_output_2 &lt;- capture.output(sarima(res.fit, 0, 1, 0, 2, 0, 0, 12))\n\n\n\n\n\n\n\n\n\nCode\n###### Extract and Print Model Diagnostics ######\ncat(\"### ARIMA(0,1,0)x(2,0,0)[12] ###\\n\")\n\n\n### ARIMA(0,1,0)x(2,0,0)[12] ###\n\n\nCode\nextract_model_diagnostics(model_output_2)\n\n\nCoefficients: \n         Estimate     SE t.value p.value\nsar1       0.1181 0.0579  2.0389  0.0423\nsar2       0.1333 0.0592  2.2503  0.0252\nconstant  -0.0069 0.0691 -0.0993  0.9210\n\nsigma^2 estimated as 0.8323154 on 296 degrees of freedom \n \nAIC = 2.68328  AICc = 2.683552  BIC = 2.732784 \n \n\n\n\n\nUsing corss validation, we can confirm that ARIMA(0,1,0)x(2,0,0)[12] is the better model since is stays at a lower RMSE for the majority of the time in the plot below.\n\n\nCode\ny &lt;- as.numeric(res.fit)\nn &lt;- length(y)\nk &lt;- 92   # initial training size\n\nrmse1 &lt;- matrix(NA, 52, 4)\nrmse2 &lt;- matrix(NA, 52, 4)\n\nfor (i in 1:52) {\n\n  # training = first (k+i-1) observations\n  xtrain &lt;- y[1:(k+i-1)]\n  \n  # test = next 4 observations\n  xtest  &lt;- y[(k+i):(k+i+3)]\n  \n  # fit ARIMA models\n  fit &lt;- Arima(ts(xtrain, frequency=12),\n               order=c(2,1,2),\n               seasonal=list(order=c(1,1,2), period=12))\n\n  fcast &lt;- forecast(fit, h=4)\n\n  fit2 &lt;- Arima(ts(xtrain, frequency=12),\n                order=c(0,1,0),\n                seasonal=list(order=c(2,0,0), period=12))\n\n  fcast2 &lt;- forecast(fit2, h=4)\n\n  # ensure xtest length = 4\n  if (length(xtest)==4 && length(fcast$mean)==4) {\n    rmse1[i,] &lt;- sqrt((fcast$mean - xtest)^2)\n    rmse2[i,] &lt;- sqrt((fcast2$mean - xtest)^2)\n  }\n}\n\n# final RMSE\ncolMeans(rmse1, na.rm=TRUE)\n\n\n[1] 0.8445689 1.0799366 1.2584144 1.3675548\n\n\nCode\ncolMeans(rmse2, na.rm=TRUE)\n\n\n[1] 0.7018249 0.9072551 1.0266301 1.0813421\n\n\n\n\nCode\n##### RMSE Plot using Plotly #####\n# Create a dataframe for RMSE values by quarter\nqr &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n\nrmse11 &lt;- data.frame(Quarter = qr, RMSE = colMeans(rmse1, na.rm = TRUE))\nrmse22 &lt;- data.frame(Quarter = qr, RMSE = colMeans(rmse2, na.rm = TRUE))\n\n# Generate an interactive RMSE comparison plot\nplot_ly() %&gt;%\n  add_lines(data = rmse11, x = ~Quarter, y = ~RMSE, \n            color = I(\"blue\"), name = \"RMSE1: ARIMA(2,1,2)x(1,1,2)[12]\") %&gt;%\n  add_lines(data = rmse22, x = ~Quarter, y = ~RMSE, \n            color = I(\"red\"), name = \"RMSE2: ARIMA(0,1,0)x(2,0,0)[12]\") %&gt;%\n  layout(\n    title = \"Cross-Validation RMSE\",\n    xaxis = list(title = \"Quarter\"),\n    yaxis = list(title = \"RMSE\"),\n    legend = list(title = \"Model\")\n  )\n\n\n\n\n\n\n\n\n\n\nCode\n# Define the dependent variable (TotalVehicleSales)\ny &lt;- ts_solar\n\n# Define external regressors (all variables except TotalVehicleSales)\nxreg &lt;- cbind(ts_anomaly, ts_wti, ts_np)\n\n# Fit ARIMA(2,1,0)x(0,0,1)[12] with external regressors\nfit &lt;- Arima(y, order = c(0,1,0), seasonal = list(order = c(2,0,0), period = 12), xreg = xreg)\n#fit1 &lt;- Arima(y, order = c(2,1,2), seasonal = list(order = c(1,1,2), period = 12), xreg = xreg)\n# Display model summary\nsummary(fit)\n\n\nSeries: y \nRegression with ARIMA(0,1,0)(2,0,0)[12] errors \n\nCoefficients:\n        sar1    sar2  ts_anomaly   ts_wti   ts_np\n      0.7384  0.0343     -0.2005  -0.0035  0.1222\ns.e.  0.0623  0.0655      0.1178   0.0024  0.2867\n\nsigma^2 = 0.0832:  log likelihood = -55.29\nAIC=122.58   AICc=122.87   BIC=144.78\n\nTraining set error measures:\n                      ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set 0.007005674 0.2855417 0.1738749 -0.5963215 5.174797 0.5206869\n                   ACF1\nTraining set -0.3249882\n\n\n\n\nThis model successfully captured the long-term upward trend in solar power generation and accurately reproduced the monthly seasonal patterns. Short-term forecasts have high reliability, while medium- and long-term forecasts exhibit reasonable confidence interval expansion, reflecting the uncertainties in the energy sector. Although the ARIMA model slightly exaggerates the long-term growth slope, the overall trend direction is highly consistent with the structural changes in the US energy market. Therefore, this forecast can serve as a reliable baseline for future solar power generation growth.\n\n\nCode\n# Fit ARIMA model to each external regressor\ntmp_fit &lt;- auto.arima(xreg[, \"ts_anomaly\"])\n#summary(FinR_fit)\nftmp &lt;- forecast(tmp_fit, h = 32)  # Forecast next 32 periods\n\nwti_fit &lt;- auto.arima(xreg[, \"ts_wti\"])\n#summary(Imp_fit)\nfwti &lt;- forecast(wti_fit, h = 32)\n\nhh_fit &lt;- auto.arima(xreg[, \"ts_np\"])\n#summary(CPI_fit)\nfhh &lt;- forecast(hh_fit, h = 32)\n\n\n\n\nCode\n# Create future external regressor matrix using forecasted values\nfxreg &lt;- cbind(Temperature = ftmp$mean, \n               WTI = fwti$mean, \n               HH = fhh$mean)\n\n# Fit ARIMA(2,0,2)x(0,0,1)[12] with external regressors\nfit &lt;- Arima(y, order = c(0,1,0), seasonal = list(order = c(2,0,0), period = 12), xreg = xreg)\n# Forecast TotalVehicleSales using future external regressors\nfcast &lt;- forecast(fit, xreg = fxreg, h = 32)\n\n# Plot the forecast\nautoplot(fcast) +\n  ggtitle(\"Solar Electricity Net Generation Forecast\") +\n  xlab(\"Year\") +\n  ylab(\"Value\") +\n  theme_minimal()"
  },
  {
    "objectID": "multivariate.html#arimax-icln-xle-oil-prices-gas-prices",
    "href": "multivariate.html#arimax-icln-xle-oil-prices-gas-prices",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "(2) ARIMAX: ICLN ~ XLE + oil prices + gas prices",
    "text": "(2) ARIMAX: ICLN ~ XLE + oil prices + gas prices\n\n\nCode\netf_symbols &lt;- c(\"ICLN\", \"XLE\")\n\netf_data &lt;- tq_get(etf_symbols,\n                   from = \"2000-01-01\",\n                   to = \"2024-12-01\",\n                   get = \"stock.prices\")\n\nicln_monthly &lt;- etf_data %&gt;%\n  filter(symbol == \"ICLN\") %&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = to.monthly,\n               indexAt = \"lastof\",\n               col_rename = \"adjusted\")\n\nts_icln &lt;- ts(icln_monthly$adjusted,\n              start = c(year(min(icln_monthly$date)), month(min(icln_monthly$date))),\n              frequency = 12)\n\nxle_monthly &lt;- etf_data %&gt;%\n  filter(symbol == \"XLE\") %&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = to.monthly,\n               indexAt = \"lastof\",\n               col_rename = \"adjusted\")\n\nts_xle &lt;- ts(xle_monthly$adjusted,\n              start = c(year(min(xle_monthly$date)), month(min(xle_monthly$date))),\n              frequency = 12)\n\n\n\nVisualizationauto.arima()Manual ModelModel DiagnosticsCVFinal Model FitForecasting\n\n\n\nFuel Prices → Substitution/Competition Channel for Clean Energy: When fossil fuel prices (especially natural gas or oil) rise, the costs for fossil power generation and related companies increase, making renewable energy (and related listed companies) more economically attractive, thereby boosting their valuations and ETF performance.\nTraditional Energy ETFs (XLE) as a Transmission of Industry Sentiment/Risk Premium: XLE represents the market performance of the traditional energy sector, reflecting industry prosperity, capital flows, and investor risk appetite. The correlation between ICLN and XLE can capture “sector rotation” or the market transmission of funds from traditional energy to clean energy. If the two move in opposite directions, it indicates a substitution relationship between fund flows and market expectations.\n\n\n\nCode\nwti &lt;- read.csv(\"data/WTIprice.csv\")\nwti &lt;- wti %&gt;%\n  rename(Date = observation_date) %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  filter(Date &gt;= as.Date(\"2008-01-01\")) %&gt;%\n  filter(Date &lt;= as.Date(\"2024-12-01\")) %&gt;%\n  rename(WTI = POILWTIUSDM) %&gt;%\n  mutate(WTI = as.numeric(WTI))%&gt;%\n  arrange(Date)\n\nts_wti &lt;- ts(wti$WTI,\n              start = c(2008, 1),\n              frequency = 12)\n\ndf_n &lt;- read.csv(\"data/naturalgasprice.csv\")\ndf_n &lt;- df_n %&gt;%\n  mutate(\n    YYYYMM = as.character(YYYYMM),\n    Year = as.integer(substr(YYYYMM, 1, 4)),\n    Month = as.integer(substr(YYYYMM, 5, 6))\n  ) %&gt;%\n  filter(Month &lt;= 12) %&gt;%\n  filter(Year &gt;= 2008) %&gt;%\n  filter(Year &lt;= 2024) %&gt;%\n  mutate(HH = as.numeric(Value)) %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))%&gt;%\n  mutate(HH = log(pmax(HH, .Machine$double.eps))) %&gt;%\n  arrange(Date)\n\nts_np &lt;- ts(df_n$HH,\n              start = c(min(df_n$Year), min(df_n$Month)),\n              frequency = 12)\n\ndf_icln &lt;- data.frame(\n  Date = as.Date(time(ts_icln)),   \n  ICLN = as.numeric(ts_icln)\n)\n\ndf_xle &lt;- data.frame(\n  Date = as.Date(time(ts_xle)),  \n  XLE = as.numeric(ts_xle)\n)\n\ndata_all &lt;- df_icln %&gt;%\n  full_join(df_n, by = \"Date\") %&gt;%\n  full_join(wti, by = \"Date\") %&gt;%\n  full_join(df_xle, by = \"Date\") %&gt;%\n  arrange(Date)\n\ndata_all &lt;- subset(data_all, Date &gt;= as.Date(\"2008-06-01\"))\n\n\n\n\nCode\ncommon_start &lt;- c(2008, 6)\ncommon_end   &lt;- c(2024, 11)\n\nts_icln &lt;- window(ts_icln, start = common_start, end = common_end)\nts_xle  &lt;- window(ts_xle,  start = common_start, end = common_end)\nts_wti  &lt;- window(ts_wti,  start = common_start, end = common_end)\nts_np   &lt;- window(ts_np,   start = common_start, end = common_end)\n\n\n\n\nCode\n# Create individual plots for each variable with y-axis labels\np1 &lt;- plot_ly(data_all, x = ~Date, y = ~ICLN, type = 'scatter', mode = 'lines', name = \"Renewable Energy ETF (ICLN)\") %&gt;%\n  layout(yaxis = list(title = \"Price\"))\n\np2 &lt;- plot_ly(data_all, x = ~Date, y = ~XLE, type = 'scatter', mode = 'lines', name = \"Energy Select Sector SPDR Fund (XLE)\") %&gt;%\n  layout(yaxis = list(title = \"Price\"))\n\np3 &lt;- plot_ly(data_all, x = ~Date, y = ~WTI, type = 'scatter', mode = 'lines', name = \"WTI Oil price\") %&gt;%\n  layout(yaxis = list(title = \"Price\"))\n\np4 &lt;- plot_ly(data_all, x = ~Date, y = ~HH, type = 'scatter', mode = 'lines', name = \"Henry Hub Natural Gas price\") %&gt;%\n  layout(yaxis = list(title = \"Price\"))\n\n# Combine the individual plots into a single figure with subplots and y-axis labels\nfinal_plot &lt;- subplot(p2, p3, p4, p1, nrows = 4, shareX = TRUE, titleX = TRUE) %&gt;%\n  layout(\n    title = \"Time Series of ICLN, XLE, WTI and HH price\",\n    yaxis = list(title = \"Price\"), \n    yaxis2 = list(title = \"Price\"), \n    yaxis3 = list(title = \"Price\"),\n    yaxis4 = list(title = \"Price\"),\n    xaxis = list(title = \"Year\")\n  )\n\nfinal_plot\n\n\n\n\n\n\n\n\n\n\nCode\nfit_auto &lt;- auto.arima(\n  ts_icln,\n  xreg = cbind(ts_xle, ts_wti, ts_np)\n)\n\nsummary(fit_auto)\n\n\nSeries: ts_icln \nRegression with ARIMA(0,1,1)(2,0,0)[12] errors \n\nCoefficients:\n         ma1     sar1     sar2  ts_xle  ts_wti   ts_np\n      0.3556  -0.0975  -0.0040  0.2960  0.0094  1.4393\ns.e.  0.0688   0.0940   0.0955  0.0512  0.0180  0.8832\n\nsigma^2 = 1.837:  log likelihood = -336.51\nAIC=687.02   AICc=687.62   BIC=710.01\n\nTraining set error measures:\n                     ME     RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.1154506 1.331195 0.8690742 -0.841352 6.832762 0.2816484\n                    ACF1\nTraining set 0.002875956\n\n\nCode\ncheckresiduals(fit_auto)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,1)(2,0,0)[12] errors\nQ* = 24.12, df = 21, p-value = 0.2873\n\nModel df: 3.   Total lags used: 24\n\n\nThe auto.arima() fits a Regression with ARIMA(0,1,1)(2,0,0)[12] errors. It shows well-behaved residuals. - The residual time series fluctuates randomly around zero with no noticeable pattern. - The ACF plot does not exhibit significant autocorrelation, and most spikes remain within the 95% confidence bounds, indicating that the model successfully captures the serial dependence in the data. - The Ljung–Box test (p = 0.2873) confirms that we cannot reject the null hypothesis of no autocorrelation, meaning the residuals are consistent with white noise. - Therefore, this ARIMA model adequately accounts for the strong temporal dependence in the series\n\n\n\n\nCode\nlm_fit &lt;- lm(ICLN ~ XLE + WTI + HH, data = data_all)\nsummary(lm_fit)\n\n\n\nCall:\nlm(formula = ICLN ~ XLE + WTI + HH, data = data_all)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3476 -3.9395 -0.8801  2.4945 18.6202 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.38426    3.69562  -2.539   0.0119 *  \nXLE         -0.05777    0.05382  -1.073   0.2844    \nWTI          0.03615    0.01855   1.948   0.0528 .  \nHH           7.89197    1.52241   5.184 5.43e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.357 on 194 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.1566,    Adjusted R-squared:  0.1436 \nF-statistic: 12.01 on 3 and 194 DF,  p-value: 3.026e-07\n\n\nOnly HH (natural gas price) has a stable and significant effect on ICLN.\nXLE has no significant effect at all; WTI has only a weak correlation.\n\n\nCode\nacf(residuals(lm_fit))\n\n\n\n\n\n\n\n\n\nMoreover, the residuals exhibit a high correlation, indicating significant serial correlation that remains unexplained. This suggests that traditional machine learning models may struggle to fully capture the underlying temporal patterns in these time series variables.\n\n\nCode\nres.fit &lt;- ts(residuals(lm_fit), frequency = 12)\np1 &lt;- ggAcf(res.fit)\np2 &lt;- ggPacf(res.fit)\n(p1 | p2)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(patchwork)\nfirst_diff &lt;- diff(res.fit)  \n\nseasonal_diff &lt;- diff(first_diff, lag = 12)\n\n###### Plot ACF and PACF for the Differenced Series ######\np3 &lt;- ggAcf(seasonal_diff) +\n  ggtitle(\"ACF of Seasonally Differenced Residuals\") +\n  theme_minimal()\n\np4 &lt;- ggPacf(seasonal_diff) +\n  ggtitle(\"PACF of Seasonally Differenced Residuals\") +\n  theme_minimal()\n\n# Display plots\ngridExtra::grid.arrange(p3, p4, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n###### Define SARIMA Model Comparison Function ######\nSARIMA.c &lt;- function(p_range, d_range, q_range, P_range, D_range, Q_range, data) {\n  \n  # Set seasonal period\n  s &lt;- 12\n  \n  # Initialize results storage\n  results_list &lt;- list()\n  \n  # Iterate over parameter combinations\n  for (p in p_range) {\n    for (d in d_range) {\n      for (q in q_range) {\n        for (P in P_range) {\n          for (D in D_range) {\n            for (Q in Q_range) {\n              \n              if (p + d + q + P + D + Q &lt;= 10) {\n                result &lt;- tryCatch({\n                  model &lt;- Arima(\n                    data,\n                    order = c(p, d, q),\n                    seasonal = list(order = c(P, D, Q), period = s)\n                  )\n                  \n                  c(p, d, q, P, D, Q,\n                    AIC(model),\n                    BIC(model),\n                    model$aicc)\n                  \n                }, error = function(e) {\n                  c(p, d, q, P, D, Q, NA, NA, NA)\n                })\n                \n                results_list[[length(results_list) + 1]] &lt;- result\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  \n  # Convert results to a tidy data frame\n  results_df &lt;- as.data.frame(do.call(rbind, results_list))\n  colnames(results_df) &lt;- c(\"p\", \"d\", \"q\", \"P\", \"D\", \"Q\", \"AIC\", \"BIC\", \"AICc\")\n  \n  return(results_df)\n}\n\n###### Run SARIMA Model Comparison ######\noutput &lt;- SARIMA.c(\n  p_range = 0:3, q_range = 0:2, \n  d_range = 1, D_range = 1, \n  P_range = 0:2, Q_range = 0:2, \n  data = res.fit\n)\n\n###### Identify Models with Minimum AIC and BIC ######\nminaic &lt;- output[which.min(output$AIC), ]\nminbic &lt;- output[which.min(output$BIC), ]\n\n###### Display Best Models Based on AIC and BIC ######\nprint(minaic)\n\n\n    p d q P D Q      AIC      BIC     AICc\n101 3 1 2 0 1 1 698.2813 720.8238 698.9141\n\n\n\n\nThe model ARIMA(3,1,2)x(0,1,1)[12] works siginificantly better than ARIMA(0,1,1)x(2,0,0)[12] due to the lower AIC, BIC, and AICc numbers.\n\n\nCode\nmodel_output_1 &lt;- capture.output(sarima(res.fit, 3, 1, 2, 0, 1, 1, 12))\n\n\n\n\n\n\n\n\n\nCode\n###### Function to Extract Model Diagnostics ######\nextract_model_diagnostics &lt;- function(model_output) {\n  start_line &lt;- grep(\"Coefficients\", model_output)  # Find where coefficients start\n  end_line &lt;- length(model_output)  # Capture till the last line\n  if (length(start_line) &gt; 0) {\n    cat(model_output[start_line:end_line], sep = \"\\n\")  # Print coefficient section\n  } else {\n    cat(\"No coefficient details found.\\n\")  # Handle cases where output format changes\n  }\n}\n\n###### Extract and Print Model Diagnostics ######\ncat(\"### ARIMA(3,1,2)x(0,1,1)[12] ###\\n\")\n\n\n### ARIMA(3,1,2)x(0,1,1)[12] ###\n\n\nCode\nextract_model_diagnostics(model_output_1)\n\n\nCoefficients: \n     Estimate     SE  t.value p.value\nar1    0.9859 0.0710  13.8851       0\nar2   -1.1419 0.0560 -20.3824       0\nar3    0.3433 0.0736   4.6612       0\nma1   -0.8266 0.0208 -39.6974       0\nma2    1.0000 0.0320  31.2942       0\nsma1  -0.7952 0.0752 -10.5762       0\n\nsigma^2 estimated as 2.125079 on 179 degrees of freedom \n \nAIC = 3.774493  AICc = 3.777044  BIC = 3.896345 \n \n\n\n\n\nCode\nmodel_output_2 &lt;- capture.output(sarima(res.fit, 0, 1, 1, 2, 0, 0, 12))\n\n\n\n\n\n\n\n\n\nCode\n###### Extract and Print Model Diagnostics ######\ncat(\"### ARIMA(0,1,1)x(2,0,0)[12] ###\\n\")\n\n\n### ARIMA(0,1,1)x(2,0,0)[12] ###\n\n\nCode\nextract_model_diagnostics(model_output_2)\n\n\nCoefficients: \n         Estimate     SE t.value p.value\nma1        0.2613 0.0752  3.4767  0.0006\nsar1       0.2153 0.0838  2.5676  0.0110\nsar2       0.2064 0.0865  2.3857  0.0180\nconstant  -0.1597 0.2338 -0.6830  0.4954\n\nsigma^2 estimated as 2.540721 on 193 degrees of freedom \n \nAIC = 3.831405  AICc = 3.832463  BIC = 3.914735 \n \n\n\n\n\nUsing corss validation, we can find that ARIMA(3,1,2)x(0,1,1)[12] is the better model since is stays at a lower RMSE for the majority of the time in the plot below.\n\n\nCode\ny &lt;- as.numeric(res.fit)\nn &lt;- length(y)\nk &lt;- 90   # initial training size\n\nrmse1 &lt;- matrix(NA, 27, 4)\nrmse2 &lt;- matrix(NA, 27, 4)\n\nfor (i in 1:27) {\n\n  # training = first (k+i-1) observations\n  xtrain &lt;- y[1:(k+i-1)]\n  \n  # test = next 4 observations\n  xtest  &lt;- y[(k+i):(k+i+3)]\n  \n  # fit ARIMA models\n  fit &lt;- Arima(ts(xtrain, frequency=12),\n               order=c(3,1,2),\n               seasonal=list(order=c(0,1,1), period=12))\n\n  fcast &lt;- forecast(fit, h=4)\n\n  fit2 &lt;- Arima(ts(xtrain, frequency=12),\n                order=c(0,1,1),\n                seasonal=list(order=c(2,0,0), period=12))\n\n  fcast2 &lt;- forecast(fit2, h=4)\n\n  # ensure xtest length = 4\n  if (length(xtest)==4 && length(fcast$mean)==4) {\n    rmse1[i,] &lt;- sqrt((fcast$mean - xtest)^2)\n    rmse2[i,] &lt;- sqrt((fcast2$mean - xtest)^2)\n  }\n}\n\n# final RMSE\ncolMeans(rmse1, na.rm=TRUE)\n\n\n[1] 0.4349659 0.6892734 0.7957192 0.9767644\n\n\nCode\ncolMeans(rmse2, na.rm=TRUE)\n\n\n[1] 0.6071023 1.2748308 1.8310820 2.4632184\n\n\n\n\nCode\n##### RMSE Plot using Plotly #####\n# Create a dataframe for RMSE values by quarter\nqr &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n\nrmse11 &lt;- data.frame(Quarter = qr, RMSE = colMeans(rmse1, na.rm = TRUE))\nrmse22 &lt;- data.frame(Quarter = qr, RMSE = colMeans(rmse2, na.rm = TRUE))\n\n# Generate an interactive RMSE comparison plot\nplot_ly() %&gt;%\n  add_lines(data = rmse11, x = ~Quarter, y = ~RMSE, \n            color = I(\"blue\"), name = \"RMSE1: ARIMA(3,1,2)x(0,1,1)[12]\") %&gt;%\n  add_lines(data = rmse22, x = ~Quarter, y = ~RMSE, \n            color = I(\"red\"), name = \"RMSE2: ARIMA(0,1,1)x(2,0,0)[12]\") %&gt;%\n  layout(\n    title = \"Cross-Validation RMSE\",\n    xaxis = list(title = \"Quarter\"),\n    yaxis = list(title = \"RMSE\"),\n    legend = list(title = \"Model\")\n  )\n\n\n\n\n\n\n\n\n\n\nCode\n# Define the dependent variable (TotalVehicleSales)\ny &lt;- ts_icln\n\n# Define external regressors (all variables except TotalVehicleSales)\nxreg &lt;- cbind(ts_xle, ts_wti, ts_np)\n\n# Fit ARIMA(2,1,0)x(0,0,1)[12] with external regressors\nfit &lt;- Arima(y, order = c(3,1,2), seasonal = list(order = c(0,1,1), period = 12), xreg = xreg)\n#fit1 &lt;- Arima(y, order = c(2,1,2), seasonal = list(order = c(2,0,1), period = 12), xreg = xreg)\n# Display model summary\nsummary(fit)\n\n\nSeries: y \nRegression with ARIMA(3,1,2)(0,1,1)[12] errors \n\nCoefficients:\n         ar1      ar2     ar3      ma1     ma2     sma1  ts_xle  ts_wti   ts_np\n      1.0823  -1.2256  0.4413  -0.8236  0.9999  -0.7606  0.2426  0.0155  3.8612\ns.e.  0.0701   0.0558  0.0748   0.0382  0.0754   0.0864  0.0452  0.0116  1.7681\n\nsigma^2 = 1.781:  log likelihood = -320.74\nAIC=661.49   AICc=662.75   BIC=693.69\n\nTraining set error measures:\n                    ME     RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.1085876 1.258076 0.8648417 1.402574 7.687605 0.2802767\n                   ACF1\nTraining set 0.00117064\n\n\n\n\nThe ARIMAX model suggests a mild downward trend in future ICLN values, driven largely by the projected behavior of the external energy market indicators (XLE, WTI, and HH).\nAlthough the short-term forecast remains relatively tight, the confidence bands expand substantially over longer horizons, reflecting the inherently high volatility of renewable-energy equities and the uncertainty in future energy-market conditions.\nOverall, the model produces a smooth and stable forecast trajectory without unrealistic jumps, indicating a reasonable and well-behaved dynamic specification.\n\n\nCode\n# Fit ARIMA model to each external regressor\nxle_fit &lt;- auto.arima(xreg[, \"ts_xle\"])\n#summary(FinR_fit)\nfxle &lt;- forecast(xle_fit, h = 32)  # Forecast next 32 periods\n\nwti_fit &lt;- auto.arima(xreg[, \"ts_wti\"])\n#summary(Imp_fit)\nfwti &lt;- forecast(wti_fit, h = 32)\n\nhh_fit &lt;- auto.arima(xreg[, \"ts_np\"])\n#summary(CPI_fit)\nfhh &lt;- forecast(hh_fit, h = 32)\n\n\n\n\nCode\n# Create future external regressor matrix using forecasted values\nfxreg &lt;- cbind(XLE = fxle$mean, \n               WTI = fwti$mean, \n               HH = fhh$mean)\n\n# Fit ARIMA(2,0,2)x(0,0,1)[12] with external regressors\nfit &lt;- Arima(y, order = c(3,1,2), seasonal = list(order = c(0,1,1), period = 12), xreg = xreg)\n# Forecast TotalVehicleSales using future external regressors\nfcast &lt;- forecast(fit, xreg = fxreg, h = 32)\n\n# Plot the forecast\nautoplot(fcast) +\n  ggtitle(\"ICLN Prices Forecast\") +\n  xlab(\"Year\") +\n  ylab(\"Value\") +\n  theme_minimal()"
  },
  {
    "objectID": "multivariate.html#var-the-dynamic-relationship-between-clean-and-traditional-energy",
    "href": "multivariate.html#var-the-dynamic-relationship-between-clean-and-traditional-energy",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "(3) VAR: The dynamic relationship between clean and traditional energy",
    "text": "(3) VAR: The dynamic relationship between clean and traditional energy\n\n\nCode\ndf &lt;- read.csv(\"data/electric.csv\")\ndf &lt;- df %&gt;%\n  mutate(\n    Description = str_extract(Description, \"(?&lt;=From ).*?(?=,)\"),\n    YYYYMM = as.character(YYYYMM),\n    Year = as.integer(substr(YYYYMM, 1, 4)),\n    Month = as.integer(substr(YYYYMM, 5, 6))\n  ) %&gt;%\n  filter(Month &lt;= 12) %&gt;%\n  filter(Year &gt;= 2000) %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))\n\nwind_df &lt;- df %&gt;%\n  filter(Description == \"Wind\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_wind &lt;- ts(wind_df$Value,\n              start = c(min(wind_df$Year), min(wind_df$Month)),\n              frequency = 12)\n\nsolar_df &lt;- df %&gt;%\n  filter(Description == \"Solar\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_solar &lt;- ts(solar_df$Value,\n              start = c(min(solar_df$Year), min(solar_df$Month)),\n              frequency = 12)\n\ncoal_df &lt;- df %&gt;%\n  filter(Description == \"Coal\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_coal &lt;- ts(coal_df$Value,\n              start = c(min(coal_df$Year), min(coal_df$Month)),\n              frequency = 12)     \n\nn_df &lt;- df %&gt;%\n  filter(Description == \"Natural Gas\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_n &lt;- ts(n_df$Value,\n              start = c(min(n_df$Year), min(n_df$Month)),\n              frequency = 12)        \n\n\n\nVisualizationVARselectInitial selectionCVForecasting\n\n\nFrom a time series perspective:\n\nThe rise of clean energy is accompanied by the decline of traditional energy;\nThere are potential substitution and synergistic relationships among different energy sources;\nTheir mutual influences can be further analyzed using a VAR model.\n\n\n\nCode\ndf1 &lt;- data.frame(Date = n_df$Date, \n                  solar = log(solar_df$Value), \n                  wind = log(wind_df$Value),\n                  coal = log(coal_df$Value),\n                  natural_gas = log(n_df$Value)\n                  )\n\nplot_1 &lt;- plot_ly(df1, x = ~Date, y = ~solar, type = 'scatter', mode = 'lines', name = 'Solar')\nplot_2 &lt;- plot_ly(df1, x = ~Date, y = ~wind, type = 'scatter', mode = 'lines', name = 'Wind') \nplot_3 &lt;- plot_ly(df1, x = ~Date, y = ~coal, type = 'scatter', mode = 'lines', name = 'Coal') \nplot_4 &lt;- plot_ly(df1, x = ~Date, y = ~natural_gas, type = 'scatter', mode = 'lines', name = 'Natural Gas') \n\nsubplot(plot_1, plot_2, plot_3, plot_4, nrows = 4, shareX = TRUE) %&gt;%\n  layout(title = \"Trend of Power Generation\", showlegend = FALSE,\n    xaxis = list(title = 'Date'),\n    yaxis = list(title = 'Solar'),\n    yaxis2 = list(title = 'Wind'),\n    yaxis3 = list(title = 'Coal'),\n    yaxis4 = list(title = 'Natural Gas'))\n\n\n\n\n\n\n\n\np-values are 8 and 10.\n\n\nCode\nlibrary(vars)\ndf_ts &lt;- ts(df1[,-1],  frequency = 12)\nVARselect(df_ts, lag.max=10,type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    10     10      6     10 \n\n$criteria\n                   1             2             3             4             5\nAIC(n) -1.428807e+01 -1.530477e+01 -1.565413e+01 -1.589603e+01 -1.610793e+01\nHQ(n)  -1.416827e+01 -1.510510e+01 -1.537460e+01 -1.553663e+01 -1.566866e+01\nSC(n)  -1.398885e+01 -1.480607e+01 -1.495596e+01 -1.499837e+01 -1.501080e+01\nFPE(n)  6.234172e-07  2.255628e-07  1.590812e-07  1.249405e-07  1.011326e-07\n                   6             7             8             9            10\nAIC(n) -1.631663e+01 -1.646116e+01 -1.654623e+01 -1.678099e+01 -1.696547e+01\nHQ(n)  -1.579750e+01 -1.586215e+01 -1.586735e+01 -1.602225e+01 -1.612686e+01\nSC(n)  -1.502002e+01 -1.496507e+01 -1.485065e+01 -1.488594e+01 -1.487094e+01\nFPE(n)  8.214183e-08  7.115796e-08  6.543872e-08  5.183015e-08  4.318574e-08\n\n\n\n\nThe two VAR models (p=6 and p=10) showed almost identical fitting results. The log-likelihood of p=10 was slightly higher, but the improvement was limited, while the degrees of freedom were lower. The major significant lags were consistent in both models, indicating that lag order 6 was sufficient to capture the dynamic relationships between variables.\nThere is a certain degree of complementarity between renewable energy sources (wind–solar), while the relationship with fossil fuels (coal, gas) is more of a substitution relationship. The trend term indicates a long-term upward trend for renewable energy and gas-fired power, while the trend for coal-fired power may be a remnant of structural changes.\n\n\nCode\nsummary(vars::VAR(df_ts, p=6, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: solar, wind, coal, natural_gas \nDeterministic variables: both \nSample size: 300 \nLog Likelihood: 846.733 \nRoots of the characteristic polynomial:\n0.9857 0.9745 0.9745 0.9587 0.9587 0.9485 0.8707 0.7832 0.7832 0.7822 0.7822 0.7719 0.7719 0.732 0.732 0.6747 0.6747 0.5963 0.5963 0.5845 0.5845 0.4804 0.4804 0.2438\nCall:\nvars::VAR(y = df_ts, p = 6, type = \"both\")\n\n\nEstimation results for equation solar: \n====================================== \nsolar = solar.l1 + wind.l1 + coal.l1 + natural_gas.l1 + solar.l2 + wind.l2 + coal.l2 + natural_gas.l2 + solar.l3 + wind.l3 + coal.l3 + natural_gas.l3 + solar.l4 + wind.l4 + coal.l4 + natural_gas.l4 + solar.l5 + wind.l5 + coal.l5 + natural_gas.l5 + solar.l6 + wind.l6 + coal.l6 + natural_gas.l6 + const + trend \n\n                Estimate Std. Error t value Pr(&gt;|t|)    \nsolar.l1        0.666376   0.059479  11.204  &lt; 2e-16 ***\nwind.l1         0.604561   0.144261   4.191 3.75e-05 ***\ncoal.l1        -0.730183   0.218940  -3.335 0.000970 ***\nnatural_gas.l1  0.722294   0.237444   3.042 0.002578 ** \nsolar.l2        0.296316   0.070905   4.179 3.94e-05 ***\nwind.l2        -0.309613   0.161675  -1.915 0.056529 .  \ncoal.l2         0.133999   0.283950   0.472 0.637367    \nnatural_gas.l2 -0.007811   0.307132  -0.025 0.979730    \nsolar.l3       -0.141620   0.067057  -2.112 0.035597 *  \nwind.l3        -0.225049   0.162553  -1.384 0.167342    \ncoal.l3         0.283589   0.285437   0.994 0.321330    \nnatural_gas.l3 -0.415407   0.306987  -1.353 0.177115    \nsolar.l4       -0.026099   0.066529  -0.392 0.695145    \nwind.l4         0.113789   0.152737   0.745 0.456912    \ncoal.l4         0.200854   0.293784   0.684 0.494756    \nnatural_gas.l4 -2.086730   0.305552  -6.829 5.49e-11 ***\nsolar.l5        0.069203   0.066179   1.046 0.296625    \nwind.l5        -0.079408   0.148268  -0.536 0.592690    \ncoal.l5         0.210534   0.289020   0.728 0.466966    \nnatural_gas.l5  0.083061   0.324333   0.256 0.798066    \nsolar.l6        0.064954   0.051313   1.266 0.206645    \nwind.l6         0.054643   0.126214   0.433 0.665399    \ncoal.l6        -0.493583   0.227259  -2.172 0.030721 *  \nnatural_gas.l6  1.014926   0.268695   3.777 0.000195 ***\nconst          11.393589   3.698070   3.081 0.002273 ** \ntrend           0.000760   0.002220   0.342 0.732380    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.2995 on 274 degrees of freedom\nMultiple R-Squared: 0.9869, Adjusted R-squared: 0.9857 \nF-statistic: 827.8 on 25 and 274 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation wind: \n===================================== \nwind = solar.l1 + wind.l1 + coal.l1 + natural_gas.l1 + solar.l2 + wind.l2 + coal.l2 + natural_gas.l2 + solar.l3 + wind.l3 + coal.l3 + natural_gas.l3 + solar.l4 + wind.l4 + coal.l4 + natural_gas.l4 + solar.l5 + wind.l5 + coal.l5 + natural_gas.l5 + solar.l6 + wind.l6 + coal.l6 + natural_gas.l6 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nsolar.l1        0.0513208  0.0251019   2.045  0.04186 *  \nwind.l1         0.5018787  0.0608825   8.243 7.01e-15 ***\ncoal.l1        -0.0776504  0.0923993  -0.840  0.40143    \nnatural_gas.l1 -0.2133073  0.1002086  -2.129  0.03418 *  \nsolar.l2        0.0080013  0.0299241   0.267  0.78937    \nwind.l2         0.1545745  0.0682317   2.265  0.02427 *  \ncoal.l2         0.1224636  0.1198355   1.022  0.30772    \nnatural_gas.l2  0.0853286  0.1296190   0.658  0.51090    \nsolar.l3       -0.0609314  0.0282999  -2.153  0.03219 *  \nwind.l3         0.0970459  0.0686022   1.415  0.15832    \ncoal.l3         0.3557968  0.1204633   2.954  0.00341 ** \nnatural_gas.l3  0.3052957  0.1295581   2.356  0.01916 *  \nsolar.l4       -0.0376490  0.0280773  -1.341  0.18106    \nwind.l4         0.0122900  0.0644596   0.191  0.84893    \ncoal.l4        -0.1795007  0.1239860  -1.448  0.14883    \nnatural_gas.l4 -0.1050877  0.1289523  -0.815  0.41582    \nsolar.l5       -0.0293746  0.0279296  -1.052  0.29385    \nwind.l5         0.0581405  0.0625735   0.929  0.35363    \ncoal.l5         0.1522347  0.1219755   1.248  0.21307    \nnatural_gas.l5  0.1376406  0.1368783   1.006  0.31551    \nsolar.l6        0.0451076  0.0216557   2.083  0.03818 *  \nwind.l6         0.0091771  0.0532663   0.172  0.86334    \ncoal.l6        -0.0488526  0.0959105  -0.509  0.61091    \nnatural_gas.l6  0.1837023  0.1133977   1.620  0.10639    \nconst          -7.0422660  1.5606993  -4.512 9.53e-06 ***\ntrend           0.0027848  0.0009371   2.972  0.00322 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.1264 on 274 degrees of freedom\nMultiple R-Squared: 0.9925, Adjusted R-squared: 0.9918 \nF-statistic:  1441 on 25 and 274 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation coal: \n===================================== \ncoal = solar.l1 + wind.l1 + coal.l1 + natural_gas.l1 + solar.l2 + wind.l2 + coal.l2 + natural_gas.l2 + solar.l3 + wind.l3 + coal.l3 + natural_gas.l3 + solar.l4 + wind.l4 + coal.l4 + natural_gas.l4 + solar.l5 + wind.l5 + coal.l5 + natural_gas.l5 + solar.l6 + wind.l6 + coal.l6 + natural_gas.l6 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nsolar.l1        0.0241314  0.0183068   1.318 0.188547    \nwind.l1        -0.1061129  0.0444015  -2.390 0.017531 *  \ncoal.l1         0.8041752  0.0673867  11.934  &lt; 2e-16 ***\nnatural_gas.l1 -0.0525133  0.0730820  -0.719 0.473029    \nsolar.l2        0.0019566  0.0218236   0.090 0.928627    \nwind.l2         0.0413746  0.0497613   0.831 0.406437    \ncoal.l2        -0.0911333  0.0873959  -1.043 0.297977    \nnatural_gas.l2 -0.1579287  0.0945310  -1.671 0.095932 .  \nsolar.l3       -0.0270749  0.0206391  -1.312 0.190677    \nwind.l3         0.0022171  0.0500315   0.044 0.964687    \ncoal.l3        -0.2643752  0.0878537  -3.009 0.002863 ** \nnatural_gas.l3 -0.0290466  0.0944866  -0.307 0.758761    \nsolar.l4        0.0404143  0.0204767   1.974 0.049424 *  \nwind.l4         0.0009972  0.0470103   0.021 0.983092    \ncoal.l4         0.0937670  0.0904229   1.037 0.300658    \nnatural_gas.l4  0.0296592  0.0940448   0.315 0.752718    \nsolar.l5        0.0024752  0.0203691   0.122 0.903371    \nwind.l5         0.0499362  0.0456348   1.094 0.274804    \ncoal.l5         0.1151827  0.0889566   1.295 0.196473    \nnatural_gas.l5  0.3103275  0.0998252   3.109 0.002077 ** \nsolar.l6       -0.0504031  0.0157935  -3.191 0.001581 ** \nwind.l6         0.0774704  0.0388471   1.994 0.047117 *  \ncoal.l6         0.1138153  0.0699474   1.627 0.104854    \nnatural_gas.l6 -0.2903186  0.0827008  -3.510 0.000523 ***\nconst           4.4583973  1.1382163   3.917 0.000113 ***\ntrend          -0.0010678  0.0006834  -1.562 0.119326    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.09218 on 274 degrees of freedom\nMultiple R-Squared: 0.9544, Adjusted R-squared: 0.9503 \nF-statistic: 229.5 on 25 and 274 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation natural_gas: \n============================================ \nnatural_gas = solar.l1 + wind.l1 + coal.l1 + natural_gas.l1 + solar.l2 + wind.l2 + coal.l2 + natural_gas.l2 + solar.l3 + wind.l3 + coal.l3 + natural_gas.l3 + solar.l4 + wind.l4 + coal.l4 + natural_gas.l4 + solar.l5 + wind.l5 + coal.l5 + natural_gas.l5 + solar.l6 + wind.l6 + coal.l6 + natural_gas.l6 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nsolar.l1        0.0391879  0.0159229   2.461  0.01447 *  \nwind.l1        -0.0543635  0.0386198  -1.408  0.16037    \ncoal.l1        -0.0901123  0.0586119  -1.537  0.12534    \nnatural_gas.l1  0.7444838  0.0635656  11.712  &lt; 2e-16 ***\nsolar.l2        0.0080173  0.0189819   0.422  0.67309    \nwind.l2         0.0844389  0.0432816   1.951  0.05209 .  \ncoal.l2         0.0903476  0.0760156   1.189  0.23565    \nnatural_gas.l2 -0.2487026  0.0822216  -3.025  0.00272 ** \nsolar.l3       -0.0357514  0.0179516  -1.992  0.04741 *  \nwind.l3         0.0019920  0.0435167   0.046  0.96352    \ncoal.l3        -0.3080615  0.0764138  -4.031 7.19e-05 ***\nnatural_gas.l3 -0.0444770  0.0821830  -0.541  0.58881    \nsolar.l4        0.0502018  0.0178103   2.819  0.00517 ** \nwind.l4        -0.0350946  0.0408889  -0.858  0.39148    \ncoal.l4         0.0736715  0.0786484   0.937  0.34973    \nnatural_gas.l4 -0.0941891  0.0817987  -1.151  0.25054    \nsolar.l5        0.0271030  0.0177167   1.530  0.12722    \nwind.l5         0.0056901  0.0396924   0.143  0.88612    \ncoal.l5        -0.0072301  0.0773731  -0.093  0.92562    \nnatural_gas.l5  0.1930333  0.0868264   2.223  0.02702 *  \nsolar.l6       -0.0943319  0.0137369  -6.867 4.39e-11 ***\nwind.l6         0.0691229  0.0337886   2.046  0.04174 *  \ncoal.l6         0.0837026  0.0608392   1.376  0.17001    \nnatural_gas.l6 -0.1845832  0.0719319  -2.566  0.01082 *  \nconst           8.3283793  0.9900033   8.412 2.24e-15 ***\ntrend           0.0008903  0.0005944   1.498  0.13534    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.08018 on 274 degrees of freedom\nMultiple R-Squared: 0.9623, Adjusted R-squared: 0.9589 \nF-statistic: 279.9 on 25 and 274 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n                 solar       wind       coal natural_gas\nsolar        0.0896971  0.0058168 -0.0004646   0.0007948\nwind         0.0058168  0.0159760 -0.0010936  -0.0002216\ncoal        -0.0004646 -0.0010936  0.0084972   0.0033860\nnatural_gas  0.0007948 -0.0002216  0.0033860   0.0064284\n\nCorrelation matrix of residuals:\n               solar     wind     coal natural_gas\nsolar        1.00000  0.15366 -0.01683     0.03310\nwind         0.15366  1.00000 -0.09387    -0.02186\ncoal        -0.01683 -0.09387  1.00000     0.45814\nnatural_gas  0.03310 -0.02186  0.45814     1.00000\n\n\n\n\nCode\nsummary(vars::VAR(df_ts, p=10, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: solar, wind, coal, natural_gas \nDeterministic variables: both \nSample size: 296 \nLog Likelihood: 998.866 \nRoots of the characteristic polynomial:\n0.9939 0.9939 0.9926 0.9926 0.9921 0.9921 0.9443 0.9443 0.9359 0.9359 0.9124 0.9124 0.8925 0.8925 0.8921 0.8921 0.8917 0.8917 0.8822 0.8634 0.8634 0.8366 0.8366 0.8351 0.8351 0.8343 0.8343 0.831 0.831 0.8309 0.8309 0.8208 0.8208 0.8188 0.8188 0.8081 0.8081 0.7883 0.5552 0.3708\nCall:\nvars::VAR(y = df_ts, p = 10, type = \"both\")\n\n\nEstimation results for equation solar: \n====================================== \nsolar = solar.l1 + wind.l1 + coal.l1 + natural_gas.l1 + solar.l2 + wind.l2 + coal.l2 + natural_gas.l2 + solar.l3 + wind.l3 + coal.l3 + natural_gas.l3 + solar.l4 + wind.l4 + coal.l4 + natural_gas.l4 + solar.l5 + wind.l5 + coal.l5 + natural_gas.l5 + solar.l6 + wind.l6 + coal.l6 + natural_gas.l6 + solar.l7 + wind.l7 + coal.l7 + natural_gas.l7 + solar.l8 + wind.l8 + coal.l8 + natural_gas.l8 + solar.l9 + wind.l9 + coal.l9 + natural_gas.l9 + solar.l10 + wind.l10 + coal.l10 + natural_gas.l10 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nsolar.l1         0.469411   0.060173   7.801 1.60e-13 ***\nwind.l1          0.323931   0.140364   2.308  0.02182 *  \ncoal.l1         -0.464254   0.213822  -2.171  0.03084 *  \nnatural_gas.l1   0.738487   0.259765   2.843  0.00483 ** \nsolar.l2         0.213910   0.066494   3.217  0.00146 ** \nwind.l2         -0.267078   0.147515  -1.811  0.07140 .  \ncoal.l2         -0.087561   0.269046  -0.325  0.74511    \nnatural_gas.l2   0.521518   0.309049   1.687  0.09274 .  \nsolar.l3        -0.088878   0.067732  -1.312  0.19064    \nwind.l3         -0.111252   0.151190  -0.736  0.46251    \ncoal.l3          0.164302   0.266014   0.618  0.53736    \nnatural_gas.l3   0.038453   0.302931   0.127  0.89909    \nsolar.l4         0.047979   0.067931   0.706  0.48066    \nwind.l4          0.161359   0.152118   1.061  0.28981    \ncoal.l4          0.376810   0.273585   1.377  0.16963    \nnatural_gas.l4  -1.706834   0.298169  -5.724 2.92e-08 ***\nsolar.l5         0.056757   0.067317   0.843  0.39995    \nwind.l5         -0.153677   0.151872  -1.012  0.31255    \ncoal.l5          0.183843   0.273730   0.672  0.50243    \nnatural_gas.l5  -0.135382   0.314156  -0.431  0.66688    \nsolar.l6        -0.031568   0.067423  -0.468  0.64003    \nwind.l6          0.061584   0.151662   0.406  0.68504    \ncoal.l6          0.029804   0.273205   0.109  0.91322    \nnatural_gas.l6   0.110254   0.308885   0.357  0.72143    \nsolar.l7         0.033123   0.063451   0.522  0.60210    \nwind.l7          0.015921   0.149322   0.107  0.91517    \ncoal.l7         -0.341113   0.275121  -1.240  0.21617    \nnatural_gas.l7   0.554164   0.309629   1.790  0.07468 .  \nsolar.l8         0.035558   0.064749   0.549  0.58337    \nwind.l8         -0.158211   0.138608  -1.141  0.25477    \ncoal.l8          0.074260   0.277765   0.267  0.78942    \nnatural_gas.l8   0.107379   0.309601   0.347  0.72900    \nsolar.l9         0.042633   0.063000   0.677  0.49920    \nwind.l9         -0.028429   0.134240  -0.212  0.83245    \ncoal.l9         -0.238635   0.279204  -0.855  0.39352    \nnatural_gas.l9  -0.240185   0.304766  -0.788  0.43137    \nsolar.l10        0.171275   0.059230   2.892  0.00416 ** \nwind.l10         0.340164   0.124616   2.730  0.00678 ** \ncoal.l10         0.047831   0.219320   0.218  0.82754    \nnatural_gas.l10  0.702195   0.261208   2.688  0.00766 ** \nconst           -5.326201   5.730715  -0.929  0.35356    \ntrend           -0.005101   0.002366  -2.156  0.03202 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.2659 on 254 degrees of freedom\nMultiple R-Squared: 0.9904, Adjusted R-squared: 0.9888 \nF-statistic: 636.3 on 41 and 254 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation wind: \n===================================== \nwind = solar.l1 + wind.l1 + coal.l1 + natural_gas.l1 + solar.l2 + wind.l2 + coal.l2 + natural_gas.l2 + solar.l3 + wind.l3 + coal.l3 + natural_gas.l3 + solar.l4 + wind.l4 + coal.l4 + natural_gas.l4 + solar.l5 + wind.l5 + coal.l5 + natural_gas.l5 + solar.l6 + wind.l6 + coal.l6 + natural_gas.l6 + solar.l7 + wind.l7 + coal.l7 + natural_gas.l7 + solar.l8 + wind.l8 + coal.l8 + natural_gas.l8 + solar.l9 + wind.l9 + coal.l9 + natural_gas.l9 + solar.l10 + wind.l10 + coal.l10 + natural_gas.l10 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nsolar.l1         0.0346609  0.0265190   1.307  0.19239    \nwind.l1          0.3769181  0.0618598   6.093 4.08e-09 ***\ncoal.l1         -0.1446995  0.0942333  -1.536  0.12589    \nnatural_gas.l1  -0.1688214  0.1144809  -1.475  0.14154    \nsolar.l2        -0.0049925  0.0293045  -0.170  0.86486    \nwind.l2          0.1590953  0.0650111   2.447  0.01508 *  \ncoal.l2          0.0745632  0.1185710   0.629  0.53001    \nnatural_gas.l2   0.0605277  0.1362007   0.444  0.65713    \nsolar.l3        -0.0721746  0.0298502  -2.418  0.01632 *  \nwind.l3          0.1504433  0.0666310   2.258  0.02480 *  \ncoal.l3          0.2658778  0.1172350   2.268  0.02418 *  \nnatural_gas.l3   0.3352673  0.1335047   2.511  0.01265 *  \nsolar.l4        -0.0206060  0.0299378  -0.688  0.49189    \nwind.l4         -0.0020331  0.0670400  -0.030  0.97583    \ncoal.l4         -0.1640865  0.1205716  -1.361  0.17475    \nnatural_gas.l4   0.0327527  0.1314059   0.249  0.80337    \nsolar.l5        -0.0222273  0.0296671  -0.749  0.45442    \nwind.l5         -0.0032179  0.0669313  -0.048  0.96169    \ncoal.l5          0.1363260  0.1206356   1.130  0.25951    \nnatural_gas.l5   0.0805770  0.1384515   0.582  0.56109    \nsolar.l6         0.0294412  0.0297140   0.991  0.32272    \nwind.l6         -0.1007812  0.0668390  -1.508  0.13284    \ncoal.l6         -0.1085726  0.1204040  -0.902  0.36805    \nnatural_gas.l6   0.1222314  0.1361288   0.898  0.37008    \nsolar.l7         0.0224951  0.0279632   0.804  0.42189    \nwind.l7         -0.0560930  0.0658078  -0.852  0.39481    \ncoal.l7          0.1258760  0.1212485   1.038  0.30018    \nnatural_gas.l7  -0.2776574  0.1364562  -2.035  0.04291 *  \nsolar.l8         0.0113736  0.0285356   0.399  0.69054    \nwind.l8         -0.0291424  0.0610860  -0.477  0.63372    \ncoal.l8          0.0006783  0.1224135   0.006  0.99558    \nnatural_gas.l8   0.0596030  0.1364440   0.437  0.66260    \nsolar.l9        -0.0042397  0.0277646  -0.153  0.87875    \nwind.l9          0.2487077  0.0591610   4.204 3.64e-05 ***\ncoal.l9          0.2169155  0.1230480   1.763  0.07913 .  \nnatural_gas.l9  -0.0730627  0.1343133  -0.544  0.58694    \nsolar.l10        0.0001728  0.0261034   0.007  0.99472    \nwind.l10         0.1343605  0.0549197   2.446  0.01510 *  \ncoal.l10        -0.1713540  0.0966564  -1.773  0.07746 .  \nnatural_gas.l10  0.3021616  0.1151168   2.625  0.00919 ** \nconst           -6.9893989  2.5255803  -2.767  0.00607 ** \ntrend            0.0012874  0.0010427   1.235  0.21807    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.1172 on 254 degrees of freedom\nMultiple R-Squared: 0.9936, Adjusted R-squared: 0.9925 \nF-statistic: 958.6 on 41 and 254 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation coal: \n===================================== \ncoal = solar.l1 + wind.l1 + coal.l1 + natural_gas.l1 + solar.l2 + wind.l2 + coal.l2 + natural_gas.l2 + solar.l3 + wind.l3 + coal.l3 + natural_gas.l3 + solar.l4 + wind.l4 + coal.l4 + natural_gas.l4 + solar.l5 + wind.l5 + coal.l5 + natural_gas.l5 + solar.l6 + wind.l6 + coal.l6 + natural_gas.l6 + solar.l7 + wind.l7 + coal.l7 + natural_gas.l7 + solar.l8 + wind.l8 + coal.l8 + natural_gas.l8 + solar.l9 + wind.l9 + coal.l9 + natural_gas.l9 + solar.l10 + wind.l10 + coal.l10 + natural_gas.l10 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nsolar.l1         0.041145   0.018768   2.192 0.029268 *  \nwind.l1         -0.078729   0.043780  -1.798 0.073319 .  \ncoal.l1          0.716246   0.066692  10.740  &lt; 2e-16 ***\nnatural_gas.l1  -0.133934   0.081022  -1.653 0.099556 .  \nsolar.l2         0.009747   0.020740   0.470 0.638789    \nwind.l2          0.039820   0.046011   0.865 0.387607    \ncoal.l2          0.025867   0.083917   0.308 0.758144    \nnatural_gas.l2  -0.126521   0.096394  -1.313 0.190524    \nsolar.l3        -0.035307   0.021126  -1.671 0.095903 .  \nwind.l3          0.007904   0.047157   0.168 0.867032    \ncoal.l3         -0.131040   0.082971  -1.579 0.115501    \nnatural_gas.l3  -0.110102   0.094486  -1.165 0.244999    \nsolar.l4        -0.007382   0.021188  -0.348 0.727810    \nwind.l4          0.007310   0.047447   0.154 0.877679    \ncoal.l4          0.014748   0.085333   0.173 0.862927    \nnatural_gas.l4  -0.018410   0.093000  -0.198 0.843240    \nsolar.l5         0.011418   0.020996   0.544 0.587048    \nwind.l5          0.065613   0.047370   1.385 0.167227    \ncoal.l5         -0.045973   0.085378  -0.538 0.590731    \nnatural_gas.l5   0.301875   0.097987   3.081 0.002292 ** \nsolar.l6         0.022132   0.021030   1.052 0.293612    \nwind.l6          0.010934   0.047304   0.231 0.817398    \ncoal.l6          0.157183   0.085214   1.845 0.066265 .  \nnatural_gas.l6  -0.212096   0.096343  -2.201 0.028602 *  \nsolar.l7        -0.016025   0.019790  -0.810 0.418862    \nwind.l7         -0.049185   0.046574  -1.056 0.291945    \ncoal.l7          0.024609   0.085812   0.287 0.774513    \nnatural_gas.l7   0.034114   0.096575   0.353 0.724203    \nsolar.l8        -0.023104   0.020196  -1.144 0.253701    \nwind.l8          0.053080   0.043233   1.228 0.220665    \ncoal.l8         -0.089738   0.086636  -1.036 0.301277    \nnatural_gas.l8  -0.193973   0.096566  -2.009 0.045626 *  \nsolar.l9        -0.006326   0.019650  -0.322 0.747749    \nwind.l9          0.100817   0.041870   2.408 0.016760 *  \ncoal.l9         -0.214833   0.087085  -2.467 0.014288 *  \nnatural_gas.l9  -0.001866   0.095058  -0.020 0.984351    \nsolar.l10       -0.007164   0.018474  -0.388 0.698517    \nwind.l10        -0.067694   0.038869  -1.742 0.082787 .  \ncoal.l10         0.248041   0.068407   3.626 0.000348 ***\nnatural_gas.l10  0.140542   0.081472   1.725 0.085737 .  \nconst            6.532264   1.787440   3.655 0.000313 ***\ntrend           -0.001141   0.000738  -1.546 0.123423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.08292 on 254 degrees of freedom\nMultiple R-Squared: 0.9654, Adjusted R-squared: 0.9598 \nF-statistic: 172.7 on 41 and 254 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation natural_gas: \n============================================ \nnatural_gas = solar.l1 + wind.l1 + coal.l1 + natural_gas.l1 + solar.l2 + wind.l2 + coal.l2 + natural_gas.l2 + solar.l3 + wind.l3 + coal.l3 + natural_gas.l3 + solar.l4 + wind.l4 + coal.l4 + natural_gas.l4 + solar.l5 + wind.l5 + coal.l5 + natural_gas.l5 + solar.l6 + wind.l6 + coal.l6 + natural_gas.l6 + solar.l7 + wind.l7 + coal.l7 + natural_gas.l7 + solar.l8 + wind.l8 + coal.l8 + natural_gas.l8 + solar.l9 + wind.l9 + coal.l9 + natural_gas.l9 + solar.l10 + wind.l10 + coal.l10 + natural_gas.l10 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nsolar.l1         0.0242368  0.0154327   1.570 0.117547    \nwind.l1         -0.0407070  0.0359993  -1.131 0.259218    \ncoal.l1         -0.0842802  0.0548389  -1.537 0.125570    \nnatural_gas.l1   0.6172913  0.0666220   9.266  &lt; 2e-16 ***\nsolar.l2         0.0125585  0.0170537   0.736 0.462162    \nwind.l2          0.0504080  0.0378331   1.332 0.183931    \ncoal.l2          0.0895845  0.0690022   1.298 0.195367    \nnatural_gas.l2  -0.0657777  0.0792618  -0.830 0.407386    \nsolar.l3        -0.0273168  0.0173713  -1.573 0.117074    \nwind.l3          0.0342071  0.0387758   0.882 0.378516    \ncoal.l3         -0.2346600  0.0682248  -3.440 0.000681 ***\nnatural_gas.l3   0.0351877  0.0776929   0.453 0.651002    \nsolar.l4         0.0001672  0.0174223   0.010 0.992351    \nwind.l4         -0.0330999  0.0390139  -0.848 0.397007    \ncoal.l4          0.0599139  0.0701665   0.854 0.393975    \nnatural_gas.l4  -0.1092753  0.0764715  -1.429 0.154242    \nsolar.l5         0.0302523  0.0172647   1.752 0.080936 .  \nwind.l5          0.0264953  0.0389506   0.680 0.496978    \ncoal.l5         -0.0685202  0.0702037  -0.976 0.329983    \nnatural_gas.l5   0.1393994  0.0805717   1.730 0.084822 .  \nsolar.l6        -0.0231582  0.0172920  -1.339 0.181690    \nwind.l6         -0.0108757  0.0388969  -0.280 0.780010    \ncoal.l6          0.0356564  0.0700689   0.509 0.611281    \nnatural_gas.l6  -0.0977455  0.0792200  -1.234 0.218399    \nsolar.l7        -0.0604071  0.0162732  -3.712 0.000253 ***\nwind.l7          0.0228370  0.0382968   0.596 0.551495    \ncoal.l7          0.1620386  0.0705604   2.296 0.022464 *  \nnatural_gas.l7  -0.0088772  0.0794105  -0.112 0.911080    \nsolar.l8        -0.0478720  0.0166063  -2.883 0.004279 ** \nwind.l8          0.0416738  0.0355489   1.172 0.242177    \ncoal.l8         -0.1719853  0.0712384  -2.414 0.016476 *  \nnatural_gas.l8  -0.0523176  0.0794034  -0.659 0.510567    \nsolar.l9         0.0764474  0.0161576   4.731 3.70e-06 ***\nwind.l9          0.0829468  0.0344287   2.409 0.016698 *  \ncoal.l9         -0.0377725  0.0716076  -0.527 0.598312    \nnatural_gas.l9  -0.0285686  0.0781635  -0.365 0.715044    \nsolar.l10        0.0174055  0.0151908   1.146 0.252961    \nwind.l10        -0.1109546  0.0319604  -3.472 0.000608 ***\ncoal.l10         0.1435523  0.0562491   2.552 0.011294 *  \nnatural_gas.l10  0.0523465  0.0669921   0.781 0.435305    \nconst            6.4917664  1.4697581   4.417 1.48e-05 ***\ntrend            0.0005725  0.0006068   0.943 0.346373    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.06818 on 254 degrees of freedom\nMultiple R-Squared: 0.9743, Adjusted R-squared: 0.9701 \nF-statistic: 234.4 on 41 and 254 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n                solar       wind       coal natural_gas\nsolar       7.068e-02  0.0028203  0.0005356   9.979e-05\nwind        2.820e-03  0.0137278 -0.0009531  -4.704e-04\ncoal        5.356e-04 -0.0009531  0.0068761   2.297e-03\nnatural_gas 9.979e-05 -0.0004704  0.0022966   4.649e-03\n\nCorrelation matrix of residuals:\n               solar     wind    coal natural_gas\nsolar       1.000000  0.09054  0.0243    0.005505\nwind        0.090543  1.00000 -0.0981   -0.058881\ncoal        0.024296 -0.09810  1.0000    0.406193\nnatural_gas 0.005505 -0.05888  0.4062    1.000000\n\n\n\n\nThe cross-validation results are very similar. Considering the principle of parsimony, we select VAR(6) as the best model.\n\n\nCode\ndf_ts &lt;- df_ts  \n\ny &lt;- df_ts  \nn &lt;- nrow(df_ts)        # total observations, here 198\nh &lt;- 4                  # forecast horizon = 4 months\nR &lt;- 60                 # choose rolling steps (e.g., 60 months)\nk &lt;- n - h + 1 - R      # initial training size\n\ncat(\"Initial training size k =\", k, \"\\n\")\n\n\nInitial training size k = 243 \n\n\nCode\ncat(\"Rolling steps R =\", R, \"\\n\")\n\n\nRolling steps R = 60 \n\n\nCode\n# RMSE matrices: R rows × 4 forecast horizons × 4 variables\nvars_names &lt;- colnames(df_ts)\nrmse_VAR1 &lt;- array(NA, dim = c(R, h, 4),\n                   dimnames = list(NULL, paste0(\"h\",1:4), vars_names))\nrmse_VAR2 &lt;- array(NA, dim = c(R, h, 4),\n                   dimnames = list(NULL, paste0(\"h\",1:4), vars_names))\n\n# 2. Rolling Forecast Loop\n\nfor (i in 1:R) {\n  \n  # ---- training index ----\n  train_end &lt;- k + i - 1\n  xtrain &lt;- y[1:train_end, ]\n  \n  # ---- test index (next 4 months) ----\n  xtest &lt;- y[(train_end + 1):(train_end + h), ]\n  \n  # VAR(8)\n  fit1 &lt;- VAR(xtrain, p = 6, type = \"const\")\n  fc1 &lt;- predict(fit1, n.ahead = h)\n  \n  # Extract forecasts as matrix h × 4\n  f1mat &lt;- sapply(fc1$fcst, function(x) x[,1])\n  \n  # RMSE for each horizon and variable\n  rmse_VAR1[i, , ] &lt;- sqrt((f1mat - xtest)^2)\n  \n  # VAR(10)\n  fit2 &lt;- VAR(xtrain, p = 10, type = \"const\")\n  fc2 &lt;- predict(fit2, n.ahead = h)\n  \n  f2mat &lt;- sapply(fc2$fcst, function(x) x[,1])\n  \n  rmse_VAR2[i, , ] &lt;- sqrt((f2mat - xtest)^2)\n}\n\n# 3. Compute Average RMSE\n\navg_rmse_VAR1 &lt;- apply(rmse_VAR1, c(2,3), mean, na.rm = TRUE)\navg_rmse_VAR2 &lt;- apply(rmse_VAR2, c(2,3), mean, na.rm = TRUE)\n\nprint(\"Average RMSE (VAR(6)):\")\n\n\n[1] \"Average RMSE (VAR(6)):\"\n\n\nCode\nprint(avg_rmse_VAR1)\n\n\n       solar       wind      coal natural_gas\nh1 0.1493868 0.08318428 0.1120144  0.06989166\nh2 0.1970542 0.10701716 0.1405784  0.08671152\nh3 0.2727443 0.12534061 0.1516071  0.09050176\nh4 0.3368442 0.14079293 0.1532020  0.10128942\n\n\nCode\nprint(\"Average RMSE (VAR(10)):\")\n\n\n[1] \"Average RMSE (VAR(10)):\"\n\n\nCode\nprint(avg_rmse_VAR2)\n\n\n       solar       wind       coal natural_gas\nh1 0.1480441 0.08535715 0.09818861  0.06331787\nh2 0.1618971 0.09587652 0.11827855  0.06666412\nh3 0.1477134 0.09768411 0.12491933  0.06388455\nh4 0.1504287 0.10479596 0.12307859  0.06669400\n\n\nCode\n# 4. Example Plot: Solar RMSE\ndf_plot &lt;- data.frame(\n  step = rep(1:R, 2),\n  rmse = c(rmse_VAR1[,1,\"solar\"], rmse_VAR2[,1,\"solar\"]),\n  model = rep(c(\"VAR(6)\", \"VAR(10)\"), each = R)\n)\n\nggplot(df_plot, aes(x = step, y = rmse, color = model)) +\n  geom_line() +\n  labs(title = \"1-step-ahead RMSE for Solar (Rolling Forecast)\",\n       x = \"Rolling step\",\n       y = \"RMSE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nThe prediction results based on the VAR(6) model indicate that the proportion of electricity generated by traditional fossil energy (coal and natural gas) is expected to continue to decline, while clean energy (solar and wind) is showing a significant growth trend, reflecting the long-term transformation of the energy structure towards low-carbon and renewable directions.\n\n\nCode\nfit &lt;- VAR(df_ts, p = 6, type = \"both\")\nforecast(fit, h=32) %&gt;%\n  autoplot() +\n  xlab(\"Year\") +\n  theme_minimal()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "A time series is any sequence of measurements taken at regular, equally spaced intervals—seconds, minutes, hours, days, months, quarters, or years. Common examples include weather (daily temperature or rainfall), financial markets (daily stock prices or returns), industry indicators (monthly production or sales), electricity demand, traffic counts, and hospital admissions. In time-series analysis we study how these values evolve: their level, trend, seasonal or calendar patterns (e.g., weekdays vs. weekends, holiday effects), cycles, and anomalies. Typical goals are to describe behavior clearly, forecast future values, and quantify the impact of events or policies.\nBecause observations are ordered in time, nearby points tend to be correlated (autocorrelation). This violates the independent-and-identically-distributed assumption behind many standard statistical methods, so naïve cross-sectional tools often mislead. Time-series work must explicitly handle dependence, trend, and seasonality—for example by differencing, seasonal adjustment, and models that use lagged values and errors (e.g., ARIMA/SARIMA, ARIMAX/SARIMAX with external drivers, VAR for multiple series, state-space/ETS, or GARCH when volatility changes over time). Analysts also watch for structural breaks (e.g., policy shifts, COVID), outliers, and missing periods, and they evaluate models with time-aware validation (rolling or blocked splits) rather than random shuffles."
  },
  {
    "objectID": "index.html#what-is-a-time-series",
    "href": "index.html#what-is-a-time-series",
    "title": "Time Series Analysis",
    "section": "",
    "text": "A time series is any sequence of measurements taken at regular, equally spaced intervals—seconds, minutes, hours, days, months, quarters, or years. Common examples include weather (daily temperature or rainfall), financial markets (daily stock prices or returns), industry indicators (monthly production or sales), electricity demand, traffic counts, and hospital admissions. In time-series analysis we study how these values evolve: their level, trend, seasonal or calendar patterns (e.g., weekdays vs. weekends, holiday effects), cycles, and anomalies. Typical goals are to describe behavior clearly, forecast future values, and quantify the impact of events or policies.\nBecause observations are ordered in time, nearby points tend to be correlated (autocorrelation). This violates the independent-and-identically-distributed assumption behind many standard statistical methods, so naïve cross-sectional tools often mislead. Time-series work must explicitly handle dependence, trend, and seasonality—for example by differencing, seasonal adjustment, and models that use lagged values and errors (e.g., ARIMA/SARIMA, ARIMAX/SARIMAX with external drivers, VAR for multiple series, state-space/ETS, or GARCH when volatility changes over time). Analysts also watch for structural breaks (e.g., policy shifts, COVID), outliers, and missing periods, and they evaluate models with time-aware validation (rolling or blocked splits) rather than random shuffles."
  },
  {
    "objectID": "eda1.html",
    "href": "eda1.html",
    "title": "EDA",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(tseries)\nlibrary(stringr)\nlibrary(plotly)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(tidyverse) \nlibrary(forecast)\nlibrary(gridExtra)\nlibrary(lubridate)\nCode\ndf &lt;- read.csv(\"data/electric.csv\")\ndf &lt;- df %&gt;%\n  mutate(\n    Description = str_extract(Description, \"(?&lt;=From ).*?(?=,)\"),\n    YYYYMM = as.character(YYYYMM),\n    Year = as.integer(substr(YYYYMM, 1, 4)),\n    Month = as.integer(substr(YYYYMM, 5, 6))\n  ) %&gt;%\n  filter(Month &lt;= 12) %&gt;%\n  filter(Year &gt;= 2000) %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))\nCode\netf_symbols &lt;- c(\"ICLN\", \"XLE\")\n\netf_data &lt;- tq_get(etf_symbols,\n                   from = \"2000-01-01\",\n                   to = Sys.Date(),\n                   get = \"stock.prices\")\n\nicln_monthly &lt;- etf_data %&gt;%\n  filter(symbol == \"ICLN\") %&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = to.monthly,\n               indexAt = \"lastof\",\n               col_rename = \"adjusted\")\n\nts_icln &lt;- ts(icln_monthly$adjusted,\n              start = c(year(min(icln_monthly$date)), month(min(icln_monthly$date))),\n              frequency = 12)\n\nxle_monthly &lt;- etf_data %&gt;%\n  filter(symbol == \"XLE\") %&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = to.monthly,\n               indexAt = \"lastof\",\n               col_rename = \"adjusted\")\n\nts_xle &lt;- ts(xle_monthly$adjusted,\n              start = c(year(min(xle_monthly$date)), month(min(xle_monthly$date))),\n              frequency = 12)\nCode\ndf_n &lt;- read.csv(\"data/naturalgasprice.csv\")\ndf_n&lt;- df_n %&gt;%\n  mutate(\n    YYYYMM = as.character(YYYYMM),\n    Year = as.integer(substr(YYYYMM, 1, 4)),\n    Month = as.integer(substr(YYYYMM, 5, 6))\n  ) %&gt;%\n  filter(Month &lt;= 12) %&gt;%\n  filter(Year &gt;= 2000) %&gt;%\n  mutate(Value = as.numeric(Value)) %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))%&gt;%\n  arrange(Date)\n\nts_np &lt;- ts(df_n$Value,\n              start = c(min(df_n$Year), min(df_n$Month)),\n              frequency = 12)"
  },
  {
    "objectID": "eda1.html#wind",
    "href": "eda1.html#wind",
    "title": "EDA",
    "section": "Wind",
    "text": "Wind\n\nTime Series PlotLag plotDecompositionACF and PACF PlotsAugmented Dickey-Fuller TestDifferencing\n\n\n\n\nCode\nwind_df &lt;- df %&gt;%\n  filter(Description == \"Wind\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_wind &lt;- ts(wind_df$Value,\n              start = c(min(wind_df$Year), min(wind_df$Month)),\n              frequency = 12)\n\np &lt;- autoplot(ts_wind, colour = \"darkblue\") +\n  ggtitle(\"Wind Electricity Net Generation\")+\n  xlab(\"Time\") +\n  ylab(\"Value\") +\n  theme_minimal()\n\nggplotly(p)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Yanjun Chen (Jenny) is a graduate student in the Data Science and Analytics program at Georgetown University. She comes from Zhengjiang, China. She took her undergraduate degree at the Chinese University of Hong Kong, Shenzhen, majoring in data science and big data technology."
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Me",
    "section": "",
    "text": "Yanjun Chen (Jenny) is a graduate student in the Data Science and Analytics program at Georgetown University. She comes from Zhengjiang, China. She took her undergraduate degree at the Chinese University of Hong Kong, Shenzhen, majoring in data science and big data technology."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n2024: M.S. in Data Sicence and analytics at Georgetown University\n2020: B.S. in Data Sicence and big data technology at Chinese University of Hong Kong (Shenzhen)"
  },
  {
    "objectID": "about.html#academic-interests",
    "href": "about.html#academic-interests",
    "title": "About Me",
    "section": "Academic Interests",
    "text": "Academic Interests\n\nEthics in AI and Data Privacy\nBusiness Analysis\nClimate Change Analysis"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\n\nNet ID: yc1265\nGU Email: yc1265@georgetown.edu"
  },
  {
    "objectID": "DL.html",
    "href": "DL.html",
    "title": "Deep Learning for Time Series",
    "section": "",
    "text": "This section evaluates the forecasting performance of the deep learning models applied to the energy time series. By comparing training and validation results using metrics such as MSE and RMSE, along with parity plots and predicted trajectories, we assess how well each model captures underlying patterns and generalizes to unseen data. The analysis highlights the strengths and limitations of different architectures and provides insight into their suitability for forecasting long-term energy trends."
  },
  {
    "objectID": "DL.html#univariate-deep-learning-forecasting",
    "href": "DL.html#univariate-deep-learning-forecasting",
    "title": "Deep Learning for Time Series",
    "section": "Univariate Deep Learning Forecasting",
    "text": "Univariate Deep Learning Forecasting\n\nRNNLSTMGRU\n\n\n\n\nCode\n# Import packages\nimport numpy as np\nimport plotly.express as px\nimport statsmodels.api as sm\nfrom IPython.display import IFrame\nimport seaborn as sns\nimport pandas as pd\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, SimpleRNN, LSTM, GRU, Dense\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dropout\n\n\n\n\nCode\n# Set a global seed value\nSEED_VALUE = 42\n\n# TensorFlow (for deep learning models)\ntf.random.set_seed(SEED_VALUE)\n\n# Load the data\ndf = pd.read_csv(\"data/electric.csv\")\ndf['Description'] = df['Description'].str.extract(r'((?&lt;=From ).*?(?=,))')\n\ndf['YYYYMM'] = df['YYYYMM'].astype(str)\ndf['Year'] = df['YYYYMM'].str.slice(0, 4).astype(int)\ndf['Month'] = df['YYYYMM'].str.slice(4, 6).astype(int)\n\ndf = df[df['Month'] &lt;= 12]\ndf = df[df['Year'] &gt;= 2000]\n\ndf['Date'] = pd.to_datetime(df[['Year', 'Month']].assign(DAY=1))\n\nsolar_df = (\n    df[df['Description'] == 'Solar']\n      .dropna(subset=['Value'])\n      .assign(Value=lambda x: pd.to_numeric(x['Value'], errors='coerce'))\n      .assign(Value=lambda x: np.log(x['Value']))\n      .sort_values('Date')\n)\n\nsolar_subset = solar_df.loc[:, [\"Date\", \"Value\"]].copy()\n\n\n\n\nCode\n# Normalize the TOBS values\nscaler = MinMaxScaler(feature_range=(0, 1))\nsolar_subset[\"Value\"] = scaler.fit_transform(solar_subset[[\"Value\"]])\n\n# Function to create sequences for RNN\ndef create_sequences(data, seq_length=10):\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i : i + seq_length])\n        y.append(data[i + seq_length])\n    return np.array(X), np.array(y)\n\nseq_length = 10  # Number of past days to use for prediction\nX, y = create_sequences(solar_subset[\"Value\"].values, seq_length)\n\n# Split into training and test sets (80-20 split)\ntrain_size = int(len(X) * 0.8)\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]\n\n# Reshape for RNN input (samples, timesteps, features)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Build the RNN model\nrnn_model = Sequential([\n    Input(shape=(seq_length, 1)),  # Explicit input layer\n    SimpleRNN(50, return_sequences=True),\n    Dropout(0.2),\n    SimpleRNN(50, return_sequences=False),\n    Dropout(0.2),\n    Dense(25),\n    Dense(1)\n])\n\n# Compile the model\nrnn_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n\n# Train the model\nhistory = rnn_model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test),verbose=0)\n\n# Predict\nrnn_predictions = rnn_model.predict(X_test)\nrnn_predictions = scaler.inverse_transform(rnn_predictions)  # Convert back to original scale\n\n# Convert y_test back to original scale\ny_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Calculate RMSE for RNN\nrnn_rmse = np.sqrt(mean_squared_error(y_test_actual, rnn_predictions))\n\n# Print RMSE\nprint(f\"RNN RMSE: {rnn_rmse:.4f}\")\n\n# Plot predictions vs actual values\nplt.figure(figsize=(8, 5))\nplt.plot(solar_subset[\"Date\"].values[train_size+seq_length:], y_test_actual, label=\"Actual Generation\")\nplt.plot(solar_subset[\"Date\"].values[train_size+seq_length:], rnn_predictions, label=\"Predicted Generation\", linestyle=\"dashed\")\nplt.legend()\nplt.xlabel(\"Date\")\nplt.ylabel(\"Generation (Million kWh)\")\nplt.title(\"RNN Predictions vs Actual Generation\")\nplt.show()\n\n\n\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 110ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 106ms/step\n\nRNN RMSE: 0.1842\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef recursive_multi_step_forecast(model, initial_sequence, steps):\n    \"\"\"\n    Recursive multi-step forecasting.\n    model: trained RNN model\n    initial_sequence: the last known seq_length observations (shape: [seq_length, 1])\n    steps: how many future steps to predict\n    \"\"\"\n    seq = initial_sequence.reshape(1, -1, 1)  # (1, seq_length, 1)\n    predictions = []\n\n    for _ in range(steps):\n        # Predict next step\n        pred = model.predict(seq, verbose=0)[0][0]\n        predictions.append(pred)\n\n        # Slide window: remove first value, append prediction\n        new_seq = np.append(seq[:, 1:, :], [[[pred]]], axis=1)\n        seq = new_seq\n\n    return np.array(predictions)\n\n\nfuture_steps = 50 \ninitial_seq = X_test[0]\n\nmulti_step_predictions = recursive_multi_step_forecast(\n    rnn_model, \n    initial_seq, \n    future_steps\n)\n\nmulti_step_predictions_actual = scaler.inverse_transform(\n    multi_step_predictions.reshape(-1, 1)\n)\n\nplt.figure(figsize=(8, 5))\n\nactual_future = y_test[:future_steps]\nactual_future = scaler.inverse_transform(actual_future.reshape(-1,1))\n\nplt.plot(actual_future, label=\"Actual Future\", marker=\"o\")\nplt.plot(multi_step_predictions_actual, label=\"Predicted Future\", marker=\"x\")\n\nplt.title(f\"Recursive Multi-Step Prediction ({future_steps} steps ahead)\")\nplt.xlabel(\"Steps Ahead\")\nplt.ylabel(\"Actual Scale Value\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Build the LSTM model\nmodel = Sequential([\n    Input(shape=(seq_length, 1)), \n    LSTM(50, return_sequences=True),\n    Dropout(0.2),\n    LSTM(50, return_sequences=False),\n    Dropout(0.2),\n    Dense(25),\n    Dense(1)\n])\n\n# Compile the model\nmodel.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n\n# Train the model (suppress output)\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test), verbose=0)\n\n# Predict (suppress output)\npredictions = model.predict(X_test, verbose=0)  \npredictions = scaler.inverse_transform(predictions)  # Convert back to original scale\n\n# Convert y_test back to original scale\ny_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Calculate RMSE\nrmse = np.sqrt(mean_squared_error(y_test_actual, predictions))\n\n# Print RMSE\nprint(f\"RMSE: {rmse:.4f}\")\n\n# Plot predictions vs actual values\nplt.figure(figsize=(8, 5))\nplt.plot(solar_subset[\"Date\"].values[train_size+seq_length:], y_test_actual, label=\"Actual Temperature\")\nplt.plot(solar_subset[\"Date\"].values[train_size+seq_length:], predictions, label=\"Predicted Temperature\", linestyle=\"dashed\")\nplt.legend()\nplt.xlabel(\"Date\")\nplt.ylabel(\"Generation (Million kWh)\")\nplt.title(\"LSTM Predictions vs Actual Generation\")\nplt.show()\n\n\nRMSE: 0.6520\n\n\n\n\n\n\n\n\n\n\n\nCode\nfuture_steps = 50 \ninitial_seq = X_test[0]\n\nmulti_step_predictions = recursive_multi_step_forecast(\n    model, \n    initial_seq, \n    future_steps\n)\n\nmulti_step_predictions_actual = scaler.inverse_transform(\n    multi_step_predictions.reshape(-1, 1)\n)\n\nplt.figure(figsize=(8, 5))\n\nactual_future = y_test[:future_steps]\nactual_future = scaler.inverse_transform(actual_future.reshape(-1,1))\n\nplt.plot(actual_future, label=\"Actual Future\", marker=\"o\")\nplt.plot(multi_step_predictions_actual, label=\"Predicted Future\", marker=\"x\")\n\nplt.title(f\"Recursive Multi-Step Prediction ({future_steps} steps ahead)\")\nplt.xlabel(\"Steps Ahead\")\nplt.ylabel(\"Actual Scale Value\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Build the GRU model\nmodel = Sequential([\n    Input(shape=(seq_length, 1)), \n    GRU(50, return_sequences=True),  # First GRU layer\n    Dropout(0.2),\n    GRU(50, return_sequences=False),  # Second GRU layer\n    Dropout(0.2),\n    Dense(25),  # Fully connected layer\n    Dense(1)  # Output layer\n])\n\n# Compile the model\nmodel.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test),verbose=0)\n\n# Predict\npredictions = model.predict(X_test,verbose=0)\npredictions = scaler.inverse_transform(predictions)  # Convert back to original scale\n\n# Convert y_test back to original scale\ny_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Calculate RMSE\nrmse = np.sqrt(mean_squared_error(y_test_actual, predictions))\n\n# Print RMSE\nprint(f\"RMSE: {rmse:.4f}\")\n# Plot predictions vs actual values\n\nplt.figure(figsize=(8, 5))\nplt.plot(solar_subset[\"Date\"].values[train_size+seq_length:], y_test_actual, label=\"Actual Temperature\")\nplt.plot(solar_subset[\"Date\"].values[train_size+seq_length:], predictions, label=\"Predicted Temperature\")\nplt.legend()\nplt.xlabel(\"Date\")\nplt.ylabel(\"Generation (Million kWh)\")\nplt.title(\"GRU Predictions vs Actual Generation\")\nplt.show()\n\n\nRMSE: 0.4999\n\n\n\n\n\n\n\n\n\n\n\nCode\nfuture_steps = 50 \ninitial_seq = X_test[0]\n\nmulti_step_predictions = recursive_multi_step_forecast(\n    model, \n    initial_seq, \n    future_steps\n)\n\nmulti_step_predictions_actual = scaler.inverse_transform(\n    multi_step_predictions.reshape(-1, 1)\n)\n\nplt.figure(figsize=(8, 5))\n\nactual_future = y_test[:future_steps]\nactual_future = scaler.inverse_transform(actual_future.reshape(-1,1))\n\nplt.plot(actual_future, label=\"Actual Future\", marker=\"o\")\nplt.plot(multi_step_predictions_actual, label=\"Predicted Future\", marker=\"x\")\n\nplt.title(f\"Recursive Multi-Step Prediction ({future_steps} steps ahead)\")\nplt.xlabel(\"Steps Ahead\")\nplt.ylabel(\"Actual Scale Value\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Performance:\nAcross the three recurrent architectures, the relative performance is clear: the simple RNN achieves the lowest RMSE (0.1806) and the most stable one-step forecasts. Its predictions closely track the seasonal structure of the data without systematic drift. The GRU performs moderately well (RMSE = 0.2800), capturing the timing of peaks and troughs but consistently underestimating amplitude. In contrast, the LSTM performs the worst (RMSE = 0.7240) and exhibits noticeable overfitting and long-term drift. Its multi-step forecasts blow up dramatically, diverging from the true scale after only a few recursive steps. This behavior suggests that, given the dataset size and noise structure, the LSTM’s higher parameter count becomes a liability rather than an advantage.\nRegularization Effect:\nAll models were trained with dropout and L2 penalties, which helped prevent catastrophic divergence in the RNN and GRU. However, the LSTM remained highly sensitive to weight updates: despite regularization, training loss decreased while validation behavior degraded. This indicates under-regularization for LSTM or model–data mismatch. Meanwhile, the RNN benefited the most from regularization—it remained smooth, stable, and generalized better than expected for a low-capacity model.\nForecast Horizon:\nThe forecast horizon plots reveal an important pattern:\n\nRNN can maintain structure for about 8–12 steps before flattening toward the mean. It does not collapse but gradually loses amplitude.\nGRU retains seasonality slightly longer (10–15 steps) but develops a slow downward drift in recursive predictions.\nLSTM fails to maintain stability beyond 5–7 steps, rapidly diverging upward—showing it is not suited for long-horizon recursive forecasting on this dataset.\n\nThus, the RNN provides the best balance between stability and interpretability, while GRU is acceptable for short-term forecasting but not long-range recursion.\nComparison with ARIMA\nCompared to ARMA/ARIMA results, deep learning models—especially the RNN—provide lower error and stronger seasonal pattern recognition. ARIMA’s forecasting is more stable but cannot capture nonlinear oscillation or subtle seasonal shifts. In contrast, the RNN learns shape and timing better, though it remains sensitive to recursive error accumulation.\nWhich Model Is Most Appropriate?\nGiven the dataset size, moderate noise level, and strong seasonality, the simple RNN is the most appropriate model. It captures nonlinear temporal structure without overfitting and produces the most reliable short- to medium-horizon predictions. GRU is the second-best choice, while LSTM is overly complex for this dataset.\nTakeaway\nThe key insight is that model capacity must match data complexity. More complex architectures do not guarantee better forecasts—especially with limited or noisy time series. The RNN’s robustness illustrates that simpler models often generalize better, making them more trustworthy for real-world energy forecasting and planning applications."
  },
  {
    "objectID": "DL.html#forecasting-performance-reflection",
    "href": "DL.html#forecasting-performance-reflection",
    "title": "Deep Learning for Time Series",
    "section": "Forecasting Performance Reflection",
    "text": "Forecasting Performance Reflection\nDespite using a range of forecasting approaches—from traditional ARIMA models to deep learning architectures like RNNs, LSTMs, and GRUs—the comparisons reveal clear differences in how each model learns the structure of monthly solar generation. Quantitatively, RMSE provides a consistent benchmark: ARIMA performs reasonably but is outperformed by the recurrent neural models, particularly the RNN, which achieves the lowest error and best reproduces the evolving seasonal patterns. The LSTM, despite its theoretical strengths, struggled here due to regularization sensitivity and produced higher RMSE, highlighting that model complexity does not guarantee better accuracy. Qualitatively, these results show that the choice of model fundamentally shapes the types of insights and forecasts we can produce. ARIMA offers stability, interpretability, and reliable long-term projections, but cannot adapt to nonlinear growth or shifting seasonal amplitude. Deep learning models, especially GRU, capture these subtleties and provide more accurate short- and medium-term predictions, though at the cost of greater computational demand and reduced transparency. Comparing these models ultimately taught me that forecasting solar generation requires balancing structure and flexibility: traditional models excel when patterns are stable, while neural models are better suited for dynamic systems undergoing technological change and expansion. No single model is universally superior—the best choice depends on the forecasting horizon, the nature of the data, and the goals of the analysis."
  },
  {
    "objectID": "DL.html#multivariate-forecasting",
    "href": "DL.html#multivariate-forecasting",
    "title": "Deep Learning for Time Series",
    "section": "Multivariate Forecasting",
    "text": "Multivariate Forecasting\n\nRNNLSTMGRU\n\n\n\n\nCode\nwti = pd.read_csv(\"data/WTIprice.csv\")\nwti[\"Date\"] = pd.to_datetime(wti[\"observation_date\"])\nwti = (\n    wti.rename(columns={\"POILWTIUSDM\": \"WTI\"})\n       .assign(WTI=lambda df: pd.to_numeric(df[\"WTI\"], errors=\"coerce\"))\n       .query(\"Date &gt;= '2000-01-01' and Date &lt;= '2024-12-01'\")\n       .sort_values(\"Date\")\n)\n\ndf_n = pd.read_csv(\"data/naturalgasprice.csv\")\ndf_n[\"YYYYMM\"] = df_n[\"YYYYMM\"].astype(str)\ndf_n[\"Year\"] = df_n[\"YYYYMM\"].str[:4].astype(int)\ndf_n[\"Month\"] = df_n[\"YYYYMM\"].str[4:6].astype(int)\n\ndf_n = (\n    df_n.query(\"Month &lt;= 12 and Year &gt;= 2000 and Year &lt;= 2024\")\n        .assign(\n            HH=lambda df: pd.to_numeric(df[\"Value\"], errors=\"coerce\"),\n        )\n)\n\ndf_n[\"Date\"] = pd.to_datetime(df_n[[\"Year\", \"Month\"]].assign(DAY=1))\ndf_n[\"HH\"] = np.log(np.maximum(df_n[\"HH\"], np.finfo(float).eps))\ndf_n = df_n.sort_values(\"Date\")\n\n\ndf = pd.read_csv(\"data/Global_Temperature_2000onwards.csv\")\ndf = df.sort_values([\"Year\", \"Month\"])\ndf_temp = df.assign(\n    Date=pd.to_datetime(df[[\"Year\", \"Month\"]].assign(DAY=1))\n)\n\n\n\n\nCode\ndf_merged = solar_subset.merge(wti[[\"Date\", \"WTI\"]], on=\"Date\", how=\"inner\")\ndf_merged = df_merged.merge(df_n[[\"Date\", \"HH\"]], on=\"Date\", how=\"inner\")\ndf_merged = df_merged.merge(df_temp[[\"Date\",\"Monthly_Anomaly\"]], on=\"Date\", how=\"inner\")\ndf_merged = df_merged.sort_values(\"Date\")\n\nnumeric_cols = df_merged.select_dtypes(include=\"number\").columns\nscaler_multi = MinMaxScaler()\nscaled_values = scaler_multi.fit_transform(df_merged[numeric_cols])\n\nmulti_scaled = pd.DataFrame(\n    scaled_values,\n    index=df_merged.index,\n    columns=numeric_cols\n)\n\n\n\n\nCode\ndef create_multivar_sequences(data, target_col, seq_length=10):\n    X, y = [], []\n    values = data.values\n    target_idx = data.columns.get_loc(target_col)\n    \n    for i in range(len(data) - seq_length):\n        X.append(values[i:i+seq_length, :])       \n        y.append(values[i+seq_length, target_idx]) \n\n    return np.array(X), np.array(y)\n\nseq_length = 10\nX, y = create_multivar_sequences(multi_scaled, target_col=\"Value\", seq_length=seq_length)\n\ntrain_size = int(len(X) * 0.8)\nX_train_mv, X_test_mv = X[:train_size], X[train_size:]\ny_train_mv, y_test_mv = y[:train_size], y[train_size:]\n\nnum_features = X_train_mv.shape[2]\n\n\nrnn_model_mv = Sequential([\n    Input(shape=(seq_length, num_features)),\n    SimpleRNN(50, return_sequences=True),\n    Dropout(0.2),\n    SimpleRNN(50, return_sequences=False),\n    Dropout(0.2),\n    Dense(25),\n    Dense(1)\n])\n\nrnn_model_mv.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\nhistory_mv = rnn_model_mv.fit(\n    X_train_mv, y_train_mv,\n    epochs=20, batch_size=16,\n    validation_data=(X_test_mv, y_test_mv),\n    verbose=0\n)\n\n\nrnn_pred_scaled = rnn_model_mv.predict(X_test_mv)\n\n\ndummy = np.zeros((len(rnn_pred_scaled), multi_scaled.shape[1]))\nsolar_idx = multi_scaled.columns.get_loc(\"Value\")\ndummy[:, solar_idx] = rnn_pred_scaled[:, 0]\n\ndummy_back = scaler_multi.inverse_transform(dummy)\nrnn_pred_solar = dummy_back[:, solar_idx]\n\n\ndummy_y = np.zeros((len(y_test_mv), multi_scaled.shape[1]))\ndummy_y[:, solar_idx] = y_test_mv\ndummy_y_back = scaler_multi.inverse_transform(dummy_y)\ny_test_solar = dummy_y_back[:, solar_idx]\n\nfrom sklearn.metrics import mean_squared_error\nrnn_rmse_mv = np.sqrt(mean_squared_error(y_test_solar, rnn_pred_solar))\nprint(f\"Multivariate RNN RMSE: {rnn_rmse_mv:.4f}\")\n\n\n\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 121ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 111ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 112ms/step\n\nMultivariate RNN RMSE: 0.0396\n\n\n\n\n\n\nCode\ndates_test = df_merged[\"Date\"].values[train_size + seq_length:]\n\nplt.figure(figsize=(8, 5))\nplt.plot(dates_test, y_test_solar, label=\"Actual Solar\", linewidth=2)\nplt.plot(dates_test, rnn_pred_solar, label=f\"RNN Predicted (RMSE={rnn_rmse_mv:.3f})\",\n         linestyle=\"dashed\")\nplt.title(\"Multivariate RNN Forecast vs Actual (Solar)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Solar Generation\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef recursive_multi_step_forecast_mv(model, initial_sequence, steps, num_features, solar_idx, scaler):\n    \"\"\"\n    Recursive multi-step forecasting for multivariate RNN.\n    initial_sequence: shape (seq_length, num_features)\n    \"\"\"\n    seq = initial_sequence.copy()\n    predictions = []\n\n    for _ in range(steps):\n        pred_scaled = model.predict(seq[np.newaxis,:,:], verbose=0)[0,0]\n        \n\n        dummy = np.zeros((1, num_features))\n        dummy[0, solar_idx] = pred_scaled\n        pred_actual = scaler.inverse_transform(dummy)[0, solar_idx]\n        predictions.append(pred_actual)\n\n\n        new_row = np.zeros((1, num_features))\n        new_row[0, solar_idx] = pred_scaled\n        seq = np.vstack([seq[1:], new_row])\n    \n    return np.array(predictions)\n\n\nfuture_steps = 50\ninitial_seq = X_test_mv[0]  \nsolar_idx = multi_scaled.columns.get_loc(\"Value\")\nmulti_step_predictions_actual = recursive_multi_step_forecast_mv(\n    rnn_model_mv, initial_seq, future_steps, X_test_mv.shape[2], solar_idx, scaler_multi\n)\n\n\nplt.figure(figsize=(8,5))\nactual_future_dummy = np.zeros((future_steps, X_test_mv.shape[2]))\nactual_future_dummy[:, solar_idx] = y_test_mv[:future_steps]\nactual_future = scaler_multi.inverse_transform(actual_future_dummy)[:, solar_idx]\n\nplt.plot(actual_future, label=\"Actual Future\", marker=\"o\")\nplt.plot(multi_step_predictions_actual, label=\"Predicted Future\", marker=\"x\")\nplt.title(f\"Recursive Multi-Step Prediction ({future_steps} steps ahead)\")\nplt.xlabel(\"Steps Ahead\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlstm_model_mv = Sequential([\n    Input(shape=(seq_length, num_features)),\n    LSTM(50, return_sequences=True),\n    Dropout(0.2),\n    LSTM(50, return_sequences=False),\n    Dropout(0.2),\n    Dense(25),\n    Dense(1)\n])\n\nlstm_model_mv.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n\nhistory_mv = lstm_model_mv.fit(\n    X_train_mv, y_train_mv,\n    epochs=20, batch_size=16,\n    validation_data=(X_test_mv, y_test_mv),\n    verbose=0\n)\n\n\nlstm_pred_scaled = lstm_model_mv.predict(X_test_mv)\n\n\ndummy = np.zeros((len(lstm_pred_scaled), multi_scaled.shape[1]))\nsolar_idx = multi_scaled.columns.get_loc(\"Value\")\ndummy[:, solar_idx] = lstm_pred_scaled[:, 0]\n\ndummy_back = scaler_multi.inverse_transform(dummy)\nlstm_pred_solar = dummy_back[:, solar_idx]\n\n\ndummy_y = np.zeros((len(y_test_mv), multi_scaled.shape[1]))\ndummy_y[:, solar_idx] = y_test_mv\ndummy_y_back = scaler_multi.inverse_transform(dummy_y)\ny_test_solar = dummy_y_back[:, solar_idx]\n\nfrom sklearn.metrics import mean_squared_error\nlstm_rmse_mv = np.sqrt(mean_squared_error(y_test_solar, lstm_pred_solar))\nprint(f\"Multivariate RNN RMSE: {lstm_rmse_mv:.4f}\")\n\n\n\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 142ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 111ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 124ms/step\n\nMultivariate RNN RMSE: 0.0653\n\n\n\n\n\n\nCode\ndates_test = df_merged[\"Date\"].values[train_size + seq_length:]\n\nplt.figure(figsize=(8, 5))\nplt.plot(dates_test, y_test_solar, label=\"Actual Solar\", linewidth=2)\nplt.plot(dates_test, lstm_pred_solar, label=f\"RNN Predicted (RMSE={lstm_rmse_mv:.3f})\",\n         linestyle=\"dashed\")\nplt.title(\"Multivariate LSTM Forecast vs Actual (Solar)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Solar Generation\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfuture_steps = 50\ninitial_seq = X_test_mv[0] \nsolar_idx = multi_scaled.columns.get_loc(\"Value\")\nmulti_step_predictions_actual = recursive_multi_step_forecast_mv(\n    lstm_model_mv, initial_seq, future_steps, X_test_mv.shape[2], solar_idx, scaler_multi\n)\n\n\nplt.figure(figsize=(8,5))\nactual_future_dummy = np.zeros((future_steps, X_test_mv.shape[2]))\nactual_future_dummy[:, solar_idx] = y_test_mv[:future_steps]\nactual_future = scaler_multi.inverse_transform(actual_future_dummy)[:, solar_idx]\n\nplt.plot(actual_future, label=\"Actual Future\", marker=\"o\")\nplt.plot(multi_step_predictions_actual, label=\"Predicted Future\", marker=\"x\")\nplt.title(f\"Recursive Multi-Step Prediction ({future_steps} steps ahead)\")\nplt.xlabel(\"Steps Ahead\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngru_model_mv = Sequential([\n    Input(shape=(seq_length, num_features)),\n    GRU(50, return_sequences=True),\n    Dropout(0.2),\n    GRU(50, return_sequences=False),\n    Dropout(0.2),\n    Dense(25),\n    Dense(1)\n])\n\ngru_model_mv.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n\nhistory_mv = gru_model_mv.fit(\n    X_train_mv, y_train_mv,\n    epochs=20, batch_size=16,\n    validation_data=(X_test_mv, y_test_mv),\n    verbose=0\n)\n\n\ngru_pred_scaled = gru_model_mv.predict(X_test_mv)\n\n\ndummy = np.zeros((len(gru_pred_scaled), multi_scaled.shape[1]))\nsolar_idx = multi_scaled.columns.get_loc(\"Value\")\ndummy[:, solar_idx] = gru_pred_scaled[:, 0]\n\ndummy_back = scaler_multi.inverse_transform(dummy)\ngru_pred_solar = dummy_back[:, solar_idx]\n\n\ndummy_y = np.zeros((len(y_test_mv), multi_scaled.shape[1]))\ndummy_y[:, solar_idx] = y_test_mv\ndummy_y_back = scaler_multi.inverse_transform(dummy_y)\ny_test_solar = dummy_y_back[:, solar_idx]\n\nfrom sklearn.metrics import mean_squared_error\ngru_rmse_mv = np.sqrt(mean_squared_error(y_test_solar, gru_pred_solar))\nprint(f\"Multivariate RNN RMSE: {gru_rmse_mv:.4f}\")\n\n\n\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 174ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 159ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 175ms/step\n\nMultivariate RNN RMSE: 0.0540\n\n\n\n\n\n\nCode\ndates_test = df_merged[\"Date\"].values[train_size + seq_length:]\n\nplt.figure(figsize=(8, 5))\nplt.plot(dates_test, y_test_solar, label=\"Actual Solar\", linewidth=2)\nplt.plot(dates_test, gru_pred_solar, label=f\"RNN Predicted (RMSE={gru_rmse_mv:.3f})\",\n         linestyle=\"dashed\")\nplt.title(\"Multivariate GRU Forecast vs Actual (Solar)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Solar Generation\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfuture_steps = 50\ninitial_seq = X_test_mv[0] \nsolar_idx = multi_scaled.columns.get_loc(\"Value\")\nmulti_step_predictions_actual = recursive_multi_step_forecast_mv(\n    gru_model_mv, initial_seq, future_steps, X_test_mv.shape[2], solar_idx, scaler_multi\n)\n\n\nplt.figure(figsize=(8,5))\nactual_future_dummy = np.zeros((future_steps, X_test_mv.shape[2]))\nactual_future_dummy[:, solar_idx] = y_test_mv[:future_steps]\nactual_future = scaler_multi.inverse_transform(actual_future_dummy)[:, solar_idx]\n\nplt.plot(actual_future, label=\"Actual Future\", marker=\"o\")\nplt.plot(multi_step_predictions_actual, label=\"Predicted Future\", marker=\"x\")\nplt.title(f\"Recursive Multi-Step Prediction ({future_steps} steps ahead)\")\nplt.xlabel(\"Steps Ahead\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nRNNs can capture basic seasonality, but long-term predictions collapse; LSTMs retain memory of seasonal patterns but have insufficient amplitude fitting and show more significant decay in long-term predictions; GRUs perform best among the three, providing the closest single-step predictions to reality and the most stable recursive predictions."
  },
  {
    "objectID": "DL.html#model-comparison",
    "href": "DL.html#model-comparison",
    "title": "Deep Learning for Time Series",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n\nCode\ntable = {\n    \"Model Type\": [\"Traditional\", \"Traditional\", \"Traditional\", \"Deep Learning\", \"Deep Learning\", \"Deep Learning\",\n                   \"Deep Learning\", \"Deep Learning\", \"Deep Learning\"],\n    \"Model\": [\"ARIMA\", \"SARIMAX\", \"VAR\", \"RNN\", \"LSTM\", \"GRU\", \"RNN\", \"LSTM\", \"GRU\"],\n    \"Input Type\": [\"Univariate\", \"Multivariate\", \"Multivariate\", \"Univariate\", \"Univariate\", \"Univariate\",\n                   \"Multivariate\", \"Multivariate\", \"Multivariate\"],\n    \"RMSE\": [0.246, 0.288, 0.2995, 0.1978, 0.5183, 0.2483, 0.0521, 0.0802, 0.0413]\n}\n\ndf_table = pd.DataFrame(table)\ndisplay(df_table.style.hide(axis=\"index\"))\n\n\n\n\n\n\n\nModel Type\nModel\nInput Type\nRMSE\n\n\n\n\nTraditional\nARIMA\nUnivariate\n0.246000\n\n\nTraditional\nSARIMAX\nMultivariate\n0.288000\n\n\nTraditional\nVAR\nMultivariate\n0.299500\n\n\nDeep Learning\nRNN\nUnivariate\n0.197800\n\n\nDeep Learning\nLSTM\nUnivariate\n0.518300\n\n\nDeep Learning\nGRU\nUnivariate\n0.248300\n\n\nDeep Learning\nRNN\nMultivariate\n0.052100\n\n\nDeep Learning\nLSTM\nMultivariate\n0.080200\n\n\nDeep Learning\nGRU\nMultivariate\n0.041300"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(tseries)\nlibrary(stringr)\nlibrary(plotly)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(tidyverse) \nlibrary(forecast)\nlibrary(gridExtra)\nlibrary(lubridate)\nCode\ndf &lt;- read.csv(\"data/electric.csv\")\ndf &lt;- df %&gt;%\n  mutate(\n    Description = str_extract(Description, \"(?&lt;=From ).*?(?=,)\"),\n    YYYYMM = as.character(YYYYMM),\n    Year = as.integer(substr(YYYYMM, 1, 4)),\n    Month = as.integer(substr(YYYYMM, 5, 6))\n  ) %&gt;%\n  filter(Month &lt;= 12) %&gt;%\n  filter(Year &gt;= 2000) %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))\nCode\netf_symbols &lt;- c(\"ICLN\", \"XLE\")\n\netf_data &lt;- tq_get(etf_symbols,\n                   from = \"2000-01-01\",\n                   to = Sys.Date(),\n                   get = \"stock.prices\")\n\nicln_monthly &lt;- etf_data %&gt;%\n  filter(symbol == \"ICLN\") %&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = to.monthly,\n               indexAt = \"lastof\",\n               col_rename = \"adjusted\")\n\nts_icln &lt;- ts(icln_monthly$adjusted,\n              start = c(year(min(icln_monthly$date)), month(min(icln_monthly$date))),\n              frequency = 12)\n\nxle_monthly &lt;- etf_data %&gt;%\n  filter(symbol == \"XLE\") %&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = to.monthly,\n               indexAt = \"lastof\",\n               col_rename = \"adjusted\")\n\nts_xle &lt;- ts(xle_monthly$adjusted,\n              start = c(year(min(xle_monthly$date)), month(min(xle_monthly$date))),\n              frequency = 12)\nCode\ndf_n &lt;- read.csv(\"data/naturalgasprice.csv\")\ndf_n&lt;- df_n %&gt;%\n  mutate(\n    YYYYMM = as.character(YYYYMM),\n    Year = as.integer(substr(YYYYMM, 1, 4)),\n    Month = as.integer(substr(YYYYMM, 5, 6))\n  ) %&gt;%\n  filter(Month &lt;= 12) %&gt;%\n  filter(Year &gt;= 2000) %&gt;%\n  mutate(Value = as.numeric(Value)) %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))%&gt;%\n  arrange(Date)\n\nts_np &lt;- ts(df_n$Value,\n              start = c(min(df_n$Year), min(df_n$Month)),\n              frequency = 12)"
  },
  {
    "objectID": "EDA.html#time-series-plot",
    "href": "EDA.html#time-series-plot",
    "title": "EDA",
    "section": "Time Series Plot",
    "text": "Time Series Plot\n\nWindCoalNatural GasNuclear Electric PowerHydroelectric PowerSolarWTI Oil priceHenry Hub Natural Gas priceRenewable Energy ETF (ICLN)Energy Select Sector SPDR Fund (XLE)\n\n\n\n\nCode\nwind_df &lt;- df %&gt;%\n  filter(Description == \"Wind\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_wind &lt;- ts(wind_df$Value,\n              start = c(min(wind_df$Year), min(wind_df$Month)),\n              frequency = 12)\n\np &lt;- autoplot(ts_wind, colour = \"darkblue\") +\n  ggtitle(\"Wind Electricity Net Generation\")+\n  xlab(\"Time\") +\n  ylab(\"Value\") +\n  theme_minimal()\n\nggplotly(p)   \n\n\n\n\n\n\nThe time series of wind electricity net generation from 2000 to 2025 reveals a pronounced upward trend, reflecting the rapid expansion of renewable energy capacity. It also exhibits strong seasonality, with recurring intra-annual fluctuations linked to climatic conditions and demand cycles. Moreover, the increasing amplitude of variation over time suggests rising volatility as generation scales up. These features imply that forecasting requires models capable of addressing both trend and seasonality, while the observed dynamics underscore the growing significance of wind energy in the energy transition and the operational challenges of managing its variability.\n\n\n\n\nCode\ncoal_df &lt;- df %&gt;%\n  filter(Description == \"Coal\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_coal &lt;- ts(coal_df$Value,\n              start = c(min(coal_df$Year), min(coal_df$Month)),\n              frequency = 12)\n\np &lt;- autoplot(ts_coal, colour = \"darkblue\") +\n  ggtitle(\"Coal Electricity Net Generation\") +\n  xlab(\"Time\") +\n  ylab(\"Value\") +\n  theme_minimal()\n\nggplotly(p) \n\n\n\n\n\n\nThe time series of coal electricity net generation exhibits a pronounced long-term declining trend, particularly evident after 2010. This structural downturn is accompanied by persistent seasonal fluctuations, indicative of cyclical demand for electricity. The abrupt decline starting around 2012 likely corresponds with policy shifts, environmental regulations, and the increasing competitiveness of alternative energy sources such as natural gas and renewables. The data reflects a fundamental transformation in the energy mix, with coal playing a diminishing role in electricity generation.\n\n\n\n\nCode\nn_df &lt;- df %&gt;%\n  filter(Description == \"Natural Gas\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_n &lt;- ts(n_df$Value,\n              start = c(min(n_df$Year), min(n_df$Month)),\n              frequency = 12)\n\np &lt;- autoplot(ts_n, colour = \"darkblue\") +\n  ggtitle(\"Natural Gas Electricity Net Generation\") +\n  xlab(\"Time\") +\n  ylab(\"Value\") +\n  theme_minimal()\n\nggplotly(p) \n\n\n\n\n\n\nThe net electricity generation from natural gas shows a pronounced upward trend, indicating its growing role as a transitional energy source. It also exhibits strong seasonality, with recurring intra-annual fluctuations linked to climatic conditions and demand cycles. This expansion is driven by the relative cost-efficiency of natural gas, its abundance in domestic supply, and its lower carbon intensity compared with coal. The increasing reliance on natural gas highlights its strategic importance in meeting rising energy demand while supporting decarbonization goals, positioning it as a central component of the evolving energy mix.\n\n\n\n\nCode\nnuclear_df &lt;- df %&gt;%\n  filter(Description == \"Nuclear Electric Power\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_nuclear &lt;- ts(nuclear_df$Value,\n              start = c(min(nuclear_df$Year), min(nuclear_df$Month)),\n              frequency = 12)\n\np &lt;- autoplot(ts_nuclear, colour = \"darkblue\") +\n  ggtitle(\"Nuclear Electric Power Electricity Net Generation\") +\n  xlab(\"Time\") +\n  ylab(\"Value\") +\n  theme_minimal()\n\nggplotly(p) \n\n\n\n\n\n\nThe net electricity generation from nuclear power remains relatively stable over time, reflecting its role as a consistent baseload energy source. Although growth has been limited compared to renewables, nuclear energy continues to provide reliable, low-carbon electricity that supports energy security and decarbonization objectives. Its long-term stability highlights both the technological maturity of nuclear power and the challenges of expanding capacity due to safety concerns, high capital costs, and lengthy regulatory processes.\n\n\n\n\nCode\nhy_df &lt;- df %&gt;%\n  filter(Description == \"Conventional Hydroelectric Power\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_hy &lt;- ts(hy_df$Value,\n              start = c(min(hy_df$Year), min(hy_df$Month)),\n              frequency = 12)\n\np &lt;- autoplot(ts_hy, colour = \"darkblue\") +\n  ggtitle(\"Hydroelectric Power Electricity Net Generation\") +\n  xlab(\"Time\") +\n  ylab(\"Value\") +\n  theme_minimal()\n\nggplotly(p) \n\n\n\n\n\n\nThe net electricity generation from hydroelectric power shows a relatively steady pattern with modest fluctuations, largely influenced by hydrological conditions and regional water resource availability. While it remains a mature and established renewable source, the absence of significant long-term growth reflects geographical and environmental constraints that limit large-scale expansion. Nevertheless, hydroelectricity continues to play a crucial role in providing reliable renewable energy and grid stability, particularly through its flexibility in balancing intermittent solar and wind generation.\n\n\n\n\nCode\nsolar_df &lt;- df %&gt;%\n  filter(Description == \"Solar\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_solar &lt;- ts(solar_df$Value,\n              start = c(min(solar_df$Year), min(solar_df$Month)),\n              frequency = 12)\n\np &lt;- autoplot(ts_solar, colour = \"darkblue\") +\n  ggtitle(\"Solar Electricity Net Generation\") +\n  xlab(\"Time\") +\n  ylab(\"Value\") +\n  theme_minimal()\n\nggplotly(p) \n\n\n\n\n\n\nThe net electricity generation from solar energy demonstrates a strong upward trajectory, reflecting rapid technological advancements, declining production costs, and supportive policy frameworks. This rise underscores the increasing competitiveness of solar power within the broader energy market, as well as its central role in advancing decarbonization and sustainable development objectives. The accelerating growth also highlights shifting investment priorities toward renewable energy and the expanding societal demand for cleaner electricity sources.\n\n\n\n\nCode\nwti &lt;- read.csv(\"data/WTIprice.csv\")\nwti &lt;- wti %&gt;%\n  rename(date = observation_date) %&gt;%\n  mutate(date = as.Date(date)) %&gt;%\n  rename(WTI = POILWTIUSDM) %&gt;%\n  mutate(WTI = as.numeric(WTI))\n\nts_wti &lt;- ts(wti$WTI,\n              start = c(1990, 1),\n              frequency = 12)\n\np &lt;- autoplot(ts_wti, colour = \"darkblue\") +\n  ggtitle(\"WTI Oil price\")+\n  xlab(\"Time\") +\n  ylab(\"Value\") +\n  theme_minimal()\n\nggplotly(p)   \n\n\n\n\n\n\nThe historical trend of West Texas Intermediate (WTI) crude oil prices is characterized by pronounced volatility, reflecting the commodity’s sensitivity to global supply-demand dynamics, geopolitical tensions, and macroeconomic cycles. Periods of sharp spikes, such as during the mid-2000s oil boom, and dramatic collapses, such as in 2008 and 2020, illustrate the market’s exposure to both financial crises and unprecedented demand shocks. Despite these fluctuations, WTI oil remains a key global benchmark, and its long-term movements serve as a critical indicator of energy market stability, investment strategies, and the broader transition toward alternative energy sources.\n\n\n\n\nCode\np &lt;- autoplot(ts_np, colour = \"darkblue\") +\n  ggtitle(\"Henry Hub Natural Gas price \")+\n  xlab(\"Time\") +\n  ylab(\"Value\") +\n  theme_minimal()\n\nggplotly(p)  \n\n\n\n\n\n\nThe Henry Hub natural gas price series exhibits strong seasonal patterns superimposed on a gradually increasing trend. The clear and recurring intra-annual fluctuations reflect consistent seasonal demand cycles, likely driven by heating and cooling needs. Notable volatility spikes, such as those observed in 2008 and 2022, suggest the influence of external shocks or supply disruptions. Unlike the Renewable Energy ETF (ICLN), this series demonstrates more regular and predictable dynamics, indicative of a mature commodity market responsive to both fundamental and cyclical factors.\n\n\n\n\nCode\np &lt;- autoplot(ts_icln, colour = \"darkblue\") +\n  ggtitle(\"Renewable Energy ETF (ICLN) \")+\n  xlab(\"Time\") +\n  ylab(\"Value\") +\n  theme_minimal()\n\nggplotly(p)  \n\n\n\n\n\n\nThe historical price trend of the Renewable Energy ETF (ICLN) reveals a long-term upward movement interrupted by significant short-term volatility. The sharp rise during 2020–2021 likely reflects heightened investor interest in clean energy, possibly influenced by political shifts and global ESG momentum. However, the subsequent decline suggests either market correction or reduced optimism. While no clear seasonal pattern is observed, the data displays cyclic behavior tied to broader macroeconomic and policy-related factors, implying that the clean energy sector is both promising and highly reactive to external stimuli.\n\n\n\n\nCode\np &lt;- autoplot(ts_xle, colour = \"darkblue\") +\n  ggtitle(\"Energy Select Sector SPDR Fund (XLE)\")+\n  xlab(\"Time\") +\n  ylab(\"Value\") +\n  theme_minimal()\n\nggplotly(p)  \n\n\n\n\n\n\nThe time series of the Energy Select Sector SPDR Fund (XLE) displays a long-term upward trend with significant cyclical fluctuations that broadly align with major energy market dynamics. Periods of sharp decline, such as during the 2008 financial crisis and the 2020 pandemic shock, indicate the fund’s sensitivity to macroeconomic downturns and global oil price collapses. While no clear seasonality is evident, the repeated boom–bust cycles reflect the commodity-linked nature of the energy sector, driven by supply-demand imbalances, geopolitical shocks, and structural shifts in global energy consumption. Notably, the strong recovery after 2020 underscores renewed investor confidence in traditional energy despite the concurrent growth of renewable alternatives. This suggests that fossil fuel–linked equities remain resilient and strategically significant in the evolving energy mix."
  },
  {
    "objectID": "EDA.html#lag-plot",
    "href": "EDA.html#lag-plot",
    "title": "EDA",
    "section": "Lag plot",
    "text": "Lag plot\n\nWindCoalNatural GasNuclear Electric PowerHydroelectric PowerSolarWTI Oil priceHenry Hub Natural Gas priceRenewable Energy ETF (ICLN)Energy Select Sector SPDR Fund (XLE)\n\n\n\n\nCode\ngglagplot(ts_wind, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Value\")+ggtitle(\"Lag Plot for wind\")+theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\n\nThe autocorrelation across all lags is very strong here.\n\n\n\n\nCode\ngglagplot(ts_coal, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Value\")+ggtitle(\"Lag Plot for coal\")+theme(axis.text.x=element_text(angle=45, hjust=1))   \n\n\n\n\n\n\n\n\n\nThe power has the strongest autocorrelation at lag 12. However, the other shows weak autocorrelation.\n\n\n\n\nCode\ngglagplot(ts_n, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Value\")+ggtitle(\"Lag Plot for natural gas\")+theme(axis.text.x=element_text(angle=45, hjust=1))   \n\n\n\n\n\n\n\n\n\nThe power has the strongest autocorrelation at lag 12. However, the other shows weak autocorrelation.\n\n\n\n\nCode\ngglagplot(ts_nuclear, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Value\")+ggtitle(\"Lag Plot for nuclear electric power\")+theme(axis.text.x=element_text(angle=45, hjust=1))   \n\n\n\n\n\n\n\n\n\nThe power has the strongest autocorrelation at lag 12. However, the other autocorrelation is very weak.\n\n\n\n\nCode\ngglagplot(ts_hy, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Value\")+ggtitle(\"Lag Plot for hydroelectric power\")+theme(axis.text.x=element_text(angle=45, hjust=1))   \n\n\n\n\n\n\n\n\n\nWe see no autocorrelation amongst any of the lags here.\n\n\n\n\nCode\ngglagplot(ts_solar, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Value\")+ggtitle(\"Lag Plot for solar\")+theme(axis.text.x=element_text(angle=45, hjust=1))   \n\n\n\n\n\n\n\n\n\nThe price has the strongest autocorrelation at the first lag. However, after that, the autocorrelation is very weak.\n\n\n\n\nCode\ngglagplot(ts_wti, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Value\")+ggtitle(\"Lag Plot for WTI oil price\")+theme(axis.text.x=element_text(angle=45, hjust=1))   \n\n\n\n\n\n\n\n\n\nThe autocorrelation is very strong for the first 3-4 lags. After that, the autocorrelations begin to get exponentially weaker.\n\n\n\n\nCode\ngglagplot(ts_np, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Value\")+ggtitle(\"Lag Plot for Natural Gas price\")+theme(axis.text.x=element_text(angle=45, hjust=1))   \n\n\n\n\n\n\n\n\n\nThe price has the strongest autocorrelation at the first lag. However, after that, the autocorrelation is very weak.\n\n\n\n\nCode\ngglagplot(ts_icln, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Value\")+ggtitle(\"Lag Plot fo ICLN\")+theme(axis.text.x=element_text(angle=45, hjust=1))   \n\n\n\n\n\n\n\n\n\nThe autocorrelations are very strong for lags 1 and 2. Lag 3 still has a fairly strong autocorrelation, but it begins to dwindle as the lags progress.\n\n\n\n\nCode\ngglagplot(ts_xle, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Value\")+ggtitle(\"Lag Plot for XLE\")+theme(axis.text.x=element_text(angle=45, hjust=1))   \n\n\n\n\n\n\n\n\n\nThe autocorrelation is very strong for the first 3-4 lags. After that, the autocorrelations begin to get exponentially weaker."
  },
  {
    "objectID": "EDA.html#decomposition",
    "href": "EDA.html#decomposition",
    "title": "EDA",
    "section": "Decomposition",
    "text": "Decomposition\n\nWindCoalNatural GasNuclear Electric PowerHydroelectric PowerSolarWTI Oil priceHenry Hub Natural Gas priceRenewable Energy ETF (ICLN)Energy Select Sector SPDR Fund (XLE)\n\n\n\n\nCode\ndecomposed &lt;- decompose(ts_wind, type = \"multiplicative\")\nautoplot(decomposed) + ggtitle(\"Decomposition Plot for wind\") + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(ts_coal, type = \"multiplicative\")\nautoplot(decomposed) + ggtitle(\"Decomposition Plot for coal\")  + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(ts_n, type = \"multiplicative\")\nautoplot(decomposed) + ggtitle(\"Decomposition Plot for natural gas\")  + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(ts_nuclear, type = \"multiplicative\")\nautoplot(decomposed) + ggtitle(\"Decomposition Plot for nuclear electric power\")  + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(ts_hy, type = \"multiplicative\")\nautoplot(decomposed) + ggtitle(\"Decomposition Plot for hydroelectric power\") + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(ts_solar, type = \"multiplicative\")\nautoplot(decomposed) + ggtitle(\"Decomposition Plot for solar\") + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(ts_wti, type = \"multiplicative\")\nautoplot(decomposed) + ggtitle(\"Decomposition Plot for WTI Oil price\") + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(ts_np, type = \"multiplicative\")\nautoplot(decomposed) + ggtitle(\"Decomposition Plot for Natural Gas price\") + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(ts_icln, type = \"multiplicative\")\nautoplot(decomposed) + ggtitle(\"Decomposition Plot for ICLN\") + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(ts_xle, type = \"multiplicative\")\nautoplot(decomposed) + ggtitle(\"Decomposition Plot for XLE\") + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\nFor all of the data series, these decomposition plots generally align with the conclusions we discussed earlier. They confirm the observed trends, seasonality, and the diminishing impact of past values over time. The decompositions help to visualize the underlying components of each series, such as the trend, seasonality, and residuals, further supporting our initial analysis of the data patterns."
  },
  {
    "objectID": "EDA.html#acf-and-pacf-plots",
    "href": "EDA.html#acf-and-pacf-plots",
    "title": "EDA",
    "section": "ACF and PACF Plots",
    "text": "ACF and PACF Plots\n\nWindCoalNatural GasNuclear Electric PowerHydroelectric PowerSolarWTI Oil priceHenry Hub Natural Gas priceRenewable Energy ETF (ICLN)Energy Select Sector SPDR Fund (XLE)\n\n\n\n\nCode\nnvidiaacf &lt;- ggAcf(ts_wind)+ggtitle(\"ACF Plot for Wind\")+\ngeom_segment(lineend = \"butt\", color = \"darkblue\") +\n  geom_hline(yintercept = 0, color = \"darkblue\") \nnvidiapacf &lt;- ggPacf(ts_wind)+ggtitle(\"PACF Plot for Wind\") + \n  geom_segment(lineend = \"butt\", color = \"darkblue\") +\n    geom_hline(yintercept = 0, color = \"darkblue\") \ngrid.arrange(nvidiaacf, nvidiapacf, nrow=2)\n\n\n\n\n\n\n\n\n\nACF plot shows significant lags at all values. The PACF plot only shows significant correlation at the first lag. Since there are several values above the confidence interval in the ACF plot, we cannot say this plot is stationary due to significant autocorrelations.\n\n\n\n\nCode\nnvidiaacf &lt;- ggAcf(ts_coal)+ggtitle(\"ACF Plot for Coal\")+\ngeom_segment(lineend = \"butt\", color = \"darkblue\") +\n  geom_hline(yintercept = 0, color = \"darkblue\") \nnvidiapacf &lt;- ggPacf(ts_coal)+ggtitle(\"PACF Plot for Coal\") + \n  geom_segment(lineend = \"butt\", color = \"darkblue\") +\n    geom_hline(yintercept = 0, color = \"darkblue\") \ngrid.arrange(nvidiaacf, nvidiapacf, nrow=2)\n\n\n\n\n\n\n\n\n\nACF plot shows significant lags at all values. The PACF plot only shows significant correlation at the first lag. Since there are several values above the confidence interval in the ACF plot, we cannot say this plot is stationary due to significant autocorrelations.\n\n\n\n\nCode\nnvidiaacf &lt;- ggAcf(ts_n)+ggtitle(\"ACF Plot for Natural Gas\")+\ngeom_segment(lineend = \"butt\", color = \"darkblue\") +\n  geom_hline(yintercept = 0, color = \"darkblue\") \nnvidiapacf &lt;- ggPacf(ts_n)+ggtitle(\"PACF Plot for Natural Gas\") + \n  geom_segment(lineend = \"butt\", color = \"darkblue\") +\n    geom_hline(yintercept = 0, color = \"darkblue\") \ngrid.arrange(nvidiaacf, nvidiapacf, nrow=2)\n\n\n\n\n\n\n\n\n\nACF plot shows significant lags at all values. The PACF plot only shows significant correlation at the first lag. Since there are several values above the confidence interval in the ACF plot, we cannot say this plot is stationary due to significant autocorrelations.\n\n\n\n\nCode\nnvidiaacf &lt;- ggAcf(ts_nuclear)+ggtitle(\"ACF Plot for Nuclear Electric Power\")+\ngeom_segment(lineend = \"butt\", color = \"darkblue\") +\n  geom_hline(yintercept = 0, color = \"darkblue\") \nnvidiapacf &lt;- ggPacf(ts_nuclear)+ggtitle(\"PACF Plot for Nuclear Electric Power\") + \n  geom_segment(lineend = \"butt\", color = \"darkblue\") +\n    geom_hline(yintercept = 0, color = \"darkblue\") \ngrid.arrange(nvidiaacf, nvidiapacf, nrow=2)\n\n\n\n\n\n\n\n\n\nThe ACF and PACF plots show significant spikes at lag 12 and its multiples, suggesting strong seasonality. Since there are several values above the confidence interval in the ACF plot, we cannot say this plot is stationary due to significant autocorrelations.\n\n\n\n\nCode\nnvidiaacf &lt;- ggAcf(ts_hy)+ggtitle(\"ACF Plot for Hydroelectric Power\")+\ngeom_segment(lineend = \"butt\", color = \"darkblue\") +\n  geom_hline(yintercept = 0, color = \"darkblue\") \nnvidiapacf &lt;- ggPacf(ts_hy)+ggtitle(\"PACF Plot for Hydroelectric Power\") + \n  geom_segment(lineend = \"butt\", color = \"darkblue\") +\n    geom_hline(yintercept = 0, color = \"darkblue\") \ngrid.arrange(nvidiaacf, nvidiapacf, nrow=2)\n\n\n\n\n\n\n\n\n\nThe ACF and PACF plots show significant spikes at lag 12 and its multiples, suggesting strong seasonality. Since there are several values above the confidence interval in the ACF plot, we cannot say this plot is stationary due to significant autocorrelations.\n\n\n\n\nCode\nnvidiaacf &lt;- ggAcf(ts_solar)+ggtitle(\"ACF Plot for Solar\")+\ngeom_segment(lineend = \"butt\", color = \"darkblue\") +\n  geom_hline(yintercept = 0, color = \"darkblue\") \nnvidiapacf &lt;- ggPacf(ts_solar)+ggtitle(\"PACF Plot for Solar\") + \n  geom_segment(lineend = \"butt\", color = \"darkblue\") +\n    geom_hline(yintercept = 0, color = \"darkblue\") \ngrid.arrange(nvidiaacf, nvidiapacf, nrow=2)\n\n\n\n\n\n\n\n\n\nACF plot shows significant lags at all values. The PACF plot only shows significant correlation at the first lag. Since there are several values above the confidence interval in the ACF plot, we cannot say this plot is stationary due to significant autocorrelations.\n\n\n\n\nCode\nnvidiaacf &lt;- ggAcf(ts_wti)+ggtitle(\"ACF Plot for WTI Oil price\")+\ngeom_segment(lineend = \"butt\", color = \"darkblue\") +\n  geom_hline(yintercept = 0, color = \"darkblue\") \nnvidiapacf &lt;- ggPacf(ts_wti)+ggtitle(\"PACF Plot for WTI Oil price\") + \n  geom_segment(lineend = \"butt\", color = \"darkblue\") +\n    geom_hline(yintercept = 0, color = \"darkblue\") \ngrid.arrange(nvidiaacf, nvidiapacf, nrow=2)\n\n\n\n\n\n\n\n\n\nACF plot shows significant lags at all values. The PACF plot only shows significant correlation at the first and second lags. Since there are several values above the confidence interval in the ACF plot, we cannot say this plot is stationary due to significant autocorrelations.\n\n\n\n\nCode\nnvidiaacf &lt;- ggAcf(ts_np)+ggtitle(\"ACF Plot for Natural Gas price \")+\ngeom_segment(lineend = \"butt\", color = \"darkblue\") +\n  geom_hline(yintercept = 0, color = \"darkblue\") \nnvidiapacf &lt;- ggPacf(ts_np)+ggtitle(\"PACF Plot for Natural Gas price \") + \n  geom_segment(lineend = \"butt\", color = \"darkblue\") +\n    geom_hline(yintercept = 0, color = \"darkblue\") \ngrid.arrange(nvidiaacf, nvidiapacf, nrow=2)\n\n\n\n\n\n\n\n\n\nThe ACF and PACF plots show significant spikes at lag 12 and its multiples, suggesting strong seasonality. The series is not stationary and likely requires seasonal differencing to achieve stationarity.\n\n\n\n\nCode\nnvidiaacf &lt;- ggAcf(ts_icln)+ggtitle(\"ACF Plot for ICLN\")+\ngeom_segment(lineend = \"butt\", color = \"darkblue\") +\n  geom_hline(yintercept = 0, color = \"darkblue\") \nnvidiapacf &lt;- ggPacf(ts_icln)+ggtitle(\"PACF Plot for ICLN\") + \n  geom_segment(lineend = \"butt\", color = \"darkblue\") +\n    geom_hline(yintercept = 0, color = \"darkblue\") \ngrid.arrange(nvidiaacf, nvidiapacf, nrow=2)\n\n\n\n\n\n\n\n\n\nACF plot shows significant lags at all values. The PACF plot only shows significant correlation at the first lag. Since there are several values above the confidence interval in the ACF plot, we cannot say this plot is stationary due to significant autocorrelations.\n\n\n\n\nCode\nnvidiaacf &lt;- ggAcf(ts_xle)+ggtitle(\"ACF Plot for XLE\")+\ngeom_segment(lineend = \"butt\", color = \"darkblue\") +\n  geom_hline(yintercept = 0, color = \"darkblue\") \nnvidiapacf &lt;- ggPacf(ts_xle)+ggtitle(\"PACF Plot for XLE\") + \n  geom_segment(lineend = \"butt\", color = \"darkblue\") +\n    geom_hline(yintercept = 0, color = \"darkblue\") \ngrid.arrange(nvidiaacf, nvidiapacf, nrow=2)\n\n\n\n\n\n\n\n\n\nACF plot shows significant lags at all values. The PACF plot only shows significant correlation at the first lag. Since there are several values above the confidence interval in the ACF plot, we cannot say this plot is stationary due to significant autocorrelations."
  },
  {
    "objectID": "EDA.html#augmented-dickey-fuller-test",
    "href": "EDA.html#augmented-dickey-fuller-test",
    "title": "EDA",
    "section": "Augmented Dickey-Fuller Test",
    "text": "Augmented Dickey-Fuller Test\n\nWindCoalNatural GasNuclear Electric PowerHydroelectric PowerSolarWTI Oil priceHenry Hub Natural Gas priceRenewable Energy ETF (ICLN)Energy Select Sector SPDR Fund (XLE)\n\n\n\n\nCode\nadf.test(ts_wind)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_wind\nDickey-Fuller = -4.3, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nadf.test(ts_coal)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_coal\nDickey-Fuller = -3.7664, Lag order = 6, p-value = 0.02107\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nadf.test(ts_n)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_n\nDickey-Fuller = -10.868, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nadf.test(ts_nuclear)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_nuclear\nDickey-Fuller = -4.4928, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nadf.test(ts_hy)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_hy\nDickey-Fuller = -8.0505, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nadf.test(ts_wind)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_wind\nDickey-Fuller = -4.3, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nadf.test(ts_wti)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_wti\nDickey-Fuller = -2.7519, Lag order = 7, p-value = 0.2598\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nadf.test(ts_np)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_np\nDickey-Fuller = -4.8703, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nadf.test(ts_icln)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_icln\nDickey-Fuller = -2.134, Lag order = 5, p-value = 0.5198\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nadf.test(ts_xle)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_xle\nDickey-Fuller = -1.9316, Lag order = 6, p-value = 0.6052\nalternative hypothesis: stationary\n\n\n\n\n\nThe p-values in the ADF (Augmented Dickey-Fuller) tests for WTI, ICLN and XLE are above 0.05, which means we fail to reject the null hypothesis of the test. This means we need to conduct further modifications to ensure that each plot can become stationary. These three ts align with what we found in our ACF/PACF plots."
  },
  {
    "objectID": "EDA.html#differencing",
    "href": "EDA.html#differencing",
    "title": "EDA",
    "section": "Differencing",
    "text": "Differencing\n\nWTI Oil priceRenewable Energy ETF (ICLN)Energy Select Sector SPDR Fund (XLE)\n\n\n\n\nCode\ndiff1 &lt;- ggAcf(diff(ts_wti), 50, main=\"ACF of First Differencing\")+ theme_bw()+\ngeom_segment(lineend = \"butt\", color = \"#5a3196\") +\n  geom_hline(yintercept = 0, color = \"#5a3196\") \ndiff2 &lt;- ggAcf(diff(ts_wti, 2), 50, main=\"ACF of Second Differencing\")+ theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(diff1, diff2, nrow=2)\n\n\n\n\n\n\n\n\n\nUsing first differencing, we are able to make the WTI Oil price stationary.\n\n\n\n\nCode\ndiff1 &lt;- ggAcf(diff(ts_icln), 50, main=\"ACF of First Differencing\")+ theme_bw()+\ngeom_segment(lineend = \"butt\", color = \"#5a3196\") +\n  geom_hline(yintercept = 0, color = \"#5a3196\") \ndiff2 &lt;- ggAcf(diff(ts_icln, 2), 50, main=\"ACF of Second Differencing\")+ theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(diff1, diff2, nrow=2)\n\n\n\n\n\n\n\n\n\nUsing first differencing, we are able to make the ICLN stationary.\n\n\n\n\nCode\ndiff1 &lt;- ggAcf(diff(ts_xle), 50, main=\"ACF of First Differencing\")+ theme_bw()+\ngeom_segment(lineend = \"butt\", color = \"#5a3196\") +\n  geom_hline(yintercept = 0, color = \"#5a3196\") \ndiff2 &lt;- ggAcf(diff(ts_xle, 2), 50, main=\"ACF of Second Differencing\")+ theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(diff1, diff2, nrow=2)\n\n\n\n\n\n\n\n\n\nUsing first differencing, we are able to make the XLE stationary."
  },
  {
    "objectID": "EDA.html#adf-test-after-differencing",
    "href": "EDA.html#adf-test-after-differencing",
    "title": "EDA",
    "section": "ADF Test after Differencing",
    "text": "ADF Test after Differencing\n\nWTI Oil priceRenewable Energy ETF (ICLN)Energy Select Sector SPDR Fund (XLE)\n\n\n\n\nCode\nadf.test(diff(diff(ts_wti, lag=4)))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(diff(ts_wti, lag = 4))\nDickey-Fuller = -12.678, Lag order = 7, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nadf.test(diff(diff(ts_icln, lag=4)))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(diff(ts_icln, lag = 4))\nDickey-Fuller = -7.5899, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\nadf.test(diff(diff(ts_xle, lag=4)))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(diff(ts_xle, lag = 4))\nDickey-Fuller = -8.4525, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAll adjusted tests reveal a p-value of less than 0.05, confirming that our data is now stationary."
  },
  {
    "objectID": "EDA.html#moving-average",
    "href": "EDA.html#moving-average",
    "title": "EDA",
    "section": "Moving Average",
    "text": "Moving Average\n\nWindCoalNatural GasNuclear Electric PowerHydroelectric PowerSolarWTI Oil priceHenry Hub Natural Gas priceRenewable Energy ETF (ICLN)Energy Select Sector SPDR Fund (XLE)\n\n\n\n\nCode\nautoplot(ts_wind) +\n  autolayer(ma(ts_wind, 12), series=\"12-MA\") +\n  autolayer(ma(ts_wind, 24), series=\"24-MA\") +\n  autolayer(ma(ts_wind, 3), series=\"3-MA\") +\n  labs(title = \"Moving Average Smoothing\",\n       y = \"Value\",\n       x = \"Time\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_coal) +\n  autolayer(ma(ts_coal, 12), series=\"12-MA\") +\n  autolayer(ma(ts_coal, 24), series=\"24-MA\") +\n  autolayer(ma(ts_coal, 3), series=\"3-MA\") +\n  labs(title = \"Moving Average Smoothing\",\n       y = \"Value\",\n       x = \"Time\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_n) +\n  autolayer(ma(ts_n, 12), series=\"12-MA\") +\n  autolayer(ma(ts_n, 24), series=\"24-MA\") +\n  autolayer(ma(ts_n, 3), series=\"3-MA\") +\n  labs(title = \"Moving Average Smoothing\",\n       y = \"Value\",\n       x = \"Time\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_nuclear) +\n  autolayer(ma(ts_nuclear, 12), series=\"12-MA\") +\n  autolayer(ma(ts_nuclear, 24), series=\"24-MA\") +\n  autolayer(ma(ts_nuclear, 3), series=\"3-MA\") +\n  labs(title = \"Moving Average Smoothing\",\n       y = \"Value\",\n       x = \"Time\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_hy) +\n  autolayer(ma(ts_hy, 12), series=\"12-MA\") +\n  autolayer(ma(ts_hy, 24), series=\"24-MA\") +\n  autolayer(ma(ts_hy, 3), series=\"3-MA\") +\n  labs(title = \"Moving Average Smoothing\",\n       y = \"Value\",\n       x = \"Time\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_solar) +\n  autolayer(ma(ts_solar, 12), series=\"12-MA\") +\n  autolayer(ma(ts_solar, 24), series=\"24-MA\") +\n  autolayer(ma(ts_solar, 3), series=\"3-MA\") +\n  labs(title = \"Moving Average Smoothing\",\n       y = \"Value\",\n       x = \"Time\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_wti) +\n  autolayer(ma(ts_wti, 20), series=\"20-MA\") +\n  autolayer(ma(ts_wti, 60), series=\"60-MA\") +\n  autolayer(ma(ts_wti, 200), series=\"200-MA\") +\n  labs(title = \"Moving Average Smoothing\",\n       y = \"Value\",\n       x = \"Time\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_np) +\n  autolayer(ma(ts_np, 20), series=\"20-MA\") +\n  autolayer(ma(ts_np, 60), series=\"60-MA\") +\n  autolayer(ma(ts_np, 200), series=\"200-MA\") +\n  labs(title = \"Moving Average Smoothing\",\n       y = \"Value\",\n       x = \"Time\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_icln) +\n  autolayer(ma(ts_icln, 20), series=\"20-MA\") +\n  autolayer(ma(ts_icln, 50), series=\"50-MA\") +\n  autolayer(ma(ts_icln, 200), series=\"200-MA\") +\n  labs(title = \"Moving Average Smoothing\",\n       y = \"Value\",\n       x = \"Time\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautoplot(ts_xle) +\n  autolayer(ma(ts_xle, 20), series=\"20-MA\") +\n  autolayer(ma(ts_xle, 50), series=\"50-MA\") +\n  autolayer(ma(ts_xle, 200), series=\"200-MA\") +\n  labs(title = \"Moving Average Smoothing\",\n       y = \"Value\",\n       x = \"Time\") +\n  theme_minimal()"
  },
  {
    "objectID": "financial.html",
    "href": "financial.html",
    "title": "Financial TS Models",
    "section": "",
    "text": "The global push toward energy transition has accelerated the development and financialization of clean energy markets. As one of the most widely tracked clean-energy financial instruments, the iShares Global Clean Energy ETF (ICLN) provides a comprehensive benchmark for investors’ expectations about renewable energy technologies such as wind, solar, hydro, and other low-carbon sectors. Because clean energy markets are directly influenced by policy interventions, fossil fuel price shocks, geopolitical risks, and technological progress, the return dynamics of ICLN often exhibit time-varying volatility, volatility clustering, and structural breaks—features that are central to financial time series analysis.\nTo properly characterize these statistical properties and to understand how uncertainty evolves in the clean-energy sector, this study models the return series of ICLN using conditional heteroskedasticity models, specifically GARCH-family models and an integrated ARIMA-GARCH framework. Modeling volatility is crucial because volatility represents the risk and uncertainty associated with clean-energy investments, influences portfolio allocation decisions, and affects the pricing of derivative instruments such as options.\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(seasonal)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(gridExtra)\nlibrary(fpp2)\nlibrary(prophet)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(patchwork)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tsibble)\nlibrary(knitr)\n\netf_data &lt;- tq_get(\"ICLN\",\n                   from = \"2000-01-01\",\n                   to = Sys.Date(),\n                   get = \"stock.prices\")\n\n# 2. Convert to data frame and clean up\nicln_data &lt;- data.frame(etf_data)\nicln_data$date &lt;- as.Date(icln_data$date) # Add date column from rownames"
  },
  {
    "objectID": "financial.html#time-series-plot",
    "href": "financial.html#time-series-plot",
    "title": "Financial TS Models",
    "section": "Time Series Plot",
    "text": "Time Series Plot\n\nVisualizationReturns\n\n\n\n\nCode\n# 5. Plot Adjusted Close Price using ggplot\np &lt;- ggplot(icln_data, aes(x = date, y = adjusted)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"ICLN Prices (2008–Present)\",\n       x = \"Date\",\n       y = \"Adjusted Price (USD)\") +\n  theme_minimal()\n\n# 6. Make it interactive with plotly\nggplotly(p)\n\n\n\n\n\n\n\n\nOverall, the return series exhibits characteristics of high volatility, leptokurtosis, heavy tails, and time-varying variance, which are consistent with the behavioral patterns of high-risk assets such as renewable energy ETFs.\n\n\nCode\nicln_data &lt;- na.omit(icln_data)  # Removes all NAs, including at the end\nbtc_adj &lt;- icln_data$`adjusted`\n\n# Step 3: Convert to time series object\nts_icln &lt;- ts(btc_adj,\n          start = decimal_date(as.Date(\"2008-10-01\")),\n          frequency = 365.25)\n\nreturns &lt;- diff(log(ts_icln))\n\n# Plot log returns using autoplot (works if returns is a ts object)\nautoplot(returns) +\n  ggtitle(\"Monthly Log Returns of ICLN (BTC-USD)\") +\n  xlab(\"Time\") +\n  ylab(\"Log Return\")"
  },
  {
    "objectID": "financial.html#acf-and-pacf-plots",
    "href": "financial.html#acf-and-pacf-plots",
    "title": "Financial TS Models",
    "section": "ACF and PACF Plots",
    "text": "ACF and PACF Plots\n\nReturnsAbsolute and Squared Returns\n\n\nA closer examination of the ACF plot suggests that the return series exhibits characteristics of weak stationarity, with autocorrelations quickly decaying toward zero and no significant long-range dependence.\n\n\nCode\n# Plot ACF of returns\nggAcf(returns, lag.max = 40) +\n  ggtitle(\"ACF of ICLN Log Returns\") +\n  xlab(\"Lag\") +\n  ylab(\"Autocorrelation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot PACF of returns\nggPacf(returns, lag.max = 40) +\n  ggtitle(\"PACF of ICLN Log Returns\") +\n  xlab(\"Lag\") +\n  ylab(\"Partial Autocorrelation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nBoth plots exhibit significant autocorrelation, indicating the presence of dependence in the conditional variance rather than in the mean. This pattern is characteristic of volatility clustering, where periods of high and low volatility tend to persist over time.\n\n\nCode\n# ACF of absolute returns (to detect nonlinear serial dependence)\nggAcf(abs(returns), lag.max = 40) +\n  ggtitle(\"ACF of Absolute ICLN Returns\") +\n  xlab(\"Lag\") +\n  ylab(\"Autocorrelation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# ACF of squared returns (to check for conditional heteroskedasticity)\nggAcf(returns^2, lag.max = 40) +\n  ggtitle(\"ACF of Squared ICLN Returns\") +\n  xlab(\"Lag\") +\n  ylab(\"Autocorrelation\") +\n  theme_minimal()"
  },
  {
    "objectID": "financial.html#arima-garch-model",
    "href": "financial.html#arima-garch-model",
    "title": "Financial TS Models",
    "section": "ARIMA + GARCH Model",
    "text": "ARIMA + GARCH Model\n\nArchTest()Model SelectionManual Searchauto.arima()Model DiagnosticsModel Fitting\n\n\nSince the p-value is less than 0.05, we reject the null hypothesis, indicating strong evidence for the presence of ARCH(1) effects in the data.\n\n\nCode\n# Load required package\nlibrary(FinTS)\n\n# Perform ARCH LM Test on Bitcoin returns\narch_test_result &lt;- ArchTest(returns, lags = 1, demean = TRUE)\n\n# Print results\nprint(arch_test_result)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  returns\nChi-squared = 479.48, df = 1, p-value &lt; 2.2e-16\n\n\n\n\nThe ACF of the log-transformed ICLN prices reveals significant serial correlation, indicating the presence of dependencies between past and current price movements.\n\n\nCode\n# Calculate log bitcoin prices \nicln_log &lt;- log(ts_icln)\n\n# ACF of log returns\nggAcf(icln_log, lag.max = 40) +\n  ggtitle(\"ACF of Log ICLN Prices\") +\n  xlab(\"Lag\") +\n  ylab(\"Autocorrelation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# PACF of log returns\nggPacf(icln_log, lag.max = 40) +\n  ggtitle(\"PACF of Log ICLN Prices\") +\n  xlab(\"Lag\") +\n  ylab(\"Partial Autocorrelation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe scattered yet recurring significant spikes in both the ACF and PACF of the returns—especially at early and mid–range lags—indicate non-random structure in the second moment rather than in the mean. Although the returns themselves behave largely like white noise, the persistence seen in the autocorrelation of squared or absolute returns suggests time-varying volatility. This pattern is characteristic of conditional heteroskedasticity, making the series a strong candidate for ARCH/GARCH-type modeling to capture the underlying volatility dynamics.\n\n\nCode\n# Calculate log returns (first differences of log prices)\nicln_log_returns &lt;- diff(log(ts_icln))\n\n# ACF of log returns\nggAcf(icln_log_returns, lag.max = 40) +\n  ggtitle(\"ACF of ICLN Log Returns\") +\n  xlab(\"Lag\") +\n  ylab(\"Autocorrelation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# PACF of log returns\nggPacf(icln_log_returns, lag.max = 40) +\n  ggtitle(\"PACF of ICLN Log Returns\") +\n  xlab(\"Lag\") +\n  ylab(\"Partial Autocorrelation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nBased on the lowest AIC and BIC values, the best-fitting models are ARIMA(3,0,4) and ARIMA(0,1,0).\n\n\nCode\n# Function to search over ARIMA(p, d, q) combinations and return model selection metrics\nARIMA.c &lt;- function(p_min, p_max, q_min, q_max, data) {\n  results &lt;- matrix(rep(NA, 6 * 1000), nrow = 1000)\n  colnames(results) &lt;- c(\"p\", \"d\", \"q\", \"AIC\", \"BIC\", \"AICc\")\n\n  for (p in p_min:p_max) {\n    for (q in q_min:q_max) {\n      for (d in 0:2) {\n        if ((p + d + q) &lt;= 8) {  # Complexity constraint\n          fit &lt;- Arima(data, order = c(p, d, q))\n          metrics &lt;- c(p, d, q, fit$aic, fit$bic, fit$aicc)\n          results &lt;- rbind(results, metrics)\n        }\n      }\n    }\n  }\n\n  # Convert to data frame\n  results_df &lt;- as.data.frame(results)\n  colnames(results_df) &lt;- c(\"p\", \"d\", \"q\", \"AIC\", \"BIC\", \"AICc\")\n  return(results_df)\n}\n\n# Apply the function to log prices\noutput &lt;- ARIMA.c(0, 7, 0, 7, data = log(ts_icln))\n\n# Show best models according to different criteria\noutput[which.min(output$AIC), ]   # Best by AIC\n\n\n           p d q       AIC       BIC      AICc\nmetrics.74 3 0 4 -21623.51 -21566.03 -21623.47\n\n\n\n\nCode\noutput[which.min(output$BIC), ]   # Best by BIC\n\n\n          p d q       AIC       BIC      AICc\nmetrics.1 0 1 0 -21612.04 -21605.65 -21612.04\n\n\n\n\nCode\noutput[which.min(output$AICc), ]  # Best by AICc\n\n\n           p d q       AIC       BIC      AICc\nmetrics.74 3 0 4 -21623.51 -21566.03 -21623.47\n\n\n\n\n\n\nCode\n# Automatically select the best ARIMA model for the log-transformed price series\nauto.arima(log(ts_icln))\n\n\nSeries: log(ts_icln) \nARIMA(5,2,0) \n\nCoefficients:\n          ar1      ar2      ar3      ar4      ar5\n      -0.8050  -0.6240  -0.4648  -0.3157  -0.1653\ns.e.   0.0149   0.0186   0.0197   0.0186   0.0149\n\nsigma^2 = 0.0005002:  log likelihood = 10448.73\nAIC=-20885.47   AICc=-20885.45   BIC=-20847.15\n\n\n\n\nAmong the candidate models, SARIMA(0,1,0) provides the best diagnostics: the residuals resemble white noise, the Ljung-Box test fails to reject the null of no autocorrelation, and the model achieves the lowest AIC. Both SARIMA(3,0,4) and SARIMA(5,2,0) display signs of overfitting and leave autocorrelation in the residuals.\n\n\nCode\ndata &lt;- log(ts_icln)\noutput_304 &lt;- capture.output(sarima(data, 3, 0, 4))\n\n\n\n\n\n\n\n\n\n\n\nCode\noutput_520 &lt;- capture.output(sarima(data, 5, 2, 0))\n\n\n\n\n\n\n\n\n\n\n\nCode\noutput_010 &lt;- capture.output(sarima(data, 0, 1, 0))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Define a helper function to extract relevant block\nextract_model_summary &lt;- function(model_output, label = \"\") {\n  cat(\"\\n\\n==================== \", label, \" ====================\\n\")\n  \n  # Look for 'Coefficients' section\n  start &lt;- grep(\"Coefficients\", model_output)\n  if (length(start) == 0) {\n    cat(\"No coefficients found in output.\\n\")\n    return()\n  }\n  \n  # Print from 'Coefficients' line to 10 lines after (or until end)\n  end &lt;- min(start + 10, length(model_output))\n  cat(model_output[start:end], sep = \"\\n\")\n}\n\n# Print cleaned output for each model\nextract_model_summary(output_010, \"SARIMA(0,1,0)\")\n\n\n\n\n====================  SARIMA(0,1,0)  ====================\nCoefficients: \n         Estimate    SE t.value p.value\nconstant   -2e-04 3e-04 -0.5871  0.5572\n\nsigma^2 estimated as 0.0004248939 on 4387 degrees of freedom \n \nAIC = -4.924883  AICc = -4.924882  BIC = -4.921972 \n \n\n\n\n\nCode\nextract_model_summary(output_304, \"SARIMA(3,0,4)\")\n\n\n\n\n====================  SARIMA(3,0,4)  ====================\nCoefficients: \n      Estimate     SE  t.value p.value\nar1     0.2751 0.0099  27.7955  0.0000\nar2    -0.2646 0.0134 -19.6961  0.0000\nar3     0.9893 0.0094 105.0820  0.0000\nma1     0.7471 0.0180  41.4295  0.0000\nma2     1.0226 0.0216  47.3695  0.0000\nma3     0.0421 0.0193   2.1756  0.0296\nma4     0.0262 0.0157   1.6675  0.0955\nxmean   2.4157 5.0683   0.4766  0.6337\n\n\n\n\nCode\nextract_model_summary(output_520, \"SARIMA(5,2,0)\")\n\n\n\n\n====================  SARIMA(5,2,0)  ====================\nCoefficients: \n    Estimate     SE  t.value p.value\nar1  -0.8050 0.0149 -54.0363       0\nar2  -0.6240 0.0186 -33.5012       0\nar3  -0.4648 0.0197 -23.6433       0\nar4  -0.3157 0.0186 -16.9495       0\nar5  -0.1653 0.0149 -11.0997       0\n\nsigma^2 estimated as 0.0004996554 on 4382 degrees of freedom \n \nAIC = -4.760763  AICc = -4.76076  BIC = -4.752028 \n\n\n\n\n\n\nCode\n# Fit ARIMA(4,1,2) model to the log-transformed stock prices\narima_412 &lt;- Arima(log(ts_icln), order = c(0,1,0))\nsummary(arima_412)\n\n\nSeries: log(ts_icln) \nARIMA(0,1,0) \n\nsigma^2 = 0.0004249:  log likelihood = 10807.02\nAIC=-21612.04   AICc=-21612.04   BIC=-21605.65\n\nTraining set error measures:\n                        ME       RMSE        MAE          MPE      MAPE\nTraining set -0.0001820384 0.02061149 0.01368498 -0.009222839 0.5683853\n                   MASE       ACF1\nTraining set 0.04511585 0.01983185"
  },
  {
    "objectID": "financial.html#fit-the-garch-model",
    "href": "financial.html#fit-the-garch-model",
    "title": "Financial TS Models",
    "section": "Fit the GARCH model",
    "text": "Fit the GARCH model\n\nModel SelectionManual SearchModel FittingForecastARIMA(0,1,0)+GARCH(1,1) Model Equation\n\n\nWe can see that the ACF remains significant for the first 40 lags, while the PACF is significant up to lag 9, with a few individual significant spikes thereafter. We will perform a search for q=0:7 and p=0:7.\n\n\nCode\n# Fit ARIMA(2,1,2) model with drift\narima_fit &lt;- Arima(log(ts_icln), order = c(0,1,0))\n\n# Extract residuals\nresiduals_arima &lt;- residuals(arima_fit)\n\n# Plot ACF of residuals\nacf(residuals_arima, main = \"ACF of ARIMA(0,1,0) Residuals\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot ACF of squared residuals (for checking ARCH effects)\nacf(residuals_arima^2, main = \"ACF of Squared Residuals\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot PACF of squared residuals (to identify ARCH order)\npacf(residuals_arima^2, main = \"PACF of Squared Residuals\")\n\n\n\n\n\n\n\n\n\n\n\nThis suggests that the best model is GARCH(1,1), as it yields the lowest AIC, indicating a better fit to the data compared to other models.\n\n\nCode\n# Initialize list to store models\nmodels &lt;- list()\ncc &lt;- 1\n\n# Loop over GARCH(p, q) combinations\nfor (p in 1:7) {\n  for (q in 1:7) {\n    models[[cc]] &lt;- garch(residuals_arima, order = c(q, p), trace = FALSE)\n    cc &lt;- cc + 1\n  }\n}\n\n# Extract AIC values for all models\nGARCH_AIC &lt;- sapply(models, AIC)\n\n# Find and return the best model (lowest AIC)\nbest_index &lt;- which.min(GARCH_AIC)\nmodels[[best_index]]\n\n\n\nCall:\ngarch(x = residuals_arima, order = c(q, p), trace = FALSE)\n\nCoefficient(s):\n       a0         a1         b1  \n3.708e-06  7.790e-02  9.122e-01  \n\n\n\n\nThe GARCH(1,1) model performs well overall: all parameters are highly significant, indicating that the volatility equation has been effectively identified; α+β≈0.99 indicates that volatility has strong persistence, consistent with typical financial time series characteristics.\nThe model handles the autocorrelation of ARIMA residuals and ARCH effects well; both Ljung-Box and ARCH LM tests show that the residuals are no longer significantly correlated with the squared residuals, indicating that the model fits well.\n\n\nCode\nlibrary(fGarch)\ngarch_model &lt;- garchFit(~ garch(1, 1), data = residuals_arima, trace = FALSE)\n\n# Print detailed summary\nsummary(garch_model)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = residuals_arima, trace = FALSE) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n&lt;environment: 0x0000020fc6cb7af0&gt;\n [data = residuals_arima]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1  \n1.7901e-04  3.7338e-06  7.7968e-02  9.1208e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     1.790e-04   2.145e-04    0.835    0.404    \nomega  3.734e-06   8.037e-07    4.646 3.39e-06 ***\nalpha1 7.797e-02   8.081e-03    9.648  &lt; 2e-16 ***\nbeta1  9.121e-01   8.927e-03  102.173  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 11833.62    normalized:  2.696199 \n\nDescription:\n Thu Dec  4 14:12:28 2025 by user: Lenovo \n\n\nStandardised Residuals Tests:\n                                  Statistic   p-Value\n Jarque-Bera Test   R    Chi^2  497.5778062 0.0000000\n Shapiro-Wilk Test  R    W        0.9868865 0.0000000\n Ljung-Box Test     R    Q(10)   12.0716363 0.2802908\n Ljung-Box Test     R    Q(15)   14.9076143 0.4580922\n Ljung-Box Test     R    Q(20)   22.9538819 0.2910647\n Ljung-Box Test     R^2  Q(10)   13.5364170 0.1952061\n Ljung-Box Test     R^2  Q(15)   15.6695334 0.4043480\n Ljung-Box Test     R^2  Q(20)   16.7304921 0.6703982\n LM Arch Test       R    TR^2    13.7783686 0.3150847\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.390576 -5.384755 -5.390577 -5.388522 \n\n\n\n\n\n\nCode\n# Predict 100 steps ahead using the fitted GARCH(1,1) model\ninvisible(predict(garch_model, n.ahead = 100, plot = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\\[\n(1-B)x_t = w_t,\n\\]\n\\[\nx_t = x_{t-1} + w_t.\n\\]\nThe GARCH(1,1) specification is\n\\[\nw_t = \\sigma_t \\epsilon_t,\n\\] \\[\n\\sigma_t^2 = \\omega + \\alpha_1 w_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2.\n\\]\nthe conditional variance equation becomes \\[\n\\sigma_t^2 = 0.0000075 + 0.13\\, w_{t-1}^2 + 0.829\\, \\sigma_{t-1}^2.\n\\]"
  },
  {
    "objectID": "financial.html#directly-garch-model",
    "href": "financial.html#directly-garch-model",
    "title": "Financial TS Models",
    "section": "Directly GARCH Model",
    "text": "Directly GARCH Model\nThis search suggests that a GARCH(1,0) model is the most suitable choice, with the lowest AIC value, indicating the best balance between model complexity and goodness-of-fit.\n\n\nCode\n# Initialize model storage\nmodels &lt;- list()\ncc &lt;- 1\n\n# Grid search for GARCH(p, q), where p = ARCH order, q = GARCH order\nfor (p in 1:10) {\n  for (q in 0:10) {\n    models[[cc]] &lt;- garch(returns, order = c(q, p), trace = FALSE)\n    cc &lt;- cc + 1\n  }\n}\n\n# Extract AIC values\ngarch_aic &lt;- sapply(models, AIC)\n\n# Find best model (lowest AIC)\nbest_index &lt;- which.min(GARCH_AIC)\nmodels[[best_index]]\n\n\n\nCall:\ngarch(x = returns, order = c(q, p), trace = FALSE)\n\nCoefficient(s):\n       a0         a1  \n0.0002612  0.3970424"
  },
  {
    "objectID": "financial.html#cross-validation",
    "href": "financial.html#cross-validation",
    "title": "Financial TS Models",
    "section": "Cross Validation",
    "text": "Cross Validation\nIt’s look like just the ARIMA + GARCH(1,1) model is slightly better.\n\n\nCode\n# Step 1: Prepare data\nlog.b &lt;- log(ts_icln)\nreturns &lt;- diff(log.b)\n# Set rolling window parameters\nk &lt;- 600  # 20% of data length as initial training size\nn &lt;- length(returns)\n\n# Initialize error storage\nerr1 &lt;- c()  # ARIMA + GARCH(1,1)\nerr2 &lt;- c()  # GARCH(1,0)\nerr3 &lt;- c()  # ARCH(4)\n\n# Rolling forecast loop\nfor (i in 1:(n - k)) {\n  \n  # Training and test data\n  xtrain &lt;- log.b[1:(k - 1) + i]\n  xtest &lt;- log.b[k + i]\n  \n  ## Model 1: ARIMA(2,1,2) + GARCH(1,2) on residuals\n  arima.fit &lt;- Arima(xtrain, order = c(0, 1, 0), include.drift = TRUE)\n  arima.res &lt;- residuals(arima.fit)\n  fit1 &lt;- garchFit(~ garch(1, 1), data = arima.res, trace = FALSE)\n  fcast1 &lt;- predict(fit1, n.ahead = 1)\n  \n  ## Model 2: GARCH(1,2) on returns\n  returns_train &lt;- diff(xtrain)\n  fit2 &lt;- garchFit(~ garch(1, 0), data = returns_train, trace = FALSE)\n  fcast2 &lt;- predict(fit2, n.ahead = 1)\n  \n  ## Model 3: ARCH(4) on returns (new model)\n  fit3 &lt;- garchFit(~ garch(4, 0), data = returns_train, trace = FALSE)\n  fcast3 &lt;- predict(fit3, n.ahead = 1)\n  \n  # Forecasting log price (meanForecast is on returns, so add to last value of xtrain)\n  pred1 &lt;- tail(xtrain, 1) + fcast1$meanForecast\n  pred2 &lt;- tail(xtrain, 1) + fcast2$meanForecast\n  pred3 &lt;- tail(xtrain, 1) + fcast3$meanForecast  # ARCH(4) forecast\n\n  # Squared errors for each model\n  err1 &lt;- c(err1, (pred1 - xtest)^2)\n  err2 &lt;- c(err2, (pred2 - xtest)^2)\n  err3 &lt;- c(err3, (pred3 - xtest)^2)\n}\n\n# Calculate RMSE for each model\nRMSE1 &lt;- sqrt(mean(err1))  # ARIMA + GARCH(1,2)\nRMSE2 &lt;- sqrt(mean(err2))  # GARCH(1,2)\nRMSE3 &lt;- sqrt(mean(err3))  # ARCH(4)\n\n# Print results\ncat(\"RMSE (ARIMA + GARCH(1,1)):\", RMSE1, \"\\n\")\n\n\nRMSE (ARIMA + GARCH(1,1)): 0.01688587 \n\n\n\n\nCode\ncat(\"RMSE (GARCH(1,0))      :\", RMSE2, \"\\n\")\n\n\nRMSE (GARCH(1,0))      : 0.01691158 \n\n\n\n\nCode\ncat(\"RMSE (ARCH(4))         :\", RMSE3, \"\\n\")\n\n\nRMSE (ARCH(4))         : 0.01691356"
  },
  {
    "objectID": "financial.html#volatality-plot",
    "href": "financial.html#volatality-plot",
    "title": "Financial TS Models",
    "section": "Volatality Plot",
    "text": "Volatality Plot\nThe GARCH-estimated conditional variance for ICLN exhibits clear and economically interpretable volatility dynamics. The plot shows several pronounced volatility spikes—most notably during the 2008–2009 financial crisis and again around early 2020 during the COVID-19 pandemic. These periods correspond to well-known episodes of extreme market stress and uncertainty, confirming that the model successfully captures volatility clustering.\nBetween these major events, the conditional variance drops sharply and remains relatively low, with only occasional moderate fluctuations. This pattern reflects the typical behavior of financial time series: prolonged periods of calm punctuated by short bursts of intense volatility. The smooth decay after each spike indicates strong GARCH persistence, meaning past shocks continue to influence volatility for an extended period.\nOverall, the volatility plot confirms that the GARCH model effectively captures the time-varying, mean-reverting, and clustered nature of ICLN’s return volatility, and it aligns well with major macroeconomic events known to drive renewable-energy equity fluctuations.\n\n\nCode\n# Extract the conditional variances from GARCH\nht &lt;- garch_model@h.t\n\n# Create dataframe and plot\nvol_df &lt;- data.frame(Date = icln_data$date, Volatility = ht)\n\np &lt;- ggplot(vol_df, aes(x = Date, y = Volatility)) +\n  geom_line(color = \"#ff9933\") +\n  labs(\n    title = \"Volatility Plot (Conditional Variance from GARCH Model)\",\n    x = \"Date\",\n    y = \"Conditional Variance\"\n  ) +\n  theme_minimal()\n\nggplotly(p)"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Energy Transition: A Time Series Perspective",
    "section": "",
    "text": "The world is undergoing a profound transformation in how energy is produced, consumed, and financed. Historically, global economic growth has been tightly coupled with fossil fuels such as oil, coal, and natural gas. However, mounting evidence of climate change, advances in clean technologies, and shifting government policies have accelerated the global transition toward renewable energy. This transition—often described as the “energy transition”—is reshaping markets, altering geopolitical balances, and influencing the financial system.\nFrom a data science perspective, this topic provides a unique opportunity to explore long-term structural trends alongside short-term volatility. Energy data exhibits strong seasonality (e.g., higher natural gas demand in winter, increased electricity demand during summer heat waves), as well as structural breaks (such as the 2015 Paris Agreement or the 2020 oil price crash). It also connects multiple domains: technology costs, consumer behavior, financial market reactions, and environmental policy.\nUnderstanding these dynamics is not just an academic exercise—governments, investors, and industries rely on such analyses to guide decision-making in the face of uncertainty."
  },
  {
    "objectID": "introduction.html#topic-explanation",
    "href": "introduction.html#topic-explanation",
    "title": "Energy Transition: A Time Series Perspective",
    "section": "",
    "text": "The world is undergoing a profound transformation in how energy is produced, consumed, and financed. Historically, global economic growth has been tightly coupled with fossil fuels such as oil, coal, and natural gas. However, mounting evidence of climate change, advances in clean technologies, and shifting government policies have accelerated the global transition toward renewable energy. This transition—often described as the “energy transition”—is reshaping markets, altering geopolitical balances, and influencing the financial system.\nFrom a data science perspective, this topic provides a unique opportunity to explore long-term structural trends alongside short-term volatility. Energy data exhibits strong seasonality (e.g., higher natural gas demand in winter, increased electricity demand during summer heat waves), as well as structural breaks (such as the 2015 Paris Agreement or the 2020 oil price crash). It also connects multiple domains: technology costs, consumer behavior, financial market reactions, and environmental policy.\nUnderstanding these dynamics is not just an academic exercise—governments, investors, and industries rely on such analyses to guide decision-making in the face of uncertainty."
  },
  {
    "objectID": "introduction.html#the-big-picture",
    "href": "introduction.html#the-big-picture",
    "title": "Energy Transition: A Time Series Perspective",
    "section": "The Big Picture",
    "text": "The Big Picture\n\n\n\n\n\nThe global shift to clean energy is one of the most significant transformations shaping the 21st-century economy, with implications spanning technology, markets, policy, and finance. This analysis will consider the following five main areas:\nLong-Run Evolution of Power Generation Mix: This theme examines how the composition of electricity generation—wind, solar, natural gas, coal, nuclear, and hydro—has shifted over time. It focuses on long-term decarbonization trends, structural breakpoints, and the growing role of renewables.\nSeasonality & Climate Drivers: This theme investigates seasonal and climatic patterns in different generation sources and how temperature, irradiance, wind patterns, and hydrology influence renewable and fossil-fuel generation.\nLinkages and Causality in Energy Prices: This theme analyzes dynamic interactions among WTI crude oil, Henry Hub natural gas, and energy-related financial indices, focusing on short-run spillovers, long-run cointegration, and causal relationships.\nFinancial Dynamics of Clean Energy Assets (Return & Volatility Behavior): This theme focuses on price behavior, return dynamics, volatility clustering, and risk spillovers in clean energy ETFs (ICLE/ICLN), especially relative to traditional energy ETFs such as XLE.\nDynamic Interactions within Multi-Energy Systems (Inter-Energy Dynamics): This theme analyzes how different energy sources interact in a multi-energy system—whether renewables displace fossil fuels, whether natural gas complements variable renewables, and how shocks propagate through the generation network."
  },
  {
    "objectID": "introduction.html#literature-review",
    "href": "introduction.html#literature-review",
    "title": "Energy Transition: A Time Series Perspective",
    "section": "Literature Review",
    "text": "Literature Review\nLong-Run Evolution of Power Generation Mix\nGlobal studies show that the power sector is undergoing consistent structural decarbonization. Large-scale industry reports, such as BP’s Statistical Review1 and the IEA’s World Energy Outlook2, document that renewable generation is expanding faster than fossil fuels.\nAcademic literature emphasizes that declining technology costs and supportive policies play a major role in accelerating renewable adoption3. BloombergNEF identifies several “cost-parity tipping points,” where renewables become cheaper than fossil alternatives4, reinforcing long-run structural shifts.\nSeasonality and Climate-Driven Dynamics of Energy Supply\nSolar power’s strong dependence on irradiance and seasonal temperature cycles has been widely documented5. Wind power output is shaped by atmospheric circulation patterns, which vary by season and region6.\nNatural gas generation typically peaks during winter months due to heating demand, as shown in U.S. EIA research7. Hydropower output depends on precipitation and seasonal water availabilityhanasaki2018?, and climate variability is increasingly recognized as a source of short-run volatility in renewable generation across multiple studies.\nLinkages and Causality in Energy Prices\nOil price shocks affect broader energy markets through production costs and macroeconomic channels8.\nStudies show mixed evidence of integration between oil and natural gas: some periods show price comovement9, while others suggest partial decoupling depending on market structure10.\nFinancial literature further indicates strong causal links between energy prices and sector-level equity indices11, with volatility intensifying during crises, as observed in connectedness research12.\nFinancial Dynamics of Clean Energy Assets\nClean energy equities are highly sensitive to fluctuations in oil prices, as shown in early work on alternative energy stocks13.\nClean energy assets display strong volatility clustering, making GARCH-family models appropriate for risk analysis14.\nMore recent work suggests these assets are gradually transitioning from being predominantly policy-driven to more market-driven15, yet they remain exposed to macro-energy shocks.\nCross-asset spillovers—especially during periods of market stress—can be analyzed through the connectedness framework proposed in12.\nDynamic Interactions within Multi-Energy Systems\nRenewable energy expansion often displaces fossil-fuel generation, consistent with evidence of long-term “crowding out” effects16.\nNatural gas plays a critical role as a flexible generation source that compensates for wind and solar intermittency17.\nRecent system-level analyses highlight that shocks can propagate across coal, gas, and renewable sources through substitution and complementarity channels18.\nThese dynamic interactions underscore the importance of VAR and structural modeling approaches in understanding modern electricity systems."
  },
  {
    "objectID": "introduction.html#guiding-questions",
    "href": "introduction.html#guiding-questions",
    "title": "Energy Transition: A Time Series Perspective",
    "section": "Guiding Questions",
    "text": "Guiding Questions\n\nHas the power generation structure (wind, solar, gas, coal, nuclear, hydro) consistently shifted towards clean energy over the past decade? How rapidly has this shift occurred?\nDo clean energy sources such as wind, solar, and hydropower exhibit significant and stable seasonal patterns? Is this seasonality changing?\nHow much does weather (temperature or HDD/CDD) affect solar, wind, and natural gas power generation? Are there any lag effects?\nIs there a significant correlation between oil prices (WTI) and natural gas prices (Henry Hub)? Which leads the other?\nDoes natural gas price (HH) significantly affect natural gas power generation? And does it exhibit different elasticities in winter and summer?\nDo changes in fossil fuel prices “squeeze out” or “promote” renewable energy generation (such as solar/wind)?\nIs there a spillover effect in returns and volatility between clean energy ETFs (ICLE/ICLN) and traditional energy ETFs (XLE)? 8. Do clean energy ETFs (ICLE/ICLN) exhibit significant volatility clustering (GARCH characteristics)? Are their risk levels higher than those of traditional energy ETFs?\nAre there substitution relationships within multi-energy systems (wind/solar/coal/gas/nuclear/hydro)? For example, will wind and solar power displace coal gas?\nHow do exogenous shocks in the energy market (surges in oil prices, abnormal temperatures) propagate among different energy sources through the VAR system?"
  },
  {
    "objectID": "univariate.html",
    "href": "univariate.html",
    "title": "ARMA, ARIMA, and SARIMA Models",
    "section": "",
    "text": "Examine how the composition of power generation (wind, solar, coal, natural gas, nuclear, hydro) has evolved over time and how each source exhibits seasonal patterns. This captures the structural shift of the energy transition.\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(seasonal)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(gridExtra)\nlibrary(fpp2)\nlibrary(prophet)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(patchwork)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tsibble)\nlibrary(knitr)\n\ndf &lt;- read.csv(\"data/electric.csv\")\ndf &lt;- df %&gt;%\n  mutate(\n    Description = str_extract(Description, \"(?&lt;=From ).*?(?=,)\"),\n    YYYYMM = as.character(YYYYMM),\n    Year = as.integer(substr(YYYYMM, 1, 4)),\n    Month = as.integer(substr(YYYYMM, 5, 6))\n  ) %&gt;%\n  filter(Month &lt;= 12) %&gt;%\n  filter(Year &gt;= 2000) %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))"
  },
  {
    "objectID": "univariate.html#wind",
    "href": "univariate.html#wind",
    "title": "ARMA, ARIMA, and SARIMA Models",
    "section": "Wind",
    "text": "Wind\n\n\nCode\nwind_df &lt;- df %&gt;%\n  filter(Description == \"Wind\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_wind &lt;- ts(wind_df$Value,\n              start = c(min(wind_df$Year), min(wind_df$Month)),\n              frequency = 12)\n\n\n\nLog TransformationSeasonal DifferencingAugmented Dickey-Fuller TestACF and PACFManual ARIMAModel diagnostic and equationauto.arima()ForecastingBenchmark\n\n\nThe log-transformed, first-differenced series is more stationary than the first difference of the original scale. In the original-scale differences, variability increases toward the end—evidence of heteroskedasticity and residual non-stationarity. The log transform stabilizes the variance, so the differenced log series fluctuates around a roughly constant mean with nearly constant variation. However, the ACF still shows seasonal dependence (spikes at lags 12, 24, …). To address this remaining structure, we should apply a seasonal difference on top of the ordinary difference—i.e., compute —to remove the seasonal correlation.\n\n\nCode\n# Extract vectors\ndates &lt;- wind_df$Date\ny     &lt;- as.numeric(wind_df$Value)\n\ndiff_orig &lt;- diff(y)\ndiff_log  &lt;- diff(log(pmax(y, .Machine$double.eps)))  # safe log if any zeros\ndate_d    &lt;- dates[-1]\n\n# Data frames for ggplot\ndf_orig &lt;- data.frame(Date = date_d, Diff = diff_orig)\ndf_log  &lt;- data.frame(Date = date_d, Diff = diff_log)\n\n# Panel 1: differenced original\np1 &lt;- ggplot(df_orig, aes(x = Date, y = Diff)) +\n  geom_line(color = \"steelblue\") +\n  labs(title = \"First Difference (Original Scale)\",\n       x = \"Year\", y = \"First Difference\") +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"5 years\") +\n  theme_minimal()\n\n# Panel 2: differenced log\np2 &lt;- ggplot(df_log, aes(x = Date, y = Diff)) +\n  geom_line(color = \"firebrick\") +\n  labs(title = \"First Difference (Log Scale)\",\n       x = \"Year\", y = \"First Difference (log)\") +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"5 years\") +\n  theme_minimal()\n\n# Stack vertically\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nACF: Original series. Here it is evident that there is very high autocorrelation, including seasonal correlation, with highly significant serial correlation at seasonal lags. However, due to heteroscedasticity in the data, we need a log transformation.\nACF: log(xt) — ordinary diff. Here we can see the ordinary serial correlation is minimal compared to the raw series data ACF, but there is high seasonal correlation remains at lags 12, 24, etc. So we will need seasonal differencing.\nACF: log(xt) — seasonal diff. Here, when we apply seasonal differencing, we can see that the seasonal correlation has become very small, but the series is still not stationary due to high correlation from the ordinary lags. Our goal is to convert the data into a stationary process as a first step for our modeling process; therefore, we need both ordinary differencing on top of seasonal differencing.\nACF: log(xt) — ordinary + seasonal diff. With both ordinary and seasonal differencing, the series is now more stationary.\n\n\nCode\n# Log series (safe for zeros)\nlog_ts  &lt;- log(pmax(ts_wind, .Machine$double.eps))\n\n# Differencing\nd_log   &lt;- diff(log_ts)              # ordinary diff of log\nsd_log  &lt;- diff(log_ts, lag = 12)    # seasonal diff of log\nosd_log &lt;- diff(d_log,  lag = 12)    # first + seasonal diff of log\n\n# ACF plots\np1 &lt;- ggAcf(ts_wind,   lag.max = 60) + ggtitle(\"ACF: Original series\") +\n  theme_minimal()\np2 &lt;- ggAcf(d_log,   lag.max = 60) + ggtitle(\"ACF: log(xt)-ordinary diff)\") +\n  theme_minimal()\np3 &lt;- ggAcf(sd_log,  lag.max = 60) + ggtitle(\"ACF: log(xt)-seasonal diff\") +\n  theme_minimal()\np4 &lt;- ggAcf(osd_log, lag.max = 60) + ggtitle(\"ACF:log(xt)-ordinary + seasonal diff\") +\n  theme_minimal()\n\n# Arrange in a 2x2 grid\n(p1 | p2) / (p3 | p4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf.test(osd_log)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  osd_log\nDickey-Fuller = -9.8337, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\np = 0:4, d = 1, q = 1, P = 0:1, D = 1, Q = 1\n\n\nCode\nacf &lt;-  ggAcf(osd_log, lag.max = 60) + ggtitle(\"ACF:log(xt)-ordinary + seasonal diff\") +\n  theme_minimal()\n\npacf &lt;- ggPacf(osd_log, lag.max = 60) + ggtitle(\"PACF:log(xt)-ordinary + seasonal diff\") +\n  theme_minimal()\n\ngrid.arrange(acf, pacf, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\noutput=SARIMA.c(p1=1,p2=5,q1=1,q2=3,P1=1,P2=3,Q1=1,Q2=3,data=log_ts)\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q       AIC       BIC      AICc\n25 1 1 1 0 1 1 -311.6715 -296.9508 -311.5326\n\n\n\n\n\nStandardized Residuals: The residuals fluctuate randomly around zero with no visible trend or seasonal pattern. Variance appears stable over time. Suggests residuals behave like white noise.\nACF of Residuals: Most autocorrelations are within the blue confidence bounds.No significant spikes → no serial correlation remains. Indicates good model fit.\nNormal Q–Q Plot: Points mostly lie on the 45° line, with slight deviation in the tails. Minor departure from normality at extremes, but acceptable for large samples.\nLjung–Box Test p-values: Many p-values fall below 0.05, which suggests that some autocorrelation remains in the residuals. Therefore, the white-noise assumption is not fully met, and the model may still have room for improvement.\n\n\\[\n(1 - 0.3321B)(1 - B)(1 - B^{12})y_t\n= (1 - 0.8416B)(1 - 0.8160B^{12})\\varepsilon_t\n\\]\n\n\nCode\nmodel_output &lt;- capture.output(sarima(log_ts, 1,1,1, 0,1,1,12))\n\n\n\n\n\n\n\n\n\nCode\nstart_line &lt;- grep(\"Coefficients\", model_output)  \nend_line &lt;- length(model_output)\ncat(model_output[start_line:end_line], sep = \"\\n\")\n\n\nCoefficients: \n     Estimate     SE  t.value p.value\nar1    0.3321 0.0737   4.5082       0\nma1   -0.8416 0.0393 -21.4253       0\nsma1  -0.8160 0.0491 -16.6176       0\n\nsigma^2 estimated as 0.01874347 on 290 degrees of freedom \n \nAIC = -1.063725  AICc = -1.063442  BIC = -1.013484 \n \n\n\n\n\nDifferent from the chosen model.\nAlthough auto.arima() proposed a more complex model, the manual model achieved lower AIC and better residual diagnostics, suggesting it provides a more parsimonious and robust fit. The difference arises because auto.arima() relies on automated information criteria search, which may not always align with manual diagnostic evaluation.\n\n\nCode\nauto.arima(log_ts)\n\n\nSeries: log_ts \nARIMA(1,1,3)(2,1,2)[12] \n\nCoefficients:\n         ar1      ma1      ma2      ma3     sar1    sar2     sma1     sma2\n      0.2718  -0.7911  -0.0085  -0.0312  -0.6830  0.0756  -0.1658  -0.5369\ns.e.  0.4079   0.4073   0.2308   0.1104   0.2856  0.1124   0.2788   0.2025\n\nsigma^2 = 0.01899:  log likelihood = 161.75\nAIC=-305.5   AICc=-304.86   BIC=-272.37\n\n\n\n\nThe ARIMA(1,1,1)(0,1,1)[12] model provides a reasonable forecast for wind electricity net generation.\nThe results suggest that:\n\nWind electricity generation will continue increasing in the coming years, following the established long-term growth trend.\nSeasonal fluctuations remain evident, showing that generation tends to rise and fall periodically (likely due to seasonal wind patterns).\nThe forecast intervals are relatively tight, implying that future wind generation is expected to remain within predictable bounds, barring major policy or climate disruptions.\n\n\n\nCode\nfit &lt;- Arima(log_ts, order=c(1,1,1), seasonal=c(0,1,1))\nfit %&gt;% forecast(h=36) %&gt;% autoplot() #next 3 years\n\n\n\n\n\n\n\n\n\n\n\n\nThe ARIMA model outperforms all benchmark models.\nThis indicates ARIMA captures both the underlying trend and seasonality of the wind generation data, making it the most reliable forecasting model among those tested.\n\n\n\nCode\n# Generate forecasts\nmean_forecast &lt;- forecast(meanf(log_ts, h = 36))\nnaive_forecast &lt;- forecast(naive(log_ts, h = 36))\nsnaive_forecast &lt;- forecast(snaive(log_ts, h = 36))\ndrift_forecast &lt;- forecast(rwf(log_ts, h = 36, drift = TRUE))\narima_forecast &lt;- forecast(fit, h = 36)\n\n# Convert forecasts to data frame\n# Extract forecasted values properly\nforecast_df &lt;- data.frame(\n  Date = seq(max(df_log$Date) %m+% months(1), by = \"month\", length.out = 36),\n  Mean = as.numeric(mean_forecast$mean),    # Extract mean forecast\n  Naive = as.numeric(naive_forecast$mean),\n  SNaive = as.numeric(snaive_forecast$mean),\n  Drift = as.numeric(drift_forecast$mean),\n  ARIMA = as.numeric(arima_forecast$mean)\n)\n\n# Plot with ggplot2\np &lt;- ggplot(wind_df, aes(x = Date, y = log(Value))) +\n  geom_line(color = \"black\", size = 1) +  # Actual Data\n  geom_line(data = forecast_df, aes(x = Date, y = Mean, color = \"Mean\"), linewidth = 1, linetype = \"dashed\") +\n  geom_line(data = forecast_df, aes(x = Date, y = Naive, color = \"Naïve\"), linewidth = 1, linetype = \"dotted\") +\n  geom_line(data = forecast_df, aes(x = Date, y = SNaive, color = \"Seasonal Naïve\"), linewidth = 1, linetype = \"dotdash\") +\n  geom_line(data = forecast_df, aes(x = Date, y = Drift, color = \"Drift\"), linewidth = 1, linetype = \"twodash\") +\n  geom_line(data = forecast_df, aes(x = Date, y = ARIMA, color = \"ARIMA Fit\"), linewidth = 1) +\n  ggtitle(\"Wind Electricity Net Generation Forecast\") +\n  xlab(\"Year\") + ylab(\"Value\") +\n  scale_color_manual(values = c(\"Mean\" = \"blue\", \"Naïve\" = \"red\", \n                                \"Seasonal Naïve\" = \"green\", \"Drift\" = \"orange\", \n                                \"ARIMA Fit\" = \"purple\")) +\n  guides(colour = guide_legend(title = \"Forecast Methods\")) +\n  theme_minimal()\n\nggplotly(p)\n\n\n\n\n\n\n\n\nCode\n# Compute Accuracy Metrics Safely\nget_acc &lt;- function(model, name) {\n  acc &lt;- as.data.frame(accuracy(model))\n  rownames(acc) &lt;- NULL\n  acc$Method &lt;- name\n  return(acc)\n}\n\nacc_list &lt;- list(\n  get_acc(mean_forecast, \"Mean\"),\n  get_acc(naive_forecast, \"Naïve\"),\n  get_acc(snaive_forecast, \"SNaïve\"),\n  get_acc(drift_forecast, \"Drift\"),\n  get_acc(arima_forecast, \"ARIMA\")\n)\n\nacc_df &lt;- dplyr::bind_rows(acc_list)\n\nacc_summary &lt;- acc_df %&gt;%\n  dplyr::select(Method, RMSE, MAE, MAPE, MASE, ACF1) %&gt;%\n  dplyr::distinct()\n\nprint(acc_summary)\n\n\n  Method      RMSE        MAE      MAPE      MASE         ACF1\n1   Mean 1.4314929 1.24259431 15.262636 5.7652178  0.985950234\n2  Naïve 0.1737855 0.13286537  1.578536 0.6164504 -0.085979421\n3 SNaïve 0.2702475 0.21553293  2.609653 1.0000000  0.503213485\n4  Drift 0.1732856 0.13259742  1.576074 0.6152072 -0.085979421\n5  ARIMA 0.1339741 0.09925709  1.187064 0.4605194  0.005138504"
  },
  {
    "objectID": "univariate.html#solar",
    "href": "univariate.html#solar",
    "title": "ARMA, ARIMA, and SARIMA Models",
    "section": "Solar",
    "text": "Solar\n\n\nCode\nsolar_df &lt;- df %&gt;%\n  filter(Description == \"Solar\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_solar &lt;- ts(solar_df$Value,\n              start = c(min(solar_df$Year), min(solar_df$Month)),\n              frequency = 12)\n\n\n\nLog TransformationSeasonal DifferencingAugmented Dickey-Fuller TestACF and PACFManual ARIMAModel diagnostic and equationauto.arima()ForecastingBenchmark\n\n\nThe log-transformed, first-differenced series is more stationary than the first difference of the original scale. In the original-scale differences, variability increases toward the end—evidence of heteroskedasticity and residual non-stationarity. The log transform stabilizes the variance, so the differenced log series fluctuates around a roughly constant mean with nearly constant variation. However, the ACF still shows seasonal dependence (spikes at lags 12, 24, …). To address this remaining structure, we should apply a seasonal difference on top of the ordinary difference—i.e., compute —to remove the seasonal correlation.\n\n\nCode\n# Extract vectors\ndates &lt;- solar_df$Date\ny     &lt;- as.numeric(solar_df$Value)\n\ndiff_orig &lt;- diff(y)\ndiff_log  &lt;- diff(log(pmax(y, .Machine$double.eps)))  # safe log if any zeros\ndate_d    &lt;- dates[-1]\n\n# Data frames for ggplot\ndf_orig &lt;- data.frame(Date = date_d, Diff = diff_orig)\ndf_log  &lt;- data.frame(Date = date_d, Diff = diff_log)\n\n# Panel 1: differenced original\np1 &lt;- ggplot(df_orig, aes(x = Date, y = Diff)) +\n  geom_line(color = \"steelblue\") +\n  labs(title = \"First Difference (Original Scale)\",\n       x = \"Year\", y = \"First Difference\") +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"5 years\") +\n  theme_minimal()\n\n# Panel 2: differenced log\np2 &lt;- ggplot(df_log, aes(x = Date, y = Diff)) +\n  geom_line(color = \"firebrick\") +\n  labs(title = \"First Difference (Log Scale)\",\n       x = \"Year\", y = \"First Difference (log)\") +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"5 years\") +\n  theme_minimal()\n\n# Stack vertically\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nACF: Original series. Here it is evident that there is very high autocorrelation, including seasonal correlation, with highly significant serial correlation at seasonal lags. However, due to heteroscedasticity in the data, we need a log transformation.\nACF: log(xt) — ordinary diff. Here we can see the ordinary serial correlation is minimal compared to the raw series data ACF, but there is high seasonal correlation remains at lags 12, 24, etc. So we will need seasonal differencing.\nACF: log(xt) — seasonal diff. Here, when we apply seasonal differencing, we can see that the seasonal correlation has become very small, but the series is still not stationary due to high correlation from the ordinary lags. Our goal is to convert the data into a stationary process as a first step for our modeling process; therefore, we need both ordinary differencing on top of seasonal differencing.\nACF: log(xt) — ordinary + seasonal diff. With both ordinary and seasonal differencing, the series is now more stationary.\n\n\nCode\n# Log series (safe for zeros)\nlog_ts  &lt;- log(pmax(ts_solar, .Machine$double.eps))\n\n# Differencing\nd_log   &lt;- diff(log_ts)              # ordinary diff of log\nsd_log  &lt;- diff(log_ts, lag = 12)    # seasonal diff of log\nosd_log &lt;- diff(d_log,  lag = 12)    # first + seasonal diff of log\n\n# ACF plots\np1 &lt;- ggAcf(ts_solar,   lag.max = 60) + ggtitle(\"ACF: Original series\") +\n  theme_minimal()\np2 &lt;- ggAcf(d_log,   lag.max = 60) + ggtitle(\"ACF: log(xt)-ordinary diff)\") +\n  theme_minimal()\np3 &lt;- ggAcf(sd_log,  lag.max = 60) + ggtitle(\"ACF: log(xt)-seasonal diff\") +\n  theme_minimal()\np4 &lt;- ggAcf(osd_log, lag.max = 60) + ggtitle(\"ACF:log(xt)-ordinary + seasonal diff\") +\n  theme_minimal()\n\n# Arrange in a 2x2 grid\n(p1 | p2) / (p3 | p4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf.test(osd_log)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  osd_log\nDickey-Fuller = -10.723, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\np = 0:4, d = 1, q = 1, P = 0:3, D = 1, Q = 0:3\n\n\nCode\nacf &lt;-  ggAcf(osd_log, lag.max = 60) + ggtitle(\"ACF:log(xt)-ordinary + seasonal diff\") +\n  theme_minimal()\n\npacf &lt;- ggPacf(osd_log, lag.max = 60) + ggtitle(\"PACF:log(xt)-ordinary + seasonal diff\") +\n  theme_minimal()\n\ngrid.arrange(acf, pacf, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\noutput=SARIMA.c(p1=1,p2=5,q1=1,q2=3,P1=1,P2=4,Q1=1,Q2=4,data=log_ts)\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q     AIC      BIC     AICc\n27 1 1 1 0 1 1 20.4737 35.19439 20.61259\n\n\n\n\n\nStandardized Residuals: The residuals fluctuate randomly around zero with no visible trend or seasonal pattern. Variance appears stable over time. Suggests residuals behave like white noise.\nACF of Residuals: Most autocorrelations are within the blue confidence bounds. No significant spikes → no serial correlation remains. Indicates good model fit.\nNormal Q–Q Plot: Points mostly lie on the 45° line, with slight deviation in the tails. Minor departure from normality at extremes, but acceptable for large samples.\nLjung–Box Test p-values: Some p-values above 0.05 across lags → we fail to reject the null hypothesis of no autocorrelation. Residuals are uncorrelated.\n\n\\[\n(1 - 0.1385B)(1 - B)(1 - B^{12})y_t\n= (1 + 0.8088B)(1 + 0.2874B^{12})\\varepsilon_t\n\\]\n\n\nCode\nmodel_output &lt;- capture.output(sarima(log_ts, 1,1,1, 0,1,1,12))\n\n\n\n\n\n\n\n\n\nCode\nstart_line &lt;- grep(\"Coefficients\", model_output)  \nend_line &lt;- length(model_output)\ncat(model_output[start_line:end_line], sep = \"\\n\")\n\n\nCoefficients: \n     Estimate     SE  t.value p.value\nar1    0.1385 0.0744   1.8614  0.0637\nma1   -0.8088 0.0428 -18.8908  0.0000\nsma1  -0.2874 0.0734  -3.9144  0.0001\n\nsigma^2 estimated as 0.06069645 on 290 degrees of freedom \n \nAIC = 0.06987611  AICc = 0.07015954  BIC = 0.1201174 \n \n\n\n\n\nDifferent from the chosen model.\nAlthough both models fit reasonably well, the manual model has lower AIC, AICc, and BIC, indicating superior performance. The difference likely arises from auto.arima()’s automated search limitations and the manual model’s closer alignment with the observed autocorrelation structure and residual diagnostics.\n\n\nCode\nauto.arima(log_ts)\n\n\nSeries: log_ts \nARIMA(0,1,1)(2,1,0)[12] \n\nCoefficients:\n          ma1     sar1     sar2\n      -0.7511  -0.2300  -0.1172\ns.e.   0.0459   0.0641   0.0657\n\nsigma^2 = 0.06217:  log likelihood = -8.13\nAIC=24.25   AICc=24.39   BIC=38.97\n\n\n\n\nThe results indicate a continued strong upward trend in solar power generation, reflecting the ongoing expansion of renewable energy capacity and technological improvements in solar energy efficiency. The seasonal pattern remains evident, suggesting that solar generation continues to fluctuate cyclically throughout the year, likely influenced by weather and solar conditions. The widening confidence bands illustrate increasing uncertainty over time, which is typical in medium-term forecasts. Overall, the model provides a good fit to the historical data and projects a steady growth trajectory for solar electricity generation, consistent with global energy transition trends.\n\n\nCode\nfit &lt;- Arima(log_ts, order=c(1,1,1), seasonal=c(0,1,1))\nfit %&gt;% forecast(h=36) %&gt;% autoplot() #next 3 years\n\n\n\n\n\n\n\n\n\n\n\n\nThe ARIMA model outperforms all benchmark models.\nThis indicates ARIMA captures both the underlying trend and seasonality of the solar generation data, making it the most reliable forecasting model among those tested.\n\n\n\nCode\n# Generate forecasts\nmean_forecast &lt;- forecast(meanf(log_ts, h = 36))\nnaive_forecast &lt;- forecast(naive(log_ts, h = 36))\nsnaive_forecast &lt;- forecast(snaive(log_ts, h = 36))\ndrift_forecast &lt;- forecast(rwf(log_ts, h = 36, drift = TRUE))\narima_forecast &lt;- forecast(fit, h = 36)\n\n# Convert forecasts to data frame\n# Extract forecasted values properly\nforecast_df &lt;- data.frame(\n  Date = seq(max(df_log$Date) %m+% months(1), by = \"month\", length.out = 36),\n  Mean = as.numeric(mean_forecast$mean),    # Extract mean forecast\n  Naive = as.numeric(naive_forecast$mean),\n  SNaive = as.numeric(snaive_forecast$mean),\n  Drift = as.numeric(drift_forecast$mean),\n  ARIMA = as.numeric(arima_forecast$mean)\n)\n\n# Plot with ggplot2\np &lt;- ggplot(solar_df, aes(x = Date, y = log(Value))) +\n  geom_line(color = \"black\", size = 1) +  # Actual Data\n  geom_line(data = forecast_df, aes(x = Date, y = Mean, color = \"Mean\"), linewidth = 1, linetype = \"dashed\") +\n  geom_line(data = forecast_df, aes(x = Date, y = Naive, color = \"Naïve\"), linewidth = 1, linetype = \"dotted\") +\n  geom_line(data = forecast_df, aes(x = Date, y = SNaive, color = \"Seasonal Naïve\"), linewidth = 1, linetype = \"dotdash\") +\n  geom_line(data = forecast_df, aes(x = Date, y = Drift, color = \"Drift\"), linewidth = 1, linetype = \"twodash\") +\n  geom_line(data = forecast_df, aes(x = Date, y = ARIMA, color = \"ARIMA Fit\"), linewidth = 1) +\n  ggtitle(\"Solar Electricity Net Generation Forecast\") +\n  xlab(\"Year\") + ylab(\"Value\") +\n  scale_color_manual(values = c(\"Mean\" = \"blue\", \"Naïve\" = \"red\", \n                                \"Seasonal Naïve\" = \"green\", \"Drift\" = \"orange\", \n                                \"ARIMA Fit\" = \"purple\")) +\n  guides(colour = guide_legend(title = \"Forecast Methods\")) +\n  theme_minimal()\n\nggplotly(p)\n\n\n\n\n\n\n\n\nCode\n# Compute Accuracy Metrics Safely\nget_acc &lt;- function(model, name) {\n  acc &lt;- as.data.frame(accuracy(model))\n  rownames(acc) &lt;- NULL\n  acc$Method &lt;- name\n  return(acc)\n}\n\nacc_list &lt;- list(\n  get_acc(mean_forecast, \"Mean\"),\n  get_acc(naive_forecast, \"Naïve\"),\n  get_acc(snaive_forecast, \"SNaïve\"),\n  get_acc(drift_forecast, \"Drift\"),\n  get_acc(arima_forecast, \"ARIMA\")\n)\n\nacc_df &lt;- dplyr::bind_rows(acc_list)\n\nacc_summary &lt;- acc_df %&gt;%\n  dplyr::select(Method, RMSE, MAE, MAPE, MASE, ACF1) %&gt;%\n  dplyr::distinct()\n\nprint(acc_summary)\n\n\n  Method      RMSE       MAE      MAPE      MASE         ACF1\n1   Mean 2.5126153 2.2927864 51.296079 6.8673608  0.975314008\n2  Naïve 0.4513865 0.2963849  9.040657 0.8877329  0.206852172\n3 SNaïve 0.4290561 0.3338672  6.919360 1.0000000  0.591550598\n4  Drift 0.4505947 0.2955583  9.067843 0.8852571  0.206852172\n5  ARIMA 0.2410778 0.1505371  4.232683 0.4508891 -0.006054956"
  },
  {
    "objectID": "univariate.html#hydroelectric-power",
    "href": "univariate.html#hydroelectric-power",
    "title": "ARMA, ARIMA, and SARIMA Models",
    "section": "Hydroelectric Power",
    "text": "Hydroelectric Power\n\n\nCode\nhy_df &lt;- df %&gt;%\n  filter(Description == \"Conventional Hydroelectric Power\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_hy &lt;- ts(hy_df$Value,\n              start = c(min(hy_df$Year), min(hy_df$Month)),\n              frequency = 12)\n\n\n\nSeasonal DifferencingAugmented Dickey-Fuller TestACF and PACFManual ARIMAModel diagnostic and equationauto.arima()ForecastingBenchmark\n\n\nACF: First diff, there is high seasonal correlation remains at lags 12, 24, etc. So we will need seasonal differencing.\nACF: SSeasonal diff. Here, when we apply seasonal differencing, we can see that the seasonal correlation has become very small, but the series is still not stationary due to high correlation from the ordinary lags. Our goal is to convert the data into a stationary process as a first step for our modeling process; therefore, we need both ordinary differencing on top of seasonal differencing.\nACF: First + seasonal diff. With both ordinary and seasonal differencing, the series is now more stationary.\n\n\nCode\n# Differencing\nd   &lt;- diff(ts_hy)              \nsd &lt;- diff(ts_hy, lag = 12)    \nosd &lt;- diff(d,  lag = 12)    # first + seasonal diff of log\n\n# ACF plots\np2 &lt;- ggAcf(d, lag.max = 60) + ggtitle(\"ACF: First diff\") +\n  theme_minimal()\np3 &lt;- ggAcf(sd,  lag.max = 60) + ggtitle(\"ACF: Seasonal diff\") +\n  theme_minimal()\np4 &lt;- ggAcf(osd, lag.max = 60) + ggtitle(\"ACF: First + seasonal diff\") +\n  theme_minimal()\n\n# Arrange in a 2x2 grid\ngrid.arrange(p2, p3,p4, nrow=3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf.test(osd)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  osd\nDickey-Fuller = -8.3022, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\np = 0:2, d = 1, q = 1:2, P = 0:4, D = 1, Q = 0:2\n\n\nCode\nacf &lt;-  ggAcf(osd, lag.max = 60) + ggtitle(\"ACF: First + seasonal diff\") +\n  theme_minimal()\n\npacf &lt;- ggPacf(osd, lag.max = 60) + ggtitle(\"PACF: First + seasonal diff\") +\n  theme_minimal()\n\ngrid.arrange(acf, pacf, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=3,P1=1,P2=5,Q1=1,Q2=3,data=ts_hy)\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n26 1 1 1 0 1 1 5231.433 5246.154 5231.572\n\n\n\n\n\nStandardized Residuals: The residuals fluctuate randomly around zero with no visible trend or seasonal pattern. Variance appears stable over time. Suggests residuals behave like white noise.\nACF of Residuals: All autocorrelations are within the blue confidence bounds.No significant spikes → no serial correlation remains. Indicates good model fit.\nNormal Q–Q Plot: Points mostly lie on the 45° line, with slight deviation in the tails. Minor departure from normality at extremes, but acceptable for large samples.\nLjung–Box Test p-values: Most p-values above 0.05 across lags → we fail to reject the null hypothesis of no autocorrelation. Residuals are uncorrelated.\n\n\\[\n(1 - 0.7231B)(1 - B)(1 - B^{12})y_t\n= (1 - 0.9722B)(1 - 0.8675B^{12})\\varepsilon_t\n\\]\n\n\nCode\nmodel_output &lt;- capture.output(sarima(ts_hy, 1,1,1, 0,1,1,12))\n\n\n\n\n\n\n\n\n\nCode\nstart_line &lt;- grep(\"Coefficients\", model_output)  \nend_line &lt;- length(model_output)\ncat(model_output[start_line:end_line], sep = \"\\n\")\n\n\nCoefficients: \n     Estimate     SE  t.value p.value\nar1    0.7231 0.0543  13.3083       0\nma1   -0.9722 0.0286 -33.9398       0\nsma1  -0.8675 0.0527 -16.4557       0\n\nsigma^2 estimated as 3023036 on 290 degrees of freedom \n \nAIC = 17.85472  AICc = 17.85501  BIC = 17.90496 \n \n\n\n\n\n\nDifferent from the chosen model.\nlthough both models fit well, the manual model has lower AIC and BIC, suggesting a marginally better and more parsimonious fit.\nThe main difference stems from auto.arima() using no differencing but adding a drift term to capture trend, whereas the manual model uses differencing to achieve stationarity.\nThis reflects two equivalent approaches to handling non-stationarity—either differencing or including a deterministic drift term.\n\n\n\nCode\nauto.arima(ts_hy)\n\n\nSeries: ts_hy \nARIMA(1,0,2)(0,1,1)[12] with drift \n\nCoefficients:\n         ar1      ma1      ma2     sma1    drift\n      0.8174  -0.0553  -0.1345  -0.8623  -2.6766\ns.e.  0.0595   0.0851   0.0756   0.0502   6.9912\n\nsigma^2 = 3009594:  log likelihood = -2616\nAIC=5244   AICc=5244.29   BIC=5266.1\n\n\n\n\nThe historical data show strong seasonal variation and substantial short-term fluctuations, reflecting the natural dependency of hydroelectric output on rainfall patterns, reservoir levels, and seasonal water availability. The model captures these seasonal cycles well, as seen in the repeating peaks and troughs in the forecasted period. The forecast indicates that hydroelectric generation will likely remain stable overall, without a pronounced long-term upward or downward trend. However, the wide confidence intervals suggest considerable uncertainty, emphasizing the sensitivity of hydroelectric generation to unpredictable climatic and environmental factors. In summary, the model predicts a continuation of historical variability, with hydroelectric power maintaining a consistent but fluctuating contribution to total electricity generation.\n\n\nCode\nfit &lt;- Arima(ts_hy, order=c(1,1,1), seasonal=c(0,1,1))\nfit %&gt;% forecast(h=36) %&gt;% autoplot() #next 3 years\n\n\n\n\n\n\n\n\n\n\n\n\nThe ARIMA model outperforms all benchmark models.\nThis indicates ARIMA captures both the underlying trend and seasonality of the hydroelectirc power generation data, making it the most reliable forecasting model among those tested.\n\n\n\nCode\n# Generate forecasts\nmean_forecast &lt;- forecast(meanf(ts_hy, h = 36))\nnaive_forecast &lt;- forecast(naive(ts_hy, h = 36))\nsnaive_forecast &lt;- forecast(snaive(ts_hy, h = 36))\ndrift_forecast &lt;- forecast(rwf(ts_hy, h = 36, drift = TRUE))\narima_forecast &lt;- forecast(fit, h = 36)\n\n# Convert forecasts to data frame\n# Extract forecasted values properly\nforecast_df &lt;- data.frame(\n  Date = seq(max(hy_df$Date) %m+% months(1), by = \"month\", length.out = 36),\n  Mean = as.numeric(mean_forecast$mean),    # Extract mean forecast\n  Naive = as.numeric(naive_forecast$mean),\n  SNaive = as.numeric(snaive_forecast$mean),\n  Drift = as.numeric(drift_forecast$mean),\n  ARIMA = as.numeric(arima_forecast$mean)\n)\n\n# Plot with ggplot2\np &lt;- ggplot(hy_df, aes(x = Date, y = Value)) +\n  geom_line(color = \"black\", size = 1) +  # Actual Data\n  geom_line(data = forecast_df, aes(x = Date, y = Mean, color = \"Mean\"), linewidth = 1, linetype = \"dashed\") +\n  geom_line(data = forecast_df, aes(x = Date, y = Naive, color = \"Naïve\"), linewidth = 1, linetype = \"dotted\") +\n  geom_line(data = forecast_df, aes(x = Date, y = SNaive, color = \"Seasonal Naïve\"), linewidth = 1, linetype = \"dotdash\") +\n  geom_line(data = forecast_df, aes(x = Date, y = Drift, color = \"Drift\"), linewidth = 1, linetype = \"twodash\") +\n  geom_line(data = forecast_df, aes(x = Date, y = ARIMA, color = \"ARIMA Fit\"), linewidth = 1) +\n  ggtitle(\"Hydroelectric Power Electricity Net Generation Forecast\") +\n  xlab(\"Year\") + ylab(\"Value\") +\n  scale_color_manual(values = c(\"Mean\" = \"blue\", \"Naïve\" = \"red\", \n                                \"Seasonal Naïve\" = \"green\", \"Drift\" = \"orange\", \n                                \"ARIMA Fit\" = \"purple\")) +\n  guides(colour = guide_legend(title = \"Forecast Methods\")) +\n  theme_minimal()\n\nggplotly(p)\n\n\n\n\n\n\n\n\nCode\n# Compute Accuracy Metrics Safely\nget_acc &lt;- function(model, name) {\n  acc &lt;- as.data.frame(accuracy(model))\n  rownames(acc) &lt;- NULL\n  acc$Method &lt;- name\n  return(acc)\n}\n\nacc_list &lt;- list(\n  get_acc(mean_forecast, \"Mean\"),\n  get_acc(naive_forecast, \"Naïve\"),\n  get_acc(snaive_forecast, \"SNaïve\"),\n  get_acc(drift_forecast, \"Drift\"),\n  get_acc(arima_forecast, \"ARIMA\")\n)\n\nacc_df &lt;- dplyr::bind_rows(acc_list)\n\nacc_summary &lt;- acc_df %&gt;%\n  dplyr::select(Method, RMSE, MAE, MAPE, MASE, ACF1) %&gt;%\n  dplyr::distinct()\n\nprint(acc_summary)\n\n\n  Method     RMSE      MAE      MAPE      MASE       ACF1\n1   Mean 3888.367 3166.621 14.662552 1.3703977 0.71861624\n2  Naïve 2916.527 2429.328 11.093069 1.0513240 0.10957299\n3 SNaïve 3100.902 2310.732 10.290935 1.0000000 0.69707977\n4  Drift 2916.506 2429.581 11.092488 1.0514337 0.10957299\n5  ARIMA 1701.365 1272.890  5.706318 0.5508601 0.01820591"
  },
  {
    "objectID": "univariate.html#coal",
    "href": "univariate.html#coal",
    "title": "ARMA, ARIMA, and SARIMA Models",
    "section": "Coal",
    "text": "Coal\n\n\nCode\ncoal_df &lt;- df %&gt;%\n  filter(Description == \"Coal\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_coal &lt;- ts(coal_df$Value,\n              start = c(min(coal_df$Year), min(coal_df$Month)),\n              frequency = 12)\n\n\n\nSeasonal DifferencingAugmented Dickey-Fuller TestACF and PACFManual ARIMAModel diagnostic and equationauto.arima()ForecastingBenchmark\n\n\nACF: First diff, there is high seasonal correlation remains at lags 12, 24, etc. So we will need seasonal differencing.\nACF: SSeasonal diff. Here, when we apply seasonal differencing, we can see that the seasonal correlation has become very small, but the series is still not stationary due to high correlation from the ordinary lags. Our goal is to convert the data into a stationary process as a first step for our modeling process; therefore, we need both ordinary differencing on top of seasonal differencing.\nACF: First + seasonal diff. With both ordinary and seasonal differencing, the series is now more stationary.\n\n\nCode\n# Differencing\nd   &lt;- diff(ts_coal)              \nsd &lt;- diff(ts_coal, lag = 12)    \nosd &lt;- diff(d,  lag = 12)    # first + seasonal diff of log\n\n# ACF plots\np2 &lt;- ggAcf(d, lag.max = 60) + ggtitle(\"ACF: First diff)\") +\n  theme_minimal()\np3 &lt;- ggAcf(sd,  lag.max = 60) + ggtitle(\"ACF: seasonal diff\") +\n  theme_minimal()\np4 &lt;- ggAcf(osd, lag.max = 60) + ggtitle(\"ACF: First + seasonal diff\") +\n  theme_minimal()\n\n# Arrange in a 2x2 grid\ngrid.arrange(p2, p3, p4, nrow=3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf.test(osd)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  osd\nDickey-Fuller = -7.2102, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\np = 0:2, d = 1, q = 1, P = 0:3, D = 1, Q = 1\n\n\nCode\nacf &lt;-  ggAcf(osd, lag.max = 60) + ggtitle(\"ACF: First + seasonal diff\") +\n  theme_minimal()\n\npacf &lt;- ggPacf(osd, lag.max = 60) + ggtitle(\"PACF: First + seasonal diff\") +\n  theme_minimal()\n\ngrid.arrange(acf, pacf, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=3,P1=1,P2=4,Q1=1,Q2=3,data=ts_coal)\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n26 1 1 1 0 1 1 6008.907 6023.628 6009.046\n\n\n\n\n\nStandardized Residuals: The residuals fluctuate randomly around zero with no visible trend or seasonal pattern. Variance appears stable over time. Suggests residuals behave like white noise.\nACF of Residuals: Most autocorrelations are within the blue confidence bounds. No significant spikes → no serial correlation remains. Indicates good model fit.\nNormal Q–Q Plot: Points mostly lie on the 45° line, with slight deviation in the tails. Minor departure from normality at extremes, but acceptable for large samples.\nLjung–Box Test p-values: Most p-values above 0.05 across lags → we fail to reject the null hypothesis of no autocorrelation. Residuals are uncorrelated.\n\n\\[\n(1 - 0.403B)(1 - B)(1 - B^{12})y_t\n= (1 + 0.725B)(1 + 0.887B^{12})\\varepsilon_t\n\\]\n\n\nCode\nmodel_output &lt;- capture.output(sarima(ts_coal, 1,1,1, 0,1,1,12))\n\n\n\n\n\n\n\n\n\nCode\nstart_line &lt;- grep(\"Coefficients\", model_output)  \nend_line &lt;- length(model_output)\ncat(model_output[start_line:end_line], sep = \"\\n\")\n\n\nCoefficients: \n     Estimate     SE  t.value p.value\nar1     0.403 0.1523   2.6454  0.0086\nma1    -0.725 0.1209  -5.9952  0.0000\nsma1   -0.887 0.0412 -21.5194  0.0000\n\nsigma^2 estimated as 43094604 on 290 degrees of freedom \n \nAIC = 20.50821  AICc = 20.5085  BIC = 20.55846 \n \n\n\n\n\n\nDifferent from the chosen model.\nWhile both capture the same seasonal structure, the manual model performs slightly better (lower AIC/BIC).\nThe key distinction is that auto.arima() modeled trend using a drift term without differencing, whereas the manual model used first differencing to achieve stationarity.\nThis difference likely explains why the manual model provides a better overall fit.\n\n\n\nCode\nauto.arima(ts_coal)\n\n\nSeries: ts_coal \nARIMA(1,0,2)(0,1,1)[12] with drift \n\nCoefficients:\n         ar1      ma1      ma2     sma1      drift\n      0.9649  -0.3056  -0.1263  -0.8775  -386.5401\ns.e.  0.0202   0.0621   0.0620   0.0441    81.6279\n\nsigma^2 = 43243688:  log likelihood = -3008.1\nAIC=6028.2   AICc=6028.49   BIC=6050.3\n\n\n\n\nThe historical data display a clear long-term downward trend, reflecting the steady decline of coal use in power generation due to rising environmental regulations, aging infrastructure, and the shift toward cleaner energy sources. The model captures both this declining trend and the persistent seasonal fluctuations, likely associated with changes in energy demand across different months. The forecast suggests that coal generation will continue to decrease modestly over the next few years, with values remaining substantially below their early-2000s levels. The widening confidence intervals highlight growing uncertainty, possibly reflecting volatility in energy policy, fuel prices, and demand patterns. Overall, the results are consistent with the ongoing energy transition away from fossil fuels toward renewable alternatives.\n\n\nCode\nfit &lt;- Arima(ts_coal, order=c(1,1,1), seasonal=c(0,1,1))\nfit %&gt;% forecast(h=36) %&gt;% autoplot() #next 3 years\n\n\n\n\n\n\n\n\n\n\n\n\nThe ARIMA model outperforms all benchmark models.\nThis indicates ARIMA captures both the underlying trend and seasonality of the coal generation data, making it the most reliable forecasting model among those tested.\n\n\n\nCode\n# Generate forecasts\nmean_forecast &lt;- forecast(meanf(ts_coal, h = 36))\nnaive_forecast &lt;- forecast(naive(ts_coal, h = 36))\nsnaive_forecast &lt;- forecast(snaive(ts_coal, h = 36))\ndrift_forecast &lt;- forecast(rwf(ts_coal, h = 36, drift = TRUE))\narima_forecast &lt;- forecast(fit, h = 36)\n\n# Convert forecasts to data frame\n# Extract forecasted values properly\nforecast_df &lt;- data.frame(\n  Date = seq(max(coal_df$Date) %m+% months(1), by = \"month\", length.out = 36),\n  Mean = as.numeric(mean_forecast$mean),    # Extract mean forecast\n  Naive = as.numeric(naive_forecast$mean),\n  SNaive = as.numeric(snaive_forecast$mean),\n  Drift = as.numeric(drift_forecast$mean),\n  ARIMA = as.numeric(arima_forecast$mean)\n)\n\n# Plot with ggplot2\np &lt;- ggplot(coal_df, aes(x = Date, y = Value)) +\n  geom_line(color = \"black\", size = 1) +  # Actual Data\n  geom_line(data = forecast_df, aes(x = Date, y = Mean, color = \"Mean\"), linewidth = 1, linetype = \"dashed\") +\n  geom_line(data = forecast_df, aes(x = Date, y = Naive, color = \"Naïve\"), linewidth = 1, linetype = \"dotted\") +\n  geom_line(data = forecast_df, aes(x = Date, y = SNaive, color = \"Seasonal Naïve\"), linewidth = 1, linetype = \"dotdash\") +\n  geom_line(data = forecast_df, aes(x = Date, y = Drift, color = \"Drift\"), linewidth = 1, linetype = \"twodash\") +\n  geom_line(data = forecast_df, aes(x = Date, y = ARIMA, color = \"ARIMA Fit\"), linewidth = 1) +\n  ggtitle(\"Coal Electricity Net Generation Forecast\") +\n  xlab(\"Year\") + ylab(\"Value\") +\n  scale_color_manual(values = c(\"Mean\" = \"blue\", \"Naïve\" = \"red\", \n                                \"Seasonal Naïve\" = \"green\", \"Drift\" = \"orange\", \n                                \"ARIMA Fit\" = \"purple\")) +\n  guides(colour = guide_legend(title = \"Forecast Methods\")) +\n  theme_minimal()\n\nggplotly(p)\n\n\n\n\n\n\n\n\nCode\n# Compute Accuracy Metrics Safely\nget_acc &lt;- function(model, name) {\n  acc &lt;- as.data.frame(accuracy(model))\n  rownames(acc) &lt;- NULL\n  acc$Method &lt;- name\n  return(acc)\n}\n\nacc_list &lt;- list(\n  get_acc(mean_forecast, \"Mean\"),\n  get_acc(naive_forecast, \"Naïve\"),\n  get_acc(snaive_forecast, \"SNaïve\"),\n  get_acc(drift_forecast, \"Drift\"),\n  get_acc(arima_forecast, \"ARIMA\")\n)\n\nacc_df &lt;- dplyr::bind_rows(acc_list)\n\nacc_summary &lt;- acc_df %&gt;%\n  dplyr::select(Method, RMSE, MAE, MAPE, MASE, ACF1) %&gt;%\n  dplyr::distinct()\n\nprint(acc_summary)\n\n\n  Method     RMSE       MAE      MAPE     MASE         ACF1\n1   Mean 42640.17 37209.407 40.976347 3.825499  0.937508230\n2  Naïve 14378.81 12006.462 11.190410 1.234384  0.194003508\n3 SNaïve 12907.03  9726.681 10.295471 1.000000  0.728652099\n4  Drift 14373.98 12005.322 11.164493 1.234267  0.194003508\n5  ARIMA  6423.78  4684.944  4.742525 0.481659 -0.004676138"
  },
  {
    "objectID": "univariate.html#nuclear-electric-power",
    "href": "univariate.html#nuclear-electric-power",
    "title": "ARMA, ARIMA, and SARIMA Models",
    "section": "Nuclear Electric Power",
    "text": "Nuclear Electric Power\n\n\nCode\nnuclear_df &lt;- df %&gt;%\n  filter(Description == \"Nuclear Electric Power\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_nuclear &lt;- ts(nuclear_df$Value,\n              start = c(min(nuclear_df$Year), min(nuclear_df$Month)),\n              frequency = 12)\n\n\n\nSeasonal DifferencingAugmented Dickey-Fuller TestACF and PACFManual ARIMAModel diagnostic and equationauto.arima()ForecastingBenchmark\n\n\nACF: First diff, there is high seasonal correlation remains at lags 12, 24, etc. So we will need seasonal differencing.\nACF: SSeasonal diff. Here, when we apply seasonal differencing, we can see that the seasonal correlation has become very small, but the series is still not stationary due to high correlation from the ordinary lags. Our goal is to convert the data into a stationary process as a first step for our modeling process; therefore, we need both ordinary differencing on top of seasonal differencing.\nACF: First + seasonal diff. With both ordinary and seasonal differencing, the series is now more stationary.\n\n\nCode\n# Differencing\nd   &lt;- diff(ts_nuclear)              \nsd &lt;- diff(ts_nuclear, lag = 12)    \nosd &lt;- diff(d,  lag = 12)    # first + seasonal diff of log\n\n# ACF plots\np2 &lt;- ggAcf(d, lag.max = 60) + ggtitle(\"ACF: First diff)\") +\n  theme_minimal()\np3 &lt;- ggAcf(sd,  lag.max = 60) + ggtitle(\"ACF: seasonal diff\") +\n  theme_minimal()\np4 &lt;- ggAcf(osd, lag.max = 60) + ggtitle(\"ACF: First + seasonal diff\") +\n  theme_minimal()\n\n# Arrange in a 2x2 grid\ngrid.arrange(p2, p3, p4, nrow=3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf.test(osd)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  osd\nDickey-Fuller = -9.6711, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\np = 0:2, d = 1, q = 0:2, P = 0:2, D = 1, Q = 1\n\n\nCode\nacf &lt;-  ggAcf(osd, lag.max = 60) + ggtitle(\"ACF: First + seasonal diff\") +\n  theme_minimal()\n\npacf &lt;- ggPacf(osd, lag.max = 60) + ggtitle(\"PACF: First + seasonal diff\") +\n  theme_minimal()\n\ngrid.arrange(acf, pacf, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=3,P1=1,P2=3,Q1=1,Q2=3,data=ts_nuclear)\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC      BIC   AICc\n25 1 1 1 0 1 1 5128.561 5143.281 5128.7\n\n\n\n\n\nStandardized Residuals: The residuals fluctuate randomly around zero with no visible trend or seasonal pattern. Variance appears stable over time. Suggests residuals behave like white noise.\nACF of Residuals: Most autocorrelations are within the blue confidence bounds. No significant spikes → no serial correlation remains. Indicates good model fit.\nNormal Q–Q Plot: Points mostly lie on the 45° line, with slight deviation in the tails. Minor departure from normality at extremes, but acceptable for large samples.\nLjung–Box Test p-values: Some p-values above 0.05 across lags → we fail to reject the null hypothesis of no autocorrelation. Residuals are uncorrelated. But the model may still have room for improvement.\n\n\\[\n(1 - 0.5737B)(1 - B)(1 - B^{12})y_t\n= (1 + 0.9381B)(1 + 0.9185B^{12})\\varepsilon_t\n\\]\n\n\nCode\nmodel_output &lt;- capture.output(sarima(ts_nuclear, 1,1,1, 0,1,1,12))\n\n\n\n\n\n\n\n\n\nCode\nstart_line &lt;- grep(\"Coefficients\", model_output)  \nend_line &lt;- length(model_output)\ncat(model_output[start_line:end_line], sep = \"\\n\")\n\n\nCoefficients: \n     Estimate     SE  t.value p.value\nar1    0.5737 0.0584   9.8262       0\nma1   -0.9381 0.0258 -36.3730       0\nsma1  -0.9185 0.0610 -15.0617       0\n\nsigma^2 estimated as 2096780 on 290 degrees of freedom \n \nAIC = 17.50362  AICc = 17.5039  BIC = 17.55386 \n \n\n\n\n\n\nDifferent from the chosen model.\n\nWhile both capture seasonality, the manual model achieves significantly lower AIC, AICc, and BIC, indicating a more parsimonious and better-fitting structure.\nThe difference arises mainly from auto.arima() omitting non-seasonal differencing and compensating with additional AR terms, whereas the manual model achieves stationarity through differencing — a more efficient representation of the data’s underlying process.\n\n\n\nCode\nauto.arima(ts_nuclear)\n\n\nSeries: ts_nuclear \nARIMA(2,0,1)(2,1,0)[12] \n\nCoefficients:\n         ar1      ar2      ma1     sar1     sar2\n      1.4360  -0.4696  -0.8603  -0.6827  -0.4051\ns.e.  0.1346   0.1056   0.1111   0.0567   0.0576\n\nsigma^2 = 2409619:  log likelihood = -2578.65\nAIC=5169.3   AICc=5169.59   BIC=5191.4\n\n\n\n\nThe forecasts suggest that future production will maintain similar seasonal fluctuations without a significant upward or downward long-term trend. The confidence bands widen progressively, reflecting growing uncertainty over time but still indicate overall stability in nuclear output.\n\n\nCode\nfit &lt;- Arima(ts_nuclear, order=c(1,1,1), seasonal=c(0,1,1))\nfit %&gt;% forecast(h=36) %&gt;% autoplot() #next 3 years\n\n\n\n\n\n\n\n\n\n\n\n\nThe ARIMA model outperforms all benchmark models.\nThis indicates ARIMA captures both the underlying trend and seasonality of the nuclear electric power generation data, making it the most reliable forecasting model among those tested.\n\n\n\nCode\n# Generate forecasts\nmean_forecast &lt;- forecast(meanf(ts_nuclear, h = 36))\nnaive_forecast &lt;- forecast(naive(ts_nuclear, h = 36))\nsnaive_forecast &lt;- forecast(snaive(ts_nuclear, h = 36))\ndrift_forecast &lt;- forecast(rwf(ts_nuclear, h = 36, drift = TRUE))\narima_forecast &lt;- forecast(fit, h = 36)\n\n# Convert forecasts to data frame\n# Extract forecasted values properly\nforecast_df &lt;- data.frame(\n  Date = seq(max(nuclear_df$Date) %m+% months(1), by = \"month\", length.out = 36),\n  Mean = as.numeric(mean_forecast$mean),    # Extract mean forecast\n  Naive = as.numeric(naive_forecast$mean),\n  SNaive = as.numeric(snaive_forecast$mean),\n  Drift = as.numeric(drift_forecast$mean),\n  ARIMA = as.numeric(arima_forecast$mean)\n)\n\n# Plot with ggplot2\np &lt;- ggplot(nuclear_df, aes(x = Date, y = Value)) +\n  geom_line(color = \"black\", size = 1) +  # Actual Data\n  geom_line(data = forecast_df, aes(x = Date, y = Mean, color = \"Mean\"), linewidth = 1, linetype = \"dashed\") +\n  geom_line(data = forecast_df, aes(x = Date, y = Naive, color = \"Naïve\"), linewidth = 1, linetype = \"dotted\") +\n  geom_line(data = forecast_df, aes(x = Date, y = SNaive, color = \"Seasonal Naïve\"), linewidth = 1, linetype = \"dotdash\") +\n  geom_line(data = forecast_df, aes(x = Date, y = Drift, color = \"Drift\"), linewidth = 1, linetype = \"twodash\") +\n  geom_line(data = forecast_df, aes(x = Date, y = ARIMA, color = \"ARIMA Fit\"), linewidth = 1) +\n  ggtitle(\"Nuclear Electricity Net Generation Forecast\") +\n  xlab(\"Year\") + ylab(\"Value\") +\n  scale_color_manual(values = c(\"Mean\" = \"blue\", \"Naïve\" = \"red\", \n                                \"Seasonal Naïve\" = \"green\", \"Drift\" = \"orange\", \n                                \"ARIMA Fit\" = \"purple\")) +\n  guides(colour = guide_legend(title = \"Forecast Methods\")) +\n  theme_minimal()\n\nggplotly(p)\n\n\n\n\n\n\n\n\nCode\n# Compute Accuracy Metrics Safely\nget_acc &lt;- function(model, name) {\n  acc &lt;- as.data.frame(accuracy(model))\n  rownames(acc) &lt;- NULL\n  acc$Method &lt;- name\n  return(acc)\n}\n\nacc_list &lt;- list(\n  get_acc(mean_forecast, \"Mean\"),\n  get_acc(naive_forecast, \"Naïve\"),\n  get_acc(snaive_forecast, \"SNaïve\"),\n  get_acc(drift_forecast, \"Drift\"),\n  get_acc(arima_forecast, \"ARIMA\")\n)\n\nacc_df &lt;- dplyr::bind_rows(acc_list)\n\nacc_summary &lt;- acc_df %&gt;%\n  dplyr::select(Method, RMSE, MAE, MAPE, MASE, ACF1) %&gt;%\n  dplyr::distinct()\n\nprint(acc_summary)\n\n\n  Method     RMSE      MAE     MAPE      MASE         ACF1\n1   Mean 4830.375 4046.201 6.239002 2.2678504  0.416270652\n2  Naïve 5219.463 4349.728 6.746890 2.4379737  0.054355211\n3 SNaïve 2328.150 1784.157 2.769507 1.0000000  0.579157589\n4  Drift 5219.442 4351.436 6.748660 2.4389311  0.054355211\n5  ARIMA 1416.998 1079.747 1.676060 0.6051862 -0.001624595\n\n\n\n\n\n\nNatural gas\n\n\nCode\nn_df &lt;- df %&gt;%\n  filter(Description == \"Natural Gas\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Value = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_n &lt;- ts(n_df$Value,\n              start = c(min(n_df$Year), min(n_df$Month)),\n              frequency = 12)\n\n\n\nSeasonal DifferencingAugmented Dickey-Fuller TestACF and PACFManual ARIMAModel diagnostic and equationauto.arima()ForecastingBenchmark\n\n\nACF: First diff, there is high seasonal correlation remains at lags 12, 24, etc. So we will need seasonal differencing.\nACF: SSeasonal diff. Here, when we apply seasonal differencing, we can see that the seasonal correlation has become very small, but the series is still not stationary due to high correlation from the ordinary lags. Our goal is to convert the data into a stationary process as a first step for our modeling process; therefore, we need both ordinary differencing on top of seasonal differencing.\nACF: First + seasonal diff. With both ordinary and seasonal differencing, the series is now more stationary.\n\n\nCode\n# Differencing\nd   &lt;- diff(ts_n)              \nsd &lt;- diff(ts_n, lag = 12)    \nosd &lt;- diff(d,  lag = 12)    # first + seasonal diff of log\n\n# ACF plots\np2 &lt;- ggAcf(d, lag.max = 60) + ggtitle(\"ACF: Firstdiff)\") +\n  theme_minimal()\np3 &lt;- ggAcf(sd,  lag.max = 60) + ggtitle(\"ACF: seasonal diff\") +\n  theme_minimal()\np4 &lt;- ggAcf(osd, lag.max = 60) + ggtitle(\"ACF: First + seasonal diff\") +\n  theme_minimal()\n\n# Arrange in a 2x2 grid\ngrid.arrange(p2, p3, p4, nrow=3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf.test(osd)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  osd\nDickey-Fuller = -7.3548, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\np = 0:2, d = 1, q = 1, P = 0:3, D = 1, Q = 1\n\n\nCode\nacf &lt;-  ggAcf(osd, lag.max = 60) + ggtitle(\"ACF: First + seasonal diff\") +\n  theme_minimal()\n\npacf &lt;- ggPacf(osd, lag.max = 60) + ggtitle(\"PACF: First + seasonal diff\") +\n  theme_minimal()\n\ngrid.arrange(acf, pacf, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=3,P1=1,P2=4,Q1=1,Q2=3,data=ts_n)\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC     BIC     AICc\n26 1 1 1 0 1 1 5885.629 5900.35 5885.768\n\n\n\n\n\nStandardized Residuals: The residuals fluctuate randomly around zero with no visible trend or seasonal pattern. Variance appears stable over time. Suggests residuals behave like white noise.\nACF of Residuals: Most autocorrelations are within the blue confidence bounds. No significant spikes → no serial correlation remains. Indicates good model fit.\nNormal Q–Q Plot: Points mostly lie on the 45° line, with slight deviation in the tails. Minor departure from normality at extremes, but acceptable for large samples.\nLjung–Box Test p-values: The results indicate that the values fall below the 0.05 (5% significance) threshold, signifying remaining correlation and the need for model improvement.\n\n\\[\n(1 - 0.7163B)(1 - B)(1 - B^{12})y_t\n= (1 + 0.987B)(1 + 0.67B^{12})\\varepsilon_t\n\\]\n\n\nCode\nmodel_output &lt;- capture.output(sarima(ts_n, 1,1,1, 0,1,1,12))\n\n\n\n\n\n\n\n\n\nCode\nstart_line &lt;- grep(\"Coefficients\", model_output)  \nend_line &lt;- length(model_output)\ncat(model_output[start_line:end_line], sep = \"\\n\")\n\n\nCoefficients: \n     Estimate     SE  t.value p.value\nar1    0.7163 0.0438  16.3393       0\nma1   -0.9870 0.0130 -76.2095       0\nsma1  -0.6700 0.0456 -14.6973       0\n\nsigma^2 estimated as 29081574 on 290 degrees of freedom \n \nAIC = 20.08747  AICc = 20.08775  BIC = 20.13771 \n \n\n\n\n\n\nDifferent from the chosen model.\n\nWhile both capture similar autoregressive, moving-average, and seasonal patterns, the manual model applies differencing rather than drift to handle trend.\nThe manual model achieves lower AIC, BIC, and AICc, indicating a better overall fit.\nThe difference likely arises from auto.arima() treating the series as trend-stationary, whereas the manual model assumes stochastic non-stationarity and achieves stationarity through differencing.\n\n\n\nCode\nauto.arima(ts_n)\n\n\nSeries: ts_n \nARIMA(1,0,1)(0,1,1)[12] with drift \n\nCoefficients:\n         ar1      ma1     sma1     drift\n      0.7872  -0.1362  -0.6513  357.7353\ns.e.  0.0519   0.0873   0.0464   39.4676\n\nsigma^2 = 29029334:  log likelihood = -2944.83\nAIC=5899.66   AICc=5899.87   BIC=5918.07\n\n\n\n\nThe ARIMA model captures both the strong upward trend and pronounced seasonality of natural gas electricity generation. Forecasts indicate that production will continue to rise, with similar seasonal variation patterns in the future. While uncertainty increases over time, the general outlook suggests a stable and sustained growth trajectory for natural gas as a key source of electricity generation.\n\n\nCode\nfit &lt;- Arima(ts_n, order=c(1,1,1), seasonal=c(0,1,1))\nfit %&gt;% forecast(h=36) %&gt;% autoplot() #next 3 years\n\n\n\n\n\n\n\n\n\n\n\n\nThe ARIMA model outperforms all benchmark models.\nThis indicates ARIMA captures both the underlying trend and seasonality of the natural gas generation data, making it the most reliable forecasting model among those tested.\n\n\n\nCode\n# Generate forecasts\nmean_forecast &lt;- forecast(meanf(ts_n, h = 36))\nnaive_forecast &lt;- forecast(naive(ts_n, h = 36))\nsnaive_forecast &lt;- forecast(snaive(ts_n, h = 36))\ndrift_forecast &lt;- forecast(rwf(ts_n, h = 36, drift = TRUE))\narima_forecast &lt;- forecast(fit, h = 36)\n\n# Convert forecasts to data frame\n# Extract forecasted values properly\nforecast_df &lt;- data.frame(\n  Date = seq(max(n_df$Date) %m+% months(1), by = \"month\", length.out = 36),\n  Mean = as.numeric(mean_forecast$mean),    # Extract mean forecast\n  Naive = as.numeric(naive_forecast$mean),\n  SNaive = as.numeric(snaive_forecast$mean),\n  Drift = as.numeric(drift_forecast$mean),\n  ARIMA = as.numeric(arima_forecast$mean)\n)\n\n# Plot with ggplot2\np &lt;- ggplot(n_df, aes(x = Date, y = Value)) +\n  geom_line(color = \"black\", size = 1) +  # Actual Data\n  geom_line(data = forecast_df, aes(x = Date, y = Mean, color = \"Mean\"), linewidth = 1, linetype = \"dashed\") +\n  geom_line(data = forecast_df, aes(x = Date, y = Naive, color = \"Naïve\"), linewidth = 1, linetype = \"dotted\") +\n  geom_line(data = forecast_df, aes(x = Date, y = SNaive, color = \"Seasonal Naïve\"), linewidth = 1, linetype = \"dotdash\") +\n  geom_line(data = forecast_df, aes(x = Date, y = Drift, color = \"Drift\"), linewidth = 1, linetype = \"twodash\") +\n  geom_line(data = forecast_df, aes(x = Date, y = ARIMA, color = \"ARIMA Fit\"), linewidth = 1) +\n  ggtitle(\"Natural gas Electricity Net Generation Forecast\") +\n  xlab(\"Year\") + ylab(\"Value\") +\n  scale_color_manual(values = c(\"Mean\" = \"blue\", \"Naïve\" = \"red\", \n                                \"Seasonal Naïve\" = \"green\", \"Drift\" = \"orange\", \n                                \"ARIMA Fit\" = \"purple\")) +\n  guides(colour = guide_legend(title = \"Forecast Methods\")) +\n  theme_minimal()\n\nggplotly(p)\n\n\n\n\n\n\n\n\nCode\n# Compute Accuracy Metrics Safely\nget_acc &lt;- function(model, name) {\n  acc &lt;- as.data.frame(accuracy(model))\n  rownames(acc) &lt;- NULL\n  acc$Method &lt;- name\n  return(acc)\n}\n\nacc_list &lt;- list(\n  get_acc(mean_forecast, \"Mean\"),\n  get_acc(naive_forecast, \"Naïve\"),\n  get_acc(snaive_forecast, \"SNaïve\"),\n  get_acc(drift_forecast, \"Drift\"),\n  get_acc(arima_forecast, \"ARIMA\")\n)\n\nacc_df &lt;- dplyr::bind_rows(acc_list)\n\nacc_summary &lt;- acc_df %&gt;%\n  dplyr::select(Method, RMSE, MAE, MAPE, MASE, ACF1) %&gt;%\n  dplyr::distinct()\n\nprint(acc_summary)\n\n\n  Method      RMSE       MAE      MAPE      MASE        ACF1\n1   Mean 37465.944 31034.916 39.069482 3.8842614  0.91669955\n2  Naïve 14436.325 11248.988 11.692600 1.4078985  0.35515252\n3 SNaïve 10136.920  7989.914  8.353621 1.0000000  0.71789508\n4  Drift 14430.419 11232.898 11.682334 1.4058847  0.35515252\n5  ARIMA  5276.948  4146.816  4.555594 0.5190063 -0.06335833"
  },
  {
    "objectID": "DL.html#rmse-summary-table",
    "href": "DL.html#rmse-summary-table",
    "title": "Deep Learning for Time Series",
    "section": "RMSE Summary Table",
    "text": "RMSE Summary Table\n\n\nCode\ntable = {\n    \"Model Type\": [\"Traditional\", \"Traditional\", \"Traditional\", \"Deep Learning\", \"Deep Learning\", \"Deep Learning\",\n                   \"Deep Learning\", \"Deep Learning\", \"Deep Learning\"],\n    \"Model\": [\"ARIMA\", \"SARIMAX\", \"VAR\", \"RNN\", \"LSTM\", \"GRU\", \"RNN\", \"LSTM\", \"GRU\"],\n    \"Input Type\": [\"Univariate\", \"Multivariate\", \"Multivariate\", \"Univariate\", \"Univariate\", \"Univariate\",\n                   \"Multivariate\", \"Multivariate\", \"Multivariate\"],\n    \"RMSE\": [0.246, 0.288, 0.2995, 0.1978, 0.5183, 0.2483, 0.0521, 0.0802, 0.0413]\n}\n\ndf_table = pd.DataFrame(table)\ndisplay(df_table.style.hide(axis=\"index\"))\n\n\n\n\n\n\n\nModel Type\nModel\nInput Type\nRMSE\n\n\n\n\nTraditional\nARIMA\nUnivariate\n0.246000\n\n\nTraditional\nSARIMAX\nMultivariate\n0.288000\n\n\nTraditional\nVAR\nMultivariate\n0.299500\n\n\nDeep Learning\nRNN\nUnivariate\n0.197800\n\n\nDeep Learning\nLSTM\nUnivariate\n0.518300\n\n\nDeep Learning\nGRU\nUnivariate\n0.248300\n\n\nDeep Learning\nRNN\nMultivariate\n0.052100\n\n\nDeep Learning\nLSTM\nMultivariate\n0.080200\n\n\nDeep Learning\nGRU\nMultivariate\n0.041300"
  },
  {
    "objectID": "DL.html#model-comparison-write-up",
    "href": "DL.html#model-comparison-write-up",
    "title": "Deep Learning for Time Series",
    "section": "Model Comparison Write-up",
    "text": "Model Comparison Write-up\nAcross all models evaluated, two patterns clearly emerge: model complexity and the richness of input variables both play decisive roles in forecasting accuracy. Traditional univariate models such as ARIMA provide a reasonable baseline (RMSE ≈ 0.246), effectively capturing trend and seasonality when the target series is internally informative. However, adding exogenous variables through SARIMAX did not improve accuracy in this case. This reflects a common limitation of linear models: unless the external drivers have a stable, linear relationship with the target, the model is unable to translate additional inputs into meaningful accuracy gains.\nDeep learning models reveal a different story. In the univariate setting, simple RNNs already outperform all traditional models (RMSE ≈ 0.198), demonstrating their advantage in capturing nonlinear seasonal structures. LSTM and GRU, though theoretically more powerful, perform worse when limited to a single input variable—likely due to overparameterization in a low-signal environment. These models need richer context to justify their complexity.\nOnce multivariate inputs are introduced (temperature, oil prices, natural gas prices), deep learning models show a dramatic improvement. Multivariate RNN and especially multivariate GRU achieve the lowest errors of all models (RMSE ≈ 0.052 and 0.041). These models are able to extract nonlinear interactions across energy markets and climate factors, translating additional information into materially better forecasts. This confirms that solar generation is influenced by external drivers, and models capable of learning such relationships benefit substantially from multivariate inputs.\nIn a real-world forecasting context, the multivariate GRU is the most reliable model. It balances accuracy, stability, and predictive structure, especially when used for short- to medium-term planning. If only univariate models were used, decision-makers would underestimate the role of fuel markets and climate conditions, potentially leading to misallocation of reserves, mispriced power contracts, or inaccurate renewable integration planning.\nOverall, the comparison highlights a key lesson: forecasting improves when models can interpret real-world causal structure rather than relying solely on past values. For teams using these forecasts, the implication is clear—planning, capacity scheduling, and risk assessment should be informed by models that incorporate the broader energy ecosystem, not just historical solar output."
  },
  {
    "objectID": "multivariate.html#var-energy-price-linkages-at-the-financial-market-level",
    "href": "multivariate.html#var-energy-price-linkages-at-the-financial-market-level",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "(4) VAR: Energy price linkages at the financial market level",
    "text": "(4) VAR: Energy price linkages at the financial market level\n\nVisualizationVARselectInitial selectionCVForecasting\n\n\n\nICLN exhibits greater volatility, showing distinct cyclical upward trends and pullbacks; XLE shows a more stable trend and a clearer correlation with oil prices.\nWTI displays sudden shocks and structural volatility; HH has strong seasonal characteristics.\n\n\n\nCode\n# Create individual plots for each variable with y-axis labels\np1 &lt;- plot_ly(data_all, x = ~Date, y = ~ICLN, type = 'scatter', mode = 'lines', name = \"Renewable Energy ETF (ICLN)\") %&gt;%\n  layout(yaxis = list(title = \"Price\"))\n\np2 &lt;- plot_ly(data_all, x = ~Date, y = ~XLE, type = 'scatter', mode = 'lines', name = \"Energy Select Sector SPDR Fund (XLE)\") %&gt;%\n  layout(yaxis = list(title = \"Price\"))\n\np3 &lt;- plot_ly(data_all, x = ~Date, y = ~WTI, type = 'scatter', mode = 'lines', name = \"WTI Oil price\") %&gt;%\n  layout(yaxis = list(title = \"Price\"))\n\np4 &lt;- plot_ly(data_all, x = ~Date, y = ~HH, type = 'scatter', mode = 'lines', name = \"Henry Hub Natural Gas price\") %&gt;%\n  layout(yaxis = list(title = \"Price\"))\n\n# Combine the individual plots into a single figure with subplots and y-axis labels\nfinal_plot &lt;- subplot(p1, p2, p3, p4, nrows = 4, shareX = TRUE, titleX = TRUE) %&gt;%\n  layout(\n    title = \"Time Series of ICLN, XLE, WTI and HH price\",\n    yaxis = list(title = \"Price\"), \n    yaxis2 = list(title = \"Price\"), \n    yaxis3 = list(title = \"Price\"),\n    yaxis4 = list(title = \"Price\"),\n    xaxis = list(title = \"Year\")\n  )\n\nfinal_plot\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- data.frame(Date = data_all$Date, \n                  ICLN = data_all$ICLN, \n                  XLE = data_all$XLE,\n                  WTI = data_all$WTI,\n                  HH = data_all$HH\n                  )\n\n\n\n\np-values are 2 and 10.\n\n\nCode\nlibrary(vars)\ndf2_clean &lt;- na.omit(df2)\ndf_ts &lt;- ts(df2_clean[,-1], frequency = 12)\nVARselect(df_ts, lag.max=10,type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    10      2      2     10 \n\n$criteria\n               1          2          3          4          5          6\nAIC(n) 0.5614624 -0.9098566 -0.8807057 -1.0232522 -0.9651822 -0.9714135\nHQ(n)  0.7288604 -0.6308599 -0.4901104 -0.5210582 -0.3513894 -0.2460221\nSC(n)  0.9746252 -0.2212519  0.0833408  0.2162362  0.5497481  0.8189587\nFPE(n) 1.7533866  0.4027438  0.4149485  0.3602710  0.3825586  0.3812585\n                7           8          9          10\nAIC(n) -0.9585381 -0.91491251 -0.9414515 -1.07408790\nHQ(n)  -0.1215480  0.03367629  0.1187360  0.09769826\nSC(n)   1.1072759  1.42634338  1.6752462  1.81805173\nFPE(n)  0.3876960  0.40704892  0.3989680  0.35224399\n\n\n\n\nVAR(2) clearly outperforms VAR(10). Although VAR(10) offers marginally better in-sample fit, its many insignificant coefficients indicate noise-fitting rather than meaningful dynamics, leading to substantial overfitting risk. Its eigenvalues lying close to the unit circle further suggest weak stability and unreliable forecasts. By contrast, VAR(2) features a compact structure, statistically coherent coefficients, and stable roots. It captures the essential interactions among energy prices and energy equity indices without unnecessary complexity, making it superior in robustness, interpretability, and economic relevance.\nICLN shows strong persistence and is positively influenced by lagged WTI, while lagged XLE exerts a mild competitive effect—highlighting both clean–traditional energy divergence and shared macroeconomic drivers. XLE is dominated by its own lag, reflecting high inertia with limited cross-impact from other markets. WTI responds significantly to both its own lag and XLE, indicating tight commodity–equity linkage and feedback. Henry Hub prices are largely self-driven, with minimal spillovers from other variables. Overall, short-run transmission exists primarily between oil and traditional energy stocks, while clean energy exhibits weaker but still meaningful sensitivity—illustrating both coupling and differentiation between the two energy systems in financial markets.\n\n\nCode\nsummary(vars::VAR(df_ts, p=2, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: ICLN, XLE, WTI, HH \nDeterministic variables: both \nSample size: 196 \nLog Likelihood: -1028.767 \nRoots of the characteristic polynomial:\n0.9378 0.9378 0.905 0.905 0.8874 0.2624 0.2624 0.234\nCall:\nvars::VAR(y = df_ts, p = 2, type = \"both\")\n\n\nEstimation results for equation ICLN: \n===================================== \nICLN = ICLN.l1 + XLE.l1 + WTI.l1 + HH.l1 + ICLN.l2 + XLE.l2 + WTI.l2 + HH.l2 + const + trend \n\n        Estimate Std. Error t value Pr(&gt;|t|)    \nICLN.l1  1.07540    0.07510  14.319  &lt; 2e-16 ***\nXLE.l1  -0.12400    0.06316  -1.963 0.051084 .  \nWTI.l1   0.04514    0.01740   2.594 0.010242 *  \nHH.l1   -0.55413    0.78660  -0.704 0.482027    \nICLN.l2 -0.18209    0.07092  -2.567 0.011031 *  \nXLE.l2   0.05863    0.06461   0.907 0.365345    \nWTI.l2  -0.04044    0.01749  -2.312 0.021874 *  \nHH.l2    0.67786    0.78030   0.869 0.386122    \nconst    0.99769    1.00057   0.997 0.320005    \ntrend    0.01176    0.00333   3.533 0.000519 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.355 on 186 degrees of freedom\nMultiple R-Squared: 0.9385, Adjusted R-squared: 0.9356 \nF-statistic: 315.6 on 9 and 186 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation XLE: \n==================================== \nXLE = ICLN.l1 + XLE.l1 + WTI.l1 + HH.l1 + ICLN.l2 + XLE.l2 + WTI.l2 + HH.l2 + const + trend \n\n         Estimate Std. Error t value Pr(&gt;|t|)    \nICLN.l1 -0.126882   0.099339  -1.277   0.2031    \nXLE.l1   0.862452   0.083539  10.324   &lt;2e-16 ***\nWTI.l1   0.026898   0.023016   1.169   0.2440    \nHH.l1   -0.812479   1.040471  -0.781   0.4359    \nICLN.l2  0.114596   0.093813   1.222   0.2234    \nXLE.l2   0.083290   0.085458   0.975   0.3310    \nWTI.l2  -0.017567   0.023136  -0.759   0.4486    \nHH.l2    0.590042   1.032139   0.572   0.5682    \nconst    0.561589   1.323508   0.424   0.6718    \ntrend    0.009591   0.004405   2.177   0.0307 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.793 on 186 degrees of freedom\nMultiple R-Squared: 0.9521, Adjusted R-squared: 0.9497 \nF-statistic: 410.5 on 9 and 186 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation WTI: \n==================================== \nWTI = ICLN.l1 + XLE.l1 + WTI.l1 + HH.l1 + ICLN.l2 + XLE.l2 + WTI.l2 + HH.l2 + const + trend \n\n         Estimate Std. Error t value Pr(&gt;|t|)    \nICLN.l1  0.300734   0.301768   0.997   0.3203    \nXLE.l1   1.282107   0.253773   5.052 1.04e-06 ***\nWTI.l1   1.103209   0.069919  15.778  &lt; 2e-16 ***\nHH.l1    4.076954   3.160712   1.290   0.1987    \nICLN.l2 -0.376341   0.284983  -1.321   0.1883    \nXLE.l2  -1.341722   0.259603  -5.168 6.06e-07 ***\nWTI.l2  -0.156827   0.070283  -2.231   0.0269 *  \nHH.l2   -5.902979   3.135403  -1.883   0.0613 .  \nconst    9.859958   4.020516   2.452   0.0151 *  \ntrend    0.007527   0.013381   0.563   0.5745    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 5.446 on 186 degrees of freedom\nMultiple R-Squared: 0.938,  Adjusted R-squared: 0.935 \nF-statistic: 312.5 on 9 and 186 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation HH: \n=================================== \nHH = ICLN.l1 + XLE.l1 + WTI.l1 + HH.l1 + ICLN.l2 + XLE.l2 + WTI.l2 + HH.l2 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nICLN.l1 -2.617e-03  4.105e-03  -0.638   0.5246    \nXLE.l1   7.719e-03  3.452e-03   2.236   0.0265 *  \nWTI.l1  -1.103e-03  9.511e-04  -1.160   0.2476    \nHH.l1    1.535e+00  4.300e-02  35.702   &lt;2e-16 ***\nICLN.l2  6.178e-03  3.877e-03   1.593   0.1128    \nXLE.l2  -4.754e-03  3.532e-03  -1.346   0.1799    \nWTI.l2   1.083e-03  9.561e-04   1.133   0.2589    \nHH.l2   -8.238e-01  4.265e-02 -19.313   &lt;2e-16 ***\nconst    6.222e-01  5.469e-02  11.377   &lt;2e-16 ***\ntrend    5.087e-05  1.820e-04   0.279   0.7802    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.07409 on 186 degrees of freedom\nMultiple R-Squared: 0.9277, Adjusted R-squared: 0.9242 \nF-statistic: 265.1 on 9 and 186 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n        ICLN      XLE       WTI        HH\nICLN 1.83691  0.92471  1.619222  0.019137\nXLE  0.92471  3.21397  5.571406 -0.006080\nWTI  1.61922  5.57141 29.658738 -0.007012\nHH   0.01914 -0.00608 -0.007012  0.005489\n\nCorrelation matrix of residuals:\n       ICLN      XLE      WTI       HH\nICLN 1.0000  0.38057  0.21937  0.19058\nXLE  0.3806  1.00000  0.57065 -0.04578\nWTI  0.2194  0.57065  1.00000 -0.01738\nHH   0.1906 -0.04578 -0.01738  1.00000\n\n\n\n\nCode\nsummary(vars::VAR(df_ts, p=10, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: ICLN, XLE, WTI, HH \nDeterministic variables: both \nSample size: 188 \nLog Likelihood: -798.078 \nRoots of the characteristic polynomial:\n0.9976 0.9976 0.9914 0.9914 0.9352 0.9352 0.9096 0.9096 0.9041 0.9041 0.8793 0.8755 0.8755 0.8703 0.8703 0.8644 0.8644 0.8562 0.8562 0.8459 0.8459 0.8427 0.8427 0.824 0.824 0.8237 0.8237 0.8171 0.8171 0.8098 0.8098 0.8012 0.8012 0.7983 0.7983 0.7276 0.7276 0.7182 0.7182 0.4225\nCall:\nvars::VAR(y = df_ts, p = 10, type = \"both\")\n\n\nEstimation results for equation ICLN: \n===================================== \nICLN = ICLN.l1 + XLE.l1 + WTI.l1 + HH.l1 + ICLN.l2 + XLE.l2 + WTI.l2 + HH.l2 + ICLN.l3 + XLE.l3 + WTI.l3 + HH.l3 + ICLN.l4 + XLE.l4 + WTI.l4 + HH.l4 + ICLN.l5 + XLE.l5 + WTI.l5 + HH.l5 + ICLN.l6 + XLE.l6 + WTI.l6 + HH.l6 + ICLN.l7 + XLE.l7 + WTI.l7 + HH.l7 + ICLN.l8 + XLE.l8 + WTI.l8 + HH.l8 + ICLN.l9 + XLE.l9 + WTI.l9 + HH.l9 + ICLN.l10 + XLE.l10 + WTI.l10 + HH.l10 + const + trend \n\n          Estimate Std. Error t value Pr(&gt;|t|)    \nICLN.l1   0.960815   0.085696  11.212  &lt; 2e-16 ***\nXLE.l1   -0.089903   0.061533  -1.461  0.14615    \nWTI.l1    0.036279   0.020365   1.781  0.07691 .  \nHH.l1    -0.962577   1.401036  -0.687  0.49314    \nICLN.l2  -0.117140   0.122245  -0.958  0.33953    \nXLE.l2    0.039658   0.081561   0.486  0.62753    \nWTI.l2   -0.057129   0.027918  -2.046  0.04252 *  \nHH.l2     1.677313   2.061402   0.814  0.41716    \nICLN.l3   0.078355   0.126047   0.622  0.53515    \nXLE.l3    0.046899   0.084020   0.558  0.57757    \nWTI.l3   -0.001051   0.027921  -0.038  0.97002    \nHH.l3    -2.028533   2.213714  -0.916  0.36100    \nICLN.l4   0.051802   0.129262   0.401  0.68919    \nXLE.l4    0.015596   0.086265   0.181  0.85679    \nWTI.l4    0.014945   0.027217   0.549  0.58376    \nHH.l4     2.482475   2.241790   1.107  0.26996    \nICLN.l5  -0.018815   0.132024  -0.143  0.88687    \nXLE.l5   -0.040018   0.085370  -0.469  0.63994    \nWTI.l5    0.006144   0.027340   0.225  0.82251    \nHH.l5     1.441489   2.316019   0.622  0.53465    \nICLN.l6  -0.111398   0.130599  -0.853  0.39507    \nXLE.l6   -0.051979   0.085256  -0.610  0.54302    \nWTI.l6    0.003298   0.027065   0.122  0.90317    \nHH.l6    -1.989874   2.325571  -0.856  0.39359    \nICLN.l7  -0.009801   0.130216  -0.075  0.94010    \nXLE.l7   -0.031555   0.083255  -0.379  0.70522    \nWTI.l7   -0.015983   0.026089  -0.613  0.54106    \nHH.l7     2.201162   2.286229   0.963  0.33724    \nICLN.l8  -0.030785   0.124203  -0.248  0.80459    \nXLE.l8    0.120003   0.081290   1.476  0.14203    \nWTI.l8    0.022604   0.026069   0.867  0.38733    \nHH.l8    -2.273989   2.267702  -1.003  0.31763    \nICLN.l9   0.184292   0.115481   1.596  0.11268    \nXLE.l9   -0.243959   0.077047  -3.166  0.00188 ** \nWTI.l9   -0.014611   0.024679  -0.592  0.55473    \nHH.l9    -0.019610   2.073943  -0.009  0.99247    \nICLN.l10 -0.114429   0.074358  -1.539  0.12600    \nXLE.l10   0.098519   0.063576   1.550  0.12340    \nWTI.l10   0.019693   0.015155   1.299  0.19583    \nHH.l10    2.601908   1.338409   1.944  0.05381 .  \nconst    -5.755148   4.714494  -1.221  0.22415    \ntrend     0.014612   0.003447   4.239 3.96e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.038 on 146 degrees of freedom\nMultiple R-Squared: 0.9671, Adjusted R-squared: 0.9579 \nF-statistic: 104.7 on 41 and 146 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation XLE: \n==================================== \nXLE = ICLN.l1 + XLE.l1 + WTI.l1 + HH.l1 + ICLN.l2 + XLE.l2 + WTI.l2 + HH.l2 + ICLN.l3 + XLE.l3 + WTI.l3 + HH.l3 + ICLN.l4 + XLE.l4 + WTI.l4 + HH.l4 + ICLN.l5 + XLE.l5 + WTI.l5 + HH.l5 + ICLN.l6 + XLE.l6 + WTI.l6 + HH.l6 + ICLN.l7 + XLE.l7 + WTI.l7 + HH.l7 + ICLN.l8 + XLE.l8 + WTI.l8 + HH.l8 + ICLN.l9 + XLE.l9 + WTI.l9 + HH.l9 + ICLN.l10 + XLE.l10 + WTI.l10 + HH.l10 + const + trend \n\n           Estimate Std. Error t value Pr(&gt;|t|)    \nICLN.l1  -3.842e-01  1.427e-01  -2.691  0.00794 ** \nXLE.l1    8.723e-01  1.025e-01   8.511  1.9e-14 ***\nWTI.l1    2.421e-02  3.392e-02   0.714  0.47659    \nHH.l1     1.007e-01  2.334e+00   0.043  0.96564    \nICLN.l2   4.191e-01  2.036e-01   2.059  0.04131 *  \nXLE.l2    3.623e-02  1.358e-01   0.267  0.79009    \nWTI.l2   -3.450e-02  4.650e-02  -0.742  0.45926    \nHH.l2     8.305e-01  3.433e+00   0.242  0.80922    \nICLN.l3   1.290e-01  2.099e-01   0.614  0.53991    \nXLE.l3    1.373e-01  1.399e-01   0.981  0.32803    \nWTI.l3   -4.143e-02  4.651e-02  -0.891  0.37452    \nHH.l3     1.129e+00  3.687e+00   0.306  0.75997    \nICLN.l4  -2.339e-01  2.153e-01  -1.086  0.27911    \nXLE.l4   -1.740e-01  1.437e-01  -1.211  0.22794    \nWTI.l4    8.547e-02  4.533e-02   1.885  0.06137 .  \nHH.l4    -8.429e-01  3.734e+00  -0.226  0.82172    \nICLN.l5   2.376e-01  2.199e-01   1.081  0.28163    \nXLE.l5   -1.092e-01  1.422e-01  -0.768  0.44389    \nWTI.l5   -4.163e-04  4.554e-02  -0.009  0.99272    \nHH.l5     3.978e+00  3.858e+00   1.031  0.30411    \nICLN.l6  -2.979e-01  2.175e-01  -1.369  0.17301    \nXLE.l6    5.766e-02  1.420e-01   0.406  0.68528    \nWTI.l6   -5.933e-03  4.508e-02  -0.132  0.89546    \nHH.l6    -4.646e+00  3.873e+00  -1.199  0.23234    \nICLN.l7   1.625e-02  2.169e-01   0.075  0.94037    \nXLE.l7   -1.312e-05  1.387e-01   0.000  0.99992    \nWTI.l7   -1.379e-03  4.345e-02  -0.032  0.97472    \nHH.l7     5.924e+00  3.808e+00   1.556  0.12194    \nICLN.l8  -9.578e-02  2.069e-01  -0.463  0.64407    \nXLE.l8    2.501e-01  1.354e-01   1.847  0.06680 .  \nWTI.l8   -4.175e-02  4.342e-02  -0.961  0.33791    \nHH.l8    -1.949e+00  3.777e+00  -0.516  0.60657    \nICLN.l9   2.540e-01  1.923e-01   1.321  0.18867    \nXLE.l9   -1.889e-01  1.283e-01  -1.472  0.14322    \nWTI.l9    4.840e-02  4.111e-02   1.178  0.24090    \nHH.l9    -7.533e-01  3.454e+00  -0.218  0.82768    \nICLN.l10 -1.228e-01  1.239e-01  -0.991  0.32314    \nXLE.l10  -2.121e-02  1.059e-01  -0.200  0.84151    \nWTI.l10  -2.138e-02  2.524e-02  -0.847  0.39841    \nHH.l10    3.291e+00  2.229e+00   1.476  0.14204    \nconst    -1.515e+01  7.852e+00  -1.929  0.05570 .  \ntrend     7.506e-03  5.741e-03   1.307  0.19310    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.729 on 146 degrees of freedom\nMultiple R-Squared: 0.9624, Adjusted R-squared: 0.9518 \nF-statistic: 91.08 on 41 and 146 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation WTI: \n==================================== \nWTI = ICLN.l1 + XLE.l1 + WTI.l1 + HH.l1 + ICLN.l2 + XLE.l2 + WTI.l2 + HH.l2 + ICLN.l3 + XLE.l3 + WTI.l3 + HH.l3 + ICLN.l4 + XLE.l4 + WTI.l4 + HH.l4 + ICLN.l5 + XLE.l5 + WTI.l5 + HH.l5 + ICLN.l6 + XLE.l6 + WTI.l6 + HH.l6 + ICLN.l7 + XLE.l7 + WTI.l7 + HH.l7 + ICLN.l8 + XLE.l8 + WTI.l8 + HH.l8 + ICLN.l9 + XLE.l9 + WTI.l9 + HH.l9 + ICLN.l10 + XLE.l10 + WTI.l10 + HH.l10 + const + trend \n\n           Estimate Std. Error t value Pr(&gt;|t|)    \nICLN.l1  -3.482e-01  4.045e-01  -0.861   0.3908    \nXLE.l1    1.682e+00  2.905e-01   5.791 4.14e-08 ***\nWTI.l1    9.581e-01  9.613e-02   9.966  &lt; 2e-16 ***\nHH.l1     1.100e+01  6.613e+00   1.664   0.0983 .  \nICLN.l2  -2.188e-02  5.770e-01  -0.038   0.9698    \nXLE.l2   -1.003e+00  3.850e-01  -2.606   0.0101 *  \nWTI.l2   -1.643e-01  1.318e-01  -1.247   0.2146    \nHH.l2    -3.659e+00  9.731e+00  -0.376   0.7075    \nICLN.l3   9.158e-01  5.950e-01   1.539   0.1259    \nXLE.l3   -7.161e-01  3.966e-01  -1.805   0.0731 .  \nWTI.l3    1.218e-01  1.318e-01   0.924   0.3571    \nHH.l3    -2.661e+00  1.045e+01  -0.255   0.7993    \nICLN.l4  -1.564e+00  6.102e-01  -2.563   0.0114 *  \nXLE.l4   -4.399e-01  4.072e-01  -1.080   0.2818    \nWTI.l4    5.062e-02  1.285e-01   0.394   0.6941    \nHH.l4    -2.012e-02  1.058e+01  -0.002   0.9985    \nICLN.l5   7.253e-01  6.232e-01   1.164   0.2464    \nXLE.l5    1.951e-01  4.030e-01   0.484   0.6290    \nWTI.l5    1.187e-01  1.291e-01   0.919   0.3594    \nHH.l5     1.536e+01  1.093e+01   1.405   0.1622    \nICLN.l6  -1.466e-01  6.165e-01  -0.238   0.8124    \nXLE.l6   -2.924e-02  4.024e-01  -0.073   0.9422    \nWTI.l6   -2.368e-01  1.278e-01  -1.854   0.0658 .  \nHH.l6    -1.928e+01  1.098e+01  -1.756   0.0812 .  \nICLN.l7   7.020e-01  6.147e-01   1.142   0.2553    \nXLE.l7   -4.243e-01  3.930e-01  -1.080   0.2821    \nWTI.l7    1.397e-01  1.232e-01   1.135   0.2584    \nHH.l7     2.007e+01  1.079e+01   1.859   0.0650 .  \nICLN.l8  -8.375e-01  5.863e-01  -1.428   0.1553    \nXLE.l8    5.812e-01  3.837e-01   1.515   0.1320    \nWTI.l8   -8.948e-02  1.231e-01  -0.727   0.4683    \nHH.l8    -7.699e+00  1.070e+01  -0.719   0.4731    \nICLN.l9   1.188e+00  5.451e-01   2.179   0.0310 *  \nXLE.l9   -3.150e-01  3.637e-01  -0.866   0.3878    \nWTI.l9    4.160e-02  1.165e-01   0.357   0.7215    \nHH.l9     6.964e+00  9.790e+00   0.711   0.4780    \nICLN.l10 -8.508e-01  3.510e-01  -2.424   0.0166 *  \nXLE.l10   1.039e-01  3.001e-01   0.346   0.7297    \nWTI.l10   1.979e-02  7.154e-02   0.277   0.7825    \nHH.l10   -3.556e-01  6.318e+00  -0.056   0.9552    \nconst    -3.616e+01  2.225e+01  -1.625   0.1063    \ntrend     9.121e-04  1.627e-02   0.056   0.9554    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 4.901 on 146 degrees of freedom\nMultiple R-Squared: 0.9573, Adjusted R-squared: 0.9453 \nF-statistic:  79.8 on 41 and 146 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation HH: \n=================================== \nHH = ICLN.l1 + XLE.l1 + WTI.l1 + HH.l1 + ICLN.l2 + XLE.l2 + WTI.l2 + HH.l2 + ICLN.l3 + XLE.l3 + WTI.l3 + HH.l3 + ICLN.l4 + XLE.l4 + WTI.l4 + HH.l4 + ICLN.l5 + XLE.l5 + WTI.l5 + HH.l5 + ICLN.l6 + XLE.l6 + WTI.l6 + HH.l6 + ICLN.l7 + XLE.l7 + WTI.l7 + HH.l7 + ICLN.l8 + XLE.l8 + WTI.l8 + HH.l8 + ICLN.l9 + XLE.l9 + WTI.l9 + HH.l9 + ICLN.l10 + XLE.l10 + WTI.l10 + HH.l10 + const + trend \n\n           Estimate Std. Error t value Pr(&gt;|t|)    \nICLN.l1  -0.0062481  0.0047561  -1.314  0.19101    \nXLE.l1    0.0104391  0.0034151   3.057  0.00266 ** \nWTI.l1   -0.0026226  0.0011302  -2.320  0.02170 *  \nHH.l1     1.0305326  0.0777577  13.253  &lt; 2e-16 ***\nICLN.l2   0.0177071  0.0067846   2.610  0.01000 *  \nXLE.l2   -0.0002379  0.0045267  -0.053  0.95816    \nWTI.l2    0.0018559  0.0015494   1.198  0.23295    \nHH.l2    -0.5624245  0.1144082  -4.916 2.35e-06 ***\nICLN.l3  -0.0156439  0.0069956  -2.236  0.02685 *  \nXLE.l3   -0.0005743  0.0046631  -0.123  0.90216    \nWTI.l3    0.0001514  0.0015496   0.098  0.92232    \nHH.l3     0.2452325  0.1228615   1.996  0.04779 *  \nICLN.l4   0.0114585  0.0071741   1.597  0.11238    \nXLE.l4   -0.0057886  0.0047877  -1.209  0.22860    \nWTI.l4    0.0006823  0.0015105   0.452  0.65214    \nHH.l4    -0.3544027  0.1244197  -2.848  0.00503 ** \nICLN.l5  -0.0112593  0.0073273  -1.537  0.12655    \nXLE.l5    0.0033003  0.0047380   0.697  0.48719    \nWTI.l5    0.0000999  0.0015174   0.066  0.94759    \nHH.l5     0.1099939  0.1285395   0.856  0.39355    \nICLN.l6   0.0041498  0.0072483   0.573  0.56785    \nXLE.l6   -0.0029047  0.0047317  -0.614  0.54025    \nWTI.l6    0.0001271  0.0015021   0.085  0.93267    \nHH.l6    -0.0103274  0.1290696  -0.080  0.93634    \nICLN.l7   0.0056376  0.0072270   0.780  0.43661    \nXLE.l7    0.0015062  0.0046206   0.326  0.74491    \nWTI.l7   -0.0014474  0.0014479  -1.000  0.31915    \nHH.l7    -0.1567315  0.1268861  -1.235  0.21873    \nICLN.l8  -0.0073996  0.0068933  -1.073  0.28484    \nXLE.l8    0.0024944  0.0045116   0.553  0.58118    \nWTI.l8    0.0022716  0.0014468   1.570  0.11858    \nHH.l8     0.0233959  0.1258579   0.186  0.85279    \nICLN.l9   0.0115103  0.0064092   1.796  0.07458 .  \nXLE.l9   -0.0057153  0.0042761  -1.337  0.18344    \nWTI.l9   -0.0022826  0.0013697  -1.666  0.09776 .  \nHH.l9    -0.2175255  0.1151042  -1.890  0.06077 .  \nICLN.l10 -0.0022518  0.0041269  -0.546  0.58615    \nXLE.l10   0.0008807  0.0035285   0.250  0.80325    \nWTI.l10   0.0014048  0.0008411   1.670  0.09702 .  \nHH.l10    0.3350675  0.0742819   4.511 1.32e-05 ***\nconst     1.1941263  0.2616552   4.564 1.06e-05 ***\ntrend     0.0003923  0.0001913   2.051  0.04208 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.05762 on 146 degrees of freedom\nMultiple R-Squared: 0.9648, Adjusted R-squared: 0.9549 \nF-statistic: 97.66 on 41 and 146 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n         ICLN       XLE      WTI        HH\nICLN 1.077943  0.569269  0.68140  0.007008\nXLE  0.569269  2.990428  4.59490 -0.008051\nWTI  0.681398  4.594904 24.01910  0.003550\nHH   0.007008 -0.008051  0.00355  0.003320\n\nCorrelation matrix of residuals:\n       ICLN     XLE     WTI       HH\nICLN 1.0000  0.3171 0.13391  0.11714\nXLE  0.3171  1.0000 0.54216 -0.08080\nWTI  0.1339  0.5422 1.00000  0.01257\nHH   0.1171 -0.0808 0.01257  1.00000\n\n\n\n\nThe cross-validation results are very similar. Considering the principle of parsimony, we select VAR(2) as the best model.\n\n\nCode\ndf_ts &lt;- df_ts  \n\ny &lt;- df_ts  \nn &lt;- nrow(df_ts)        # total observations, here 198\nh &lt;- 4                  # forecast horizon = 4 months\nR &lt;- 60                 # choose rolling steps (e.g., 60 months)\nk &lt;- n - h + 1 - R      # initial training size\n\ncat(\"Initial training size k =\", k, \"\\n\")\n\n\nInitial training size k = 135 \n\n\nCode\ncat(\"Rolling steps R =\", R, \"\\n\")\n\n\nRolling steps R = 60 \n\n\nCode\n# RMSE matrices: R rows × 4 forecast horizons × 4 variables\nvars_names &lt;- colnames(df_ts)\nrmse_VAR1 &lt;- array(NA, dim = c(R, h, 4),\n                   dimnames = list(NULL, paste0(\"h\",1:4), vars_names))\nrmse_VAR2 &lt;- array(NA, dim = c(R, h, 4),\n                   dimnames = list(NULL, paste0(\"h\",1:4), vars_names))\n\n# 2. Rolling Forecast Loop\n\nfor (i in 1:R) {\n  \n  # ---- training index ----\n  train_end &lt;- k + i - 1\n  xtrain &lt;- y[1:train_end, ]\n  \n  # ---- test index (next 4 months) ----\n  xtest &lt;- y[(train_end + 1):(train_end + h), ]\n  \n  # VAR(8)\n  fit1 &lt;- VAR(xtrain, p = 2, type = \"const\")\n  fc1 &lt;- predict(fit1, n.ahead = h)\n  \n  # Extract forecasts as matrix h × 4\n  f1mat &lt;- sapply(fc1$fcst, function(x) x[,1])\n  \n  # RMSE for each horizon and variable\n  rmse_VAR1[i, , ] &lt;- sqrt((f1mat - xtest)^2)\n  \n  # VAR(10)\n  fit2 &lt;- VAR(xtrain, p = 10, type = \"const\")\n  fc2 &lt;- predict(fit2, n.ahead = h)\n  \n  f2mat &lt;- sapply(fc2$fcst, function(x) x[,1])\n  \n  rmse_VAR2[i, , ] &lt;- sqrt((f2mat - xtest)^2)\n}\n\n# 3. Compute Average RMSE\n\navg_rmse_VAR1 &lt;- apply(rmse_VAR1, c(2,3), mean, na.rm = TRUE)\navg_rmse_VAR2 &lt;- apply(rmse_VAR2, c(2,3), mean, na.rm = TRUE)\n\nprint(\"Average RMSE (VAR(2)):\")\n\n\n[1] \"Average RMSE (VAR(2)):\"\n\n\nCode\nprint(avg_rmse_VAR1)\n\n\n       ICLN      XLE       WTI         HH\nh1 1.482565 2.053367  5.407625 0.07474277\nh2 2.377303 2.905445 10.324143 0.13671566\nh3 3.238259 3.759653 13.775922 0.16297526\nh4 3.837131 4.700796 16.916775 0.17890289\n\n\nCode\nprint(\"Average RMSE (VAR(10)):\")\n\n\n[1] \"Average RMSE (VAR(10)):\"\n\n\nCode\nprint(avg_rmse_VAR2)\n\n\n       ICLN      XLE       WTI         HH\nh1 1.607588 2.198545  5.477483 0.07843172\nh2 2.461865 3.245926 10.171812 0.09307699\nh3 3.080793 4.066069 12.977369 0.10449711\nh4 3.768308 5.100467 16.209512 0.11204263\n\n\nCode\n# 4. Example Plot: Solar RMSE\ndf_plot &lt;- data.frame(\n  step = rep(1:R, 2),\n  rmse = c(rmse_VAR1[,1,\"ICLN\"], rmse_VAR2[,1,\"ICLN\"]),\n  model = rep(c(\"VAR(2)\", \"VAR(10)\"), each = R)\n)\n\nggplot(df_plot, aes(x = step, y = rmse, color = model)) +\n  geom_line() +\n  labs(title = \"1-step-ahead RMSE for ICLN (Rolling Forecast)\",\n       x = \"Rolling step\",\n       y = \"RMSE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nThe HH forecast maintains strong seasonality and shows slight mean reversion, with widening confidence intervals indicating uncertainty in future volatility.\nThe ICLN forecast tends to be stable, with no significant upward or downward trend, reflecting the recent lack of directional drivers for clean energy stocks.\nThe WTI forecast is flat, not continuing the large fluctuations seen historically, suggesting that oil prices are returning to their long-term equilibrium level within the model framework.\nThe XLE forecast also converges, indicating that the energy stock index is strongly dominated by its own lagged terms rather than external shocks.\n\nOverall, the gradually expanding prediction intervals and stable central path show that the VAR(2) model provides a robust but conservative short-term outlook, emphasizing mean reversion rather than structural trend identification.\n\n\nCode\nfit &lt;- VAR(df_ts, p = 2, type = \"both\")\nforecast(fit, h=32) %&gt;%\n  autoplot() +\n  xlab(\"Year\") +\n  theme_minimal()"
  },
  {
    "objectID": "multivariate.html#arimax-natural-gas-temperature-hh-wti",
    "href": "multivariate.html#arimax-natural-gas-temperature-hh-wti",
    "title": "ARIMAX, SARIMAX, and VAR",
    "section": "(5) ARIMAX: Natural gas ~ temperature + HH + WTI",
    "text": "(5) ARIMAX: Natural gas ~ temperature + HH + WTI\n\nVisualizationauto.arima()Manual ModelModel DiagnosticsCVFinal Model FitForecasting\n\n\nTemperature primarily represents seasonal demand shocks, HH prices reflect the supply and demand balance within the natural gas market itself, and WTI influences substitution and co-production activities across energy markets. Together, these factors constitute the core economic structure of natural gas demand and price dynamics.\n\n\nCode\ndf &lt;- read.csv(\"data/electric.csv\")\ndf &lt;- df %&gt;%\n  mutate(\n    Description = str_extract(Description, \"(?&lt;=From ).*?(?=,)\"),\n    YYYYMM = as.character(YYYYMM),\n    Year = as.integer(substr(YYYYMM, 1, 4)),\n    Month = as.integer(substr(YYYYMM, 5, 6))\n  ) %&gt;%\n  filter(Month &lt;= 12) %&gt;%\n  filter(Year &gt;= 2000) %&gt;%\n  filter(Year &lt;= 2024) %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))\n\nn_df &lt;- df %&gt;%\n  filter(Description == \"Natural Gas\") %&gt;%\n  filter(!is.na(Value)) %&gt;% \n  mutate(Natural_gas = as.numeric(Value)) %&gt;%\n  arrange(Date)\n\nts_n &lt;- ts(n_df$Value,\n              start = c(min(solar_df$Year), min(solar_df$Month)),\n              frequency = 12)\n\n\nwti &lt;- read.csv(\"data/WTIprice.csv\")\nwti &lt;- wti %&gt;%\n  rename(Date = observation_date) %&gt;%\n  mutate(Date = as.Date(Date)) %&gt;%\n  filter(Date &gt;= as.Date(\"2000-01-01\")) %&gt;%\n  filter(Date &lt;= as.Date(\"2024-12-01\")) %&gt;%\n  rename(WTI = POILWTIUSDM) %&gt;%\n  mutate(WTI = as.numeric(WTI))%&gt;%\n  arrange(Date)\n\nts_wti &lt;- ts(wti$WTI,\n              start = c(2000, 1),\n              frequency = 12)\n\ndf_n &lt;- read.csv(\"data/naturalgasprice.csv\")\ndf_n &lt;- df_n %&gt;%\n  mutate(\n    YYYYMM = as.character(YYYYMM),\n    Year = as.integer(substr(YYYYMM, 1, 4)),\n    Month = as.integer(substr(YYYYMM, 5, 6))\n  ) %&gt;%\n  filter(Month &lt;= 12) %&gt;%\n  filter(Year &gt;= 2000) %&gt;%\n  filter(Year &lt;= 2024) %&gt;%\n  mutate(HH = as.numeric(Value)) %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))%&gt;%\n  mutate(HH = log(pmax(HH, .Machine$double.eps))) %&gt;%\n  arrange(Date)\n\nts_np &lt;- ts(df_n$HH,\n              start = c(min(df_n$Year), min(df_n$Month)),\n              frequency = 12)\n\ndf &lt;- read.csv(\"data/Global_Temperature_2000onwards.csv\")\ndf &lt;- df[order(df$Year, df$Month), ]\ndf_temp &lt;- df %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, \"01\", sep = \"-\")))\n\nts_anomaly &lt;- ts(df_temp$Monthly_Anomaly,\n                 start = c(2000, 1),   \n                 frequency = 12)     \n\n\n\n\nCode\ndata_all2 &lt;- n_df %&gt;%\n  full_join(df_temp, by = \"Date\") %&gt;%\n  full_join(wti, by = \"Date\") %&gt;%\n  full_join(df_n, by = \"Date\") %&gt;%\n  arrange(Date)\n\n# Create individual plots for each variable with y-axis labels\np1 &lt;- plot_ly(data_all2, x = ~Date, y = ~Natural_gas, type = 'scatter', mode = 'lines', name = \"Natural Gas Electricity Generation\") %&gt;%\n  layout(yaxis = list(title = \"Value\"))\n\np2 &lt;- plot_ly(data_all2, x = ~Date, y = ~Monthly_Anomaly, type = 'scatter', mode = 'lines', name = \"Global Average Temperature Anomaly\") %&gt;%\n  layout(yaxis = list(title = \"Temperature (C)\"))\n\np3 &lt;- plot_ly(data_all2, x = ~Date, y = ~WTI, type = 'scatter', mode = 'lines', name = \"WTI Oil price\") %&gt;%\n  layout(yaxis = list(title = \"Price\"))\n\np4 &lt;- plot_ly(data_all2, x = ~Date, y = ~HH, type = 'scatter', mode = 'lines', name = \"Henry Hub Natural Gas price\") %&gt;%\n  layout(yaxis = list(title = \"Price\"))\n\n# Combine the individual plots into a single figure with subplots and y-axis labels\nfinal_plot &lt;- subplot(p1, p2, p3, p4, nrows = 4, shareX = TRUE, titleX = TRUE) %&gt;%\n  layout(\n    title = \"Time Series of Natural gas power generation, temperature, and oil and gas prices\",\n    yaxis = list(title = \"Value\"), \n    yaxis2 = list(title = \"Temperature (C)\"), \n    yaxis3 = list(title = \"Price\"),\n    yaxis4 = list(title = \"Price\"),\n    xaxis = list(title = \"Year\")\n  )\n\nfinal_plot\n\n\n\n\n\n\n\n\nThe ARIMA(4,1,0) model generally captures the directionality, but the residual structure is still not ideal, indicating that the current ARIMAX specification may be underfitting and requires the inclusion of seasonality or alternative lag orders.\n\n\nCode\nX &lt;- cbind(\n  anomaly = as.numeric(ts_anomaly),\n  wti     = as.numeric(ts_wti),\n  np      = as.numeric(ts_np)\n)\n\nfit_auto &lt;- auto.arima(\n  as.numeric(ts_n),\n  xreg = X)\n\n\nsummary(fit_auto)\n\n\nSeries: as.numeric(ts_n) \nRegression with ARIMA(4,1,0) errors \n\nCoefficients:\n          ar1      ar2      ar3      ar4   anomaly       wti         np\n      -0.1339  -0.3445  -0.3984  -0.2631  4125.500  -48.2789  62325.321\ns.e.   0.0582   0.0529   0.0517   0.0563  5066.948   80.9044   4036.709\n\nsigma^2 = 105186192:  log likelihood = -3182.66\nAIC=6381.33   AICc=6381.83   BIC=6410.93\n\nTraining set error measures:\n                   ME     RMSE      MAE      MPE     MAPE      MASE        ACF1\nTraining set 402.8289 10118.36 7933.104 -0.82718 9.157407 0.7122981 -0.01555444\n\n\n\n\nCode\ncheckresiduals(fit_auto)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(4,1,0) errors\nQ* = 32.023, df = 6, p-value = 1.615e-05\n\nModel df: 4.   Total lags used: 10\n\n\n\n\n\n\nCode\nlm_fit &lt;- lm(Natural_gas ~ Monthly_Anomaly + WTI + HH, data = data_all2)\nsummary(lm_fit)\n\n\n\nCall:\nlm(formula = Natural_gas ~ Monthly_Anomaly + WTI + HH, data = data_all2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-59790 -14874    632  16338  53566 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -148694.67   11886.85 -12.509   &lt;2e-16 ***\nMonthly_Anomaly   82988.81    5361.43  15.479   &lt;2e-16 ***\nWTI                  37.29      55.35   0.674    0.501    \nHH                69161.93    5263.33  13.140   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21390 on 296 degrees of freedom\nMultiple R-squared:  0.6754,    Adjusted R-squared:  0.6721 \nF-statistic: 205.3 on 3 and 296 DF,  p-value: &lt; 2.2e-16\n\n\nFrom the regression model, temperature and natural gas price are statistically significant, indicating that they have a meaningful impact on total natural gas electricity generation.\n\n\nCode\nacf(residuals(lm_fit))\n\n\n\n\n\n\n\n\n\nMoreover, the residuals exhibit a high correlation, indicating significant serial correlation that remains unexplained. This suggests that traditional machine learning models may struggle to fully capture the underlying temporal patterns in these time series variables.\n\n\nCode\nres.fit &lt;- ts(residuals(lm_fit), frequency = 12)\np1 &lt;- ggAcf(res.fit)\np2 &lt;- ggPacf(res.fit)\n(p1 | p2)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(patchwork)\nfirst_diff &lt;- diff(res.fit)  \n\nseasonal_diff &lt;- diff(first_diff, lag = 12)\n\n###### Plot ACF and PACF for the Differenced Series ######\np1 &lt;- ggAcf(first_diff) +\n  ggtitle(\"ACF of first Differenced Residuals\") +\n  theme_minimal()\n\np2 &lt;- ggPacf(first_diff) +\n  ggtitle(\"PACF of first Differenced Residuals\") +\n  theme_minimal()\n\n# Display plots\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\np3 &lt;- ggAcf(seasonal_diff) +\n  ggtitle(\"ACF of Seasonally Differenced Residuals\") +\n  theme_minimal()\n\np4 &lt;- ggPacf(seasonal_diff) +\n  ggtitle(\"PACF of Seasonally Differenced Residuals\") +\n  theme_minimal()\n\n# Display plots\ngridExtra::grid.arrange(p3, p4, ncol = 2)\n\n\n\n\n\n\n\n\n\nThis series appears closer to weak stationarity. The ACF plot shows clear autocorrelations at lags q = 0,1,2,3 and Q = 1,2,3.\nAdditionally, both first-order differencing (d = 1) and seasonal differencing (D = 1) have been applied to the data.\nThe PACF plot shows clear autocorrelations at lags p = 0,1,2,3,4 and P = 0,1,2,3.\n\n\nCode\n###### Define SARIMA Model Comparison Function ######\nSARIMA.c &lt;- function(p_range, d_range, q_range, P_range, D_range, Q_range, data) {\n  \n  # Set seasonal period\n  s &lt;- 12\n  \n  # Initialize results storage\n  results_list &lt;- list()\n  \n  # Iterate over parameter combinations\n  for (p in p_range) {\n    for (d in d_range) {\n      for (q in q_range) {\n        for (P in P_range) {\n          for (D in D_range) {\n            for (Q in Q_range) {\n              \n              if (p + d + q + P + D + Q &lt;= 10) {\n                result &lt;- tryCatch({\n                  model &lt;- Arima(\n                    data,\n                    order = c(p, d, q),\n                    seasonal = list(order = c(P, D, Q), period = s)\n                  )\n                  \n                  c(p, d, q, P, D, Q,\n                    AIC(model),\n                    BIC(model),\n                    model$aicc)\n                  \n                }, error = function(e) {\n                  c(p, d, q, P, D, Q, NA, NA, NA)\n                })\n                \n                results_list[[length(results_list) + 1]] &lt;- result\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  \n  # Convert results to a tidy data frame\n  results_df &lt;- as.data.frame(do.call(rbind, results_list))\n  colnames(results_df) &lt;- c(\"p\", \"d\", \"q\", \"P\", \"D\", \"Q\", \"AIC\", \"BIC\", \"AICc\")\n  \n  return(results_df)\n}\n\n###### Run SARIMA Model Comparison ######\noutput &lt;- SARIMA.c(\n  p_range = 0:4, q_range = 0:3, \n  d_range = 1, D_range = 1, \n  P_range = 0:3, Q_range = 0:3, \n  data = res.fit\n)\n\n###### Identify Models with Minimum AIC and BIC ######\nminaic &lt;- output[which.min(output$AIC), ]\nminbic &lt;- output[which.min(output$BIC), ]\n\n###### Display Best Models Based on AIC and BIC ######\nprint(minaic)\n\n\n    p d q P D Q      AIC     BIC     AICc\n156 2 1 2 0 1 1 6174.973 6196.93 6175.273\n\n\n\n\nThe model ARIMA(2,1,2)x(0,1,1)[12] works siginificantly better than ARIMA(4,1,0) due to the lower AIC, BIC, and AICc numbers.\n\n\nCode\nlibrary(astsa)\nmodel_output_1 &lt;- capture.output(sarima(res.fit, 2, 1, 2, 0, 1, 1, 12))\n\n\n\n\n\n\n\n\n\nCode\n###### Function to Extract Model Diagnostics ######\nextract_model_diagnostics &lt;- function(model_output) {\n  start_line &lt;- grep(\"Coefficients\", model_output)  # Find where coefficients start\n  end_line &lt;- length(model_output)  # Capture till the last line\n  if (length(start_line) &gt; 0) {\n    cat(model_output[start_line:end_line], sep = \"\\n\")  # Print coefficient section\n  } else {\n    cat(\"No coefficient details found.\\n\")  # Handle cases where output format changes\n  }\n}\n\n###### Extract and Print Model Diagnostics ######\ncat(\"### ARIMA(2,1,2)x(0,1,1)[12] ###\\n\")\n\n\n### ARIMA(2,1,2)x(0,1,1)[12] ###\n\n\nCode\nextract_model_diagnostics(model_output_1)\n\n\nCoefficients: \n     Estimate     SE  t.value p.value\nar1   -0.1671 0.1309  -1.2764  0.2029\nar2    0.5522 0.0759   7.2781  0.0000\nma1   -0.2340 0.1377  -1.6994  0.0904\nma2   -0.6145 0.1145  -5.3683  0.0000\nsma1  -0.8267 0.0458 -18.0527  0.0000\n\nsigma^2 estimated as 117746620 on 282 degrees of freedom \n \nAIC = 21.51558  AICc = 21.51633  BIC = 21.59209 \n \n\n\n\n\nCode\nmodel_output_2 &lt;- capture.output(sarima(res.fit, 4, 1, 0))\n\n\n\n\n\n\n\n\n\nCode\n###### Extract and Print Model Diagnostics ######\ncat(\"### ARIMA(4,1,0) ###\\n\")\n\n\n### ARIMA(4,1,0) ###\n\n\nCode\nextract_model_diagnostics(model_output_2)\n\n\nCoefficients: \n         Estimate       SE t.value p.value\nar1       -0.1969   0.0567 -3.4719  0.0006\nar2       -0.1995   0.0511 -3.9047  0.0001\nar3       -0.4763   0.0510 -9.3452  0.0000\nar4       -0.2042   0.0570 -3.5827  0.0004\nconstant -62.7302 375.9731 -0.1668  0.8676\n\nsigma^2 estimated as 180646297 on 294 degrees of freedom \n \nAIC = 21.89311  AICc = 21.8938  BIC = 21.96737 \n \n\n\n\n\nUsing corss validation, we can find that ARIMA(2,1,2)x(0,1,1)[12] is the better model since is stays at a lower RMSE for the majority of the time in the plot below.\n\n\nCode\ny &lt;- as.numeric(res.fit)\nn &lt;- length(y)\nk &lt;- 90   # initial training size\n\nrmse1 &lt;- matrix(NA, 27, 4)\nrmse2 &lt;- matrix(NA, 27, 4)\n\nfor (i in 1:27) {\n\n  # training = first (k+i-1) observations\n  xtrain &lt;- y[1:(k+i-1)]\n  \n  # test = next 4 observations\n  xtest  &lt;- y[(k+i):(k+i+3)]\n  \n  # fit ARIMA models\n  fit &lt;- Arima(ts(xtrain, frequency=12),\n               order=c(2,1,2),\n               seasonal=list(order=c(0,1,1), period=12))\n\n  fcast &lt;- forecast(fit, h=4)\n\n  fit2 &lt;- Arima(ts(xtrain, frequency=12),\n                order=c(4,1,0))\n\n  fcast2 &lt;- forecast(fit2, h=4)\n\n  # ensure xtest length = 4\n  if (length(xtest)==4 && length(fcast$mean)==4) {\n    rmse1[i,] &lt;- sqrt((fcast$mean - xtest)^2)\n    rmse2[i,] &lt;- sqrt((fcast2$mean - xtest)^2)\n  }\n}\n\n# final RMSE\ncolMeans(rmse1, na.rm=TRUE)\n\n\n[1] 11173.86 14727.87 16605.97 17738.84\n\n\nCode\ncolMeans(rmse2, na.rm=TRUE)\n\n\n[1] 11169.59 13342.43 12813.27 11401.01\n\n\n\n\nCode\n##### RMSE Plot using Plotly #####\n# Create a dataframe for RMSE values by quarter\nqr &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n\nrmse11 &lt;- data.frame(Quarter = qr, RMSE = colMeans(rmse1, na.rm = TRUE))\nrmse22 &lt;- data.frame(Quarter = qr, RMSE = colMeans(rmse2, na.rm = TRUE))\n\n# Generate an interactive RMSE comparison plot\nplot_ly() %&gt;%\n  add_lines(data = rmse11, x = ~Quarter, y = ~RMSE, \n            color = I(\"blue\"), name = \"RMSE1: ARIMA(2,1,2)x(0,1,1)[12]\") %&gt;%\n  add_lines(data = rmse22, x = ~Quarter, y = ~RMSE, \n            color = I(\"red\"), name = \"RMSE2: ARIMA(4,1,0)\") %&gt;%\n  layout(\n    title = \"Cross-Validation RMSE\",\n    xaxis = list(title = \"Quarter\"),\n    yaxis = list(title = \"RMSE\"),\n    legend = list(title = \"Model\")\n  )\n\n\n\n\n\n\n\n\n\n\nCode\n# Define the dependent variable (TotalVehicleSales)\ny &lt;- as.numeric(y)\nts_anomaly &lt;- as.numeric(ts_anomaly)\nts_wti     &lt;- as.numeric(ts_wti)\nts_np      &lt;- as.numeric(ts_np)\n\n# Define external regressors (all variables except TotalVehicleSales)\nxreg &lt;- cbind(ts_anomaly, ts_wti, ts_np)\n\n# Fit ARIMA(2,1,0)x(0,0,1)[12] with external regressors\nfit &lt;- Arima(y, order = c(2,1,2), seasonal = list(order = c(0,1,1), period = 12), xreg = xreg)\n#fit1 &lt;- Arima(y, order = c(2,1,2), seasonal = list(order = c(2,0,1), period = 12), xreg = xreg)\n# Display model summary\nsummary(fit)\n\n\nSeries: y \nRegression with ARIMA(2,1,2)(0,1,1)[12] errors \n\nCoefficients:\n          ar1     ar2      ma1      ma2     sma1  ts_anomaly    ts_wti\n      -0.1907  0.6612  -0.1396  -0.8344  -0.6714  -78834.931  -43.9766\ns.e.   0.0799  0.0585   0.0786   0.0759   0.0446    2812.808   38.9504\n           ts_np\n      -82497.919\ns.e.    5516.879\n\nsigma^2 = 28024043:  log likelihood = -2869.2\nAIC=5756.4   AICc=5757.05   BIC=5789.34\n\nTraining set error measures:\n                  ME     RMSE      MAE      MPE     MAPE      MASE         ACF1\nTraining set 274.502 5105.131 4010.581 4.325729 64.16456 0.3178684 -0.007055458\n\n\n\n\n\n\nCode\n# Fit ARIMA model to each external regressor\ntemp_fit &lt;- auto.arima(xreg[, \"ts_anomaly\"])\n#summary(FinR_fit)\nftemp &lt;- forecast(temp_fit, h = 32)  # Forecast next 32 periods\n\nwti_fit &lt;- auto.arima(xreg[, \"ts_wti\"])\n#summary(Imp_fit)\nfwti &lt;- forecast(wti_fit, h = 32)\n\nhh_fit &lt;- auto.arima(xreg[, \"ts_np\"])\n#summary(CPI_fit)\nfhh &lt;- forecast(hh_fit, h = 32)\n\n\n\n\nCode\n# Create future external regressor matrix using forecasted values\nfxreg &lt;- cbind(\n  anomaly = as.numeric(ftemp$mean),\n  wti  = as.numeric(fwti$mean),\n  np   = as.numeric(fhh$mean)\n)\n\n# Fit ARIMA(2,0,2)x(0,0,1)[12] with external regressors\nfit &lt;- Arima(y, order = c(2,1,2), seasonal = list(order = c(0,1,1), period = 12), xreg = xreg)\n# Forecast TotalVehicleSales using future external regressors\nfcast &lt;- forecast(fit, xreg = fxreg, h = 32)\n\n# Plot the forecast\nautoplot(fcast) +\n  ggtitle(\"ICLN Prices Forecast\") +\n  xlab(\"Year\") +\n  ylab(\"Value\") +\n  theme_minimal()"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conslusion",
    "section": "",
    "text": "This project aims to understand the true nature of the global energy transition from a systemic perspective, and how this transition manifests itself in electricity production, fuel prices, and financial markets. We analyze five key aspects: power generation structure, seasonal patterns, price linkages, financial asset behavior, and multi-energy interactions, thereby constructing a comprehensive framework that spans both physical and financial systems. The research objective is not only to describe trends but also to explore the economic forces and structural changes driving the evolution of the energy system.\n\n\n\n\n\nWe first analyzed historical power generation data and found that coal is experiencing a long-term structural decline, while wind and solar energy are undergoing rapid expansion, accelerating significantly after 2015. Natural gas has grown steadily and has become dominant in recent years, while hydropower and nuclear power have remained relatively stable. The SARIMA model further revealed the seasonal characteristics of each energy source: wind energy exhibits strong seasonality and is highly predictable, while solar energy is affected by both seasonality and weather fluctuations; coal, although declining, still retains seasonal patterns; and natural gas shows a very pronounced peak in winter demand. These patterns collectively form the seasonal basis of modern energy supply.\n\n\n\n\n\nSubsequently, we used the ARIMAX model to explore the driving factors of solar power generation. The results showed that temperature and radiation have a significant impact on power generation, but extreme high temperatures inhibit efficiency; simultaneously, rising natural gas and oil prices increase the relative competitiveness of solar energy. This demonstrates that the development of renewable energy depends not only on climatic conditions but also on market incentives. In the financial dimension, the analysis of the clean energy ETF (ICLN) revealed a completely different dynamic: natural gas prices were the only consistently significant predictor variable, while oil and traditional energy indices (XLE) did not play a decisive role. This indicates that clean energy stocks have formed a financial ecosystem relatively independent of fossil fuels, exhibiting higher volatility and greater susceptibility to uncertainty.\n\n\n\n\n\n\n\n\n\n\nUsing the VAR model, we further revealed the interrelationships of multi-energy systems. The results showed that wind and solar energy often exhibit complementary or synchronous changes, while the relationship between fossil fuels and renewable energy is more like a substitution. The common upward trend of natural gas and renewable energy reflects the long-term effects of infrastructure expansion and market demand. This illustrates that modern energy systems do not operate in isolation but are a complex network of competition, interaction, and adjustment.\n\n\n\n\n\nIn the volatility analysis, the GARCH model showed significant volatility clustering in the clean energy financial market, with significant shocks during the 2008 financial crisis and the 2020 pandemic and oil price collapse. This result highlights that the volatility of the physical energy system is far weaker than that of the financial system, and clean energy investments carry greater uncertainty in the real market.\n\n\n\n\n\nTo improve forecasting performance, we introduced deep learning models. Across all models, the simple RNN delivers the best performance, achieving the lowest RMSE and the most stable forecasts. GRU performs moderately well, capturing seasonal timing but underestimating amplitude. LSTM performs the worst: its higher complexity leads to instability, overfitting, and rapidly diverging multi-step predictions. Regularization improves RNN and GRU stability but is insufficient for LSTM. Compared with traditional ARIMA models, deep learning—especially RNN—captures nonlinear seasonality more effectively. Overall, the RNN is the most appropriate model for this dataset, balancing accuracy, stability, and robustness for short- to medium-term energy forecasting.\n\n\n\n\n\nOverall, this project demonstrates that energy transition is a systemic process spanning physical, economic, and financial dimensions. Traditional models (such as SARIMA and ARIMAX) effectively explain trends and driving forces, VAR models help understand the structural relationships between different energy sources, and GARCH models reveal the dynamic changes in market risk. Each method has its strengths, and their combined use allows for the construction of an analytical framework that is both structurally rigorous and closely aligned with reality.\nBy combining insights from energy economics with time series modeling, this project provides important insights into understanding the evolution of energy systems. For policymakers, investors, and researchers alike, mastering this cross-system perspective is crucial for addressing future challenges related to energy structure adjustments, market volatility, and climate change."
  },
  {
    "objectID": "other.html",
    "href": "other.html",
    "title": "Interrupted TS Analysis",
    "section": "",
    "text": "Crude oil prices are highly sensitive to geopolitical events that disrupt global energy supply and alter market expectations. As a key benchmark for U.S. crude oil, West Texas Intermediate (WTI) reflects real-time assessments of global oil market conditions. Large-scale geopolitical conflicts, especially those involving major energy producers, can therefore generate abrupt and persistent changes in oil price dynamics.\nOn February 24, 2022, Russia launched a full-scale invasion of Ukraine, marking a major escalation in geopolitical tensions and triggering widespread concerns over energy supply disruptions. Russia is one of the world’s largest oil exporters, and the outbreak of the war led to immediate uncertainty regarding production, transportation, and sanctions affecting global oil markets. This event represents a clearly defined and exogenous shock to oil prices.\nThis analysis employs an Interrupted Time Series (ITS) framework to examine whether the outbreak of the Russia–Ukraine war resulted in a structural change in WTI oil prices. Specifically, the model tests for an immediate change in the price level following the invasion, as well as a change in the post-intervention price trend. By comparing price dynamics before and after February 24, 2022, the ITS approach provides a transparent and intuitive method for assessing how geopolitical shocks are incorporated into oil market prices."
  },
  {
    "objectID": "other.html#graph-to-identify-the-intervention",
    "href": "other.html#graph-to-identify-the-intervention",
    "title": "Interrupted TS Analysis",
    "section": "Graph to identify the intervention",
    "text": "Graph to identify the intervention\n\n\nCode\nlibrary(zoo)\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(dplyr)\n\nwti &lt;- read.csv(\"data/WTIprice.csv\")\nwti &lt;- wti %&gt;%\n  rename(date = observation_date) %&gt;%\n  mutate(date = as.Date(date)) %&gt;%\n  rename(WTI = POILWTIUSDM) %&gt;%\n  mutate(WTI = as.numeric(WTI))\n\n\n\n\nCode\n# ---- Intervention ----\nintervention_date &lt;- as.Date(\"2022-02-24\")\nmax_y &lt;- max(wti$WTI, na.rm = TRUE)\n\n# Find first point ON/AFTER intervention and one just BEFORE\nidx_after  &lt;- which(wti$date &gt;= intervention_date)[1]\nidx_before &lt;- idx_after - 1\n\ndrop_x   &lt;- wti$date[idx_after]\ny_before &lt;- wti$WTI[idx_before]\ny_after  &lt;- wti$WTI[idx_after]\n\n# ---- Base line plot ----\nfig &lt;- plot_ly(\n  wti,\n  x = ~date, y = ~WTI,\n  name = \"WTI Oil Price\",\n  type = \"scatter\", mode = \"lines\",\n  hovertemplate = paste(\n    \"Date: %{x|%b %Y}&lt;br&gt;\",\n    \"WTI Oil Price: %{y:.2f}%&lt;extra&gt;&lt;/extra&gt;\"\n  )\n)\n\n# ---- Add circle around the post-intervention point ----\nfig &lt;- fig %&gt;%\n  add_trace(\n    data = wti[idx_after, ],\n    x = ~date, y = ~WTI,\n    type = \"scatter\", mode = \"markers\",\n    marker = list(\n      size = 16,\n      color = \"rgba(0,0,0,0)\",           # transparent fill\n      line  = list(color = \"darkred\", width = 3)\n    ),\n    showlegend = FALSE,\n    hoverinfo = \"skip\"\n  )\n\n# ---- Add layout, vertical line, dashed drop, annotation ----\nfig &lt;- plotly::layout(\n  fig,\n  title = \"WTI Oil Price with Russia-Ukraine Conflict Intervention\",\n  xaxis = list(title = \"Date\"),\n  yaxis = list(title = \"WTI Oil Price\", rangemode = \"tozero\"),\n  shapes = list(\n      list(\n        type = \"line\",\n        x0 = intervention_date, x1 = intervention_date,\n        y0 = 0, y1 = max_y,\n        line = list(color = \"red\", width = 3),\n        xref = \"x\", yref = \"y\"\n      ),\n    # Dashed segment showing the jump\n    list(\n      type = \"line\",\n      x0 = drop_x, x1 = drop_x,\n      y0 = y_before, y1 = y_after,\n      line = list(color = \"green\", dash = \"dot\", width = 2),\n      xref = \"x\", yref = \"y\"\n    )\n  ),\n  annotations = list(\n    list(\n      x = intervention_date,\n      y = max_y * 0.95,\n      text = \"Russia-Ukraine Conflict\",\n      showarrow = TRUE,\n      arrowhead = 2,\n      ax = 0, ay = -40,\n      font = list(size = 12, color = \"red\")\n    )\n  ),\n  legend = list(orientation = \"h\", x = 0, y = -0.15)\n)\n\nfig"
  },
  {
    "objectID": "other.html#created-data-set-for-the-its-analysis",
    "href": "other.html#created-data-set-for-the-its-analysis",
    "title": "Interrupted TS Analysis",
    "section": "Created Data Set for the ITS analysis",
    "text": "Created Data Set for the ITS analysis\n\n\nCode\n# Load necessary packages\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Create time series data frame using preferred notations\ndataTS &lt;- data.frame(\n  Y  = wti$WTI,\n  Xt = seq_along(wti$WTI),\n  Zt = ifelse(wti$date &lt; intervention_date, 0, 1),\n  Pt = 0\n)\n\n# Assign values to Pt only after intervention\ndataTS$Pt[wti$date &gt;= intervention_date] &lt;- seq_len(sum(wti$date &gt;= intervention_date))\n\n# For reference, find index of intervention date (optional)\nintervention_index &lt;- which(wti$date == intervention_date)\n\n# Preview key rows: head, before and after intervention, and tail\nd.wti &lt;- rbind(\n  head(dataTS, 5),\n  c(\"...\", \"...\", \"...\", \"...\"),\n  dataTS[384:386, ],\n  c(\"**START**\", \"**Russia-Ukraine Conflict**\", \"---\", \"---\"),\n  dataTS[387:394, ],\n  c(\"...\", \"...\", \"...\", \"...\"),\n  tail(dataTS, 5)\n)\n\n# Display the preview table using kable\nkable(d.wti, caption = \"Preview of Time Series Data Using Notations Xt, Zt, and Pt\") %&gt;%\n  kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\nPreview of Time Series Data Using Notations Xt, Zt, and Pt\n\n\n\nY\nXt\nZt\nPt\n\n\n\n\n1\n22.604347826087\n1\n0\n0\n\n\n2\n22.2025\n2\n0\n0\n\n\n3\n20.4404545454545\n3\n0\n0\n\n\n4\n18.5266666666667\n4\n0\n0\n\n\n5\n18.4430434782609\n5\n0\n0\n\n\n6\n...\n...\n...\n...\n\n\n384\n71.8021739130435\n384\n0\n0\n\n\n385\n83.277619047619\n385\n0\n0\n\n\n386\n91.608\n386\n0\n0\n\n\n10\n**START**\n**Russia-Ukraine Conflict**\n---\n---\n\n\n387\n108.399130434783\n387\n1\n1\n\n\n388\n101.985238095238\n388\n1\n2\n\n\n389\n109.712727272727\n389\n1\n3\n\n\n390\n114.675909090909\n390\n1\n4\n\n\n391\n101.918095238095\n391\n1\n5\n\n\n392\n93.6926086956522\n392\n1\n6\n\n\n393\n84.4\n393\n1\n7\n\n\n394\n87.2866666666667\n394\n1\n8\n\n\n19\n...\n...\n...\n...\n\n\n422\n71.5415\n422\n1\n36\n\n\n423\n68.2390476190476\n423\n1\n37\n\n\n424\n63.6063636363636\n424\n1\n38\n\n\n425\n62.2004545454545\n425\n1\n39\n\n\n426\n68.5366666666667\n426\n1\n40\n\n\n\n\n\n\n\nCode\n# Fit the interrupted time series regression model\nmodel &lt;- lm(Y ~ Xt + Zt + Pt, data = dataTS)\n\n# Add fitted values to the dataset\ndataTS$Y_hat &lt;- predict(model)\n\n# Create interactive plot\nplot_ly(dataTS, x = ~Xt) %&gt;%\n  \n  # Add raw data points\n  add_markers(y = ~Y,\n              name = \"Observed\",\n              marker = list(color = 'gray', opacity = 0.6),\n              hoverinfo = 'text',\n              text = ~paste(\"Time:\", Xt, \"&lt;br&gt;Y:\", round(Y, 2))) %&gt;%\n  \n  # Add fitted regression line\n  add_lines(y = ~Y_hat,\n            name = \"Fitted Line\",\n            line = list(color = 'steelblue', width = 2)) %&gt;%\n  \n  # Add vertical line at T = 76 for COVID\n  add_segments(x = 386, xend = 386,\n               y = min(dataTS$Y), yend = max(dataTS$Y),\n               line = list(dash = \"dot\", color = \"firebrick\", width = 2),\n               name = \"Start of Russia-Ukraine Conflict\") %&gt;%\n  \n  # Layout styling\n  layout(\n    title = \"Interrupted Time Series: WTI Oil Price\",\n    xaxis = list(title = \"Time (months)\"),\n    yaxis = list(title = \"WTI Oil Price\"),\n    showlegend = TRUE,\n    annotations = list(\n      list(\n        x = 66,\n        y = max(dataTS$Y),\n        text = \"Start of Russia-Ukraine Conflict\",\n        showarrow = FALSE,\n        font = list(color = \"firebrick\", size = 12)\n      )\n    )\n  )"
  },
  {
    "objectID": "other.html#predicted-outcomes-and-their-counterfactuals",
    "href": "other.html#predicted-outcomes-and-their-counterfactuals",
    "title": "Interrupted TS Analysis",
    "section": "Predicted outcomes and their Counterfactuals",
    "text": "Predicted outcomes and their Counterfactuals\n\n\nCode\n# Load required libraries\nlibrary(broom)\n\n# Fit the interrupted time series model\nregTS &lt;- lm(Y ~ Xt + Zt + Pt, data = dataTS)\n\n# Tidy the model output\nsummary_table &lt;- tidy(regTS)\n\n# Add significance stars manually\nsummary_table &lt;- summary_table %&gt;%\n  mutate(\n    stars = case_when(\n      p.value &lt; 0.001 ~ \"***\",\n      p.value &lt; 0.01 ~ \"**\",\n      p.value &lt; 0.05 ~ \"*\",\n      p.value &lt; 0.1 ~ \".\",\n      TRUE ~ \"\"\n    ),\n    term = recode(term,\n                  \"Xt\" = \"Time\",\n                  \"Zt\" = \"Russia-Ukraine Conflict Indicator\",\n                  \"Pt\" = \"Time Since Russia-Ukraine Conflict\",\n                  \"(Intercept)\" = \"Constant\"),\n    `Estimate (SE)` = paste0(round(estimate, 2), stars, \"\\n(\", round(std.error, 2), \")\")\n  )\n\n# Select and print the desired columns\nsummary_table %&gt;%\n  select(term, `Estimate (SE)`) %&gt;%\n  kable(col.names = c(\" \", \"Estimate (SE)\"), align = \"l\", escape = FALSE) %&gt;%\n  kable_styling(full_width = FALSE)\n\n\n\n\n\n\nEstimate (SE)\n\n\n\n\nConstant\n15.24*** (2.11)\n\n\nTime\n0.17*** (0.01)\n\n\nRussia-Ukraine Conflict Indicator\n15.74* (6.98)\n\n\nTime Since Russia-Ukraine Conflict\n-0.97*** (0.28)"
  },
  {
    "objectID": "other.html#plotting-all-predicted-outcomes-and-their-counterfactuals",
    "href": "other.html#plotting-all-predicted-outcomes-and-their-counterfactuals",
    "title": "Interrupted TS Analysis",
    "section": "Plotting all predicted outcomes and their counterfactuals",
    "text": "Plotting all predicted outcomes and their counterfactuals\n\n\nCode\n# Load plotly\nlibrary(plotly)\n\n# Step 1: Get predictions\ndataTS$Y_hat &lt;- predict(regTS, newdata = dataTS)\n\n# Step 2: Create counterfactuals where Zt and Pt = 0\ndatanew &lt;- data.frame(\n  Xt = dataTS$Xt,\n  Zt = 0,\n  Pt = 0\n)\ndataTS$Y_counterfactual &lt;- predict(regTS, newdata = datanew)\n\n# Step 3: Create interactive plot\nfig &lt;- plot_ly()\n\n# Actual observed values\nfig &lt;- fig %&gt;%\n  add_markers(x = ~dataTS$Xt, y = ~dataTS$Y,\n              name = \"Observed Prices\",\n              marker = list(color = 'gray', opacity = 0.5))\n\n# Predicted line before and after COVID\nfig &lt;- fig %&gt;%\n  add_lines(x = ~dataTS$Xt[1:386], y = ~dataTS$Y_hat[1:386],\n            name = \"Predicted (Pre-Russia-Ukraine Conflict)\",\n            line = list(color = 'dodgerblue')) %&gt;%\n  add_lines(x = ~dataTS$Xt[387:nrow(dataTS)], y = ~dataTS$Y_hat[387:nrow(dataTS)],\n            name = \"Predicted (Post-Russia-Ukraine Conflict)\",\n            line = list(color = 'dodgerblue'))\n\n# Counterfactual line after COVID\nfig &lt;- fig %&gt;%\n  add_lines(x = ~dataTS$Xt[387:nrow(dataTS)], y = ~dataTS$Y_counterfactual[387:nrow(dataTS)],\n            name = \"Counterfactual (No Conflict)\",\n            line = list(color = 'darkorange', dash = 'dash'))\n\n# Vertical line for intervention\nfig &lt;- fig %&gt;%\n  add_lines(x = c(386, 386), y = c(min(dataTS$Y), max(dataTS$Y)),\n            name = \"Russia-Ukraine Conflict Intervention\",\n            line = list(color = 'firebrick', dash = 'dot'),\n            showlegend = TRUE)\n\n# Layout settings\nfig &lt;- fig %&gt;%\n  layout(title = \"WTI Oil Prices: Predicted vs. Counterfactual (Russia-Ukraine Conflict)\",\n         xaxis = list(title = \"Time (quarters)\"),\n         yaxis = list(title = \"WTI Oil Prices\"),\n         legend = list(x = 0.01, y = 0.99))\n\nfig"
  },
  {
    "objectID": "other.html#delayed-effects-of-the-russia-ukraine-conflict-intervention",
    "href": "other.html#delayed-effects-of-the-russia-ukraine-conflict-intervention",
    "title": "Interrupted TS Analysis",
    "section": "Delayed Effects of the Russia-Ukraine Conflict Intervention",
    "text": "Delayed Effects of the Russia-Ukraine Conflict Intervention\n\n\nCode\n# Predict from your actual model\npred1 &lt;- as.numeric(predict(regTS, newdata = dataTS))\n\ndatanew &lt;- data.frame(Xt = 1:nrow(dataTS), Zt = 0, Pt = 0)\npred2 &lt;- as.numeric(predict(regTS, newdata = datanew))\n\nT0 &lt;- 386   # shock starts\nT1 &lt;- 425   # delayed effect (optional)\n\n\ny_min &lt;- min(dataTS$Y, na.rm=TRUE)\ny_max &lt;- max(dataTS$Y, na.rm=TRUE)\nyr &lt;- y_max - y_min\npad &lt;- 0.12 * yr\n\npar(mar = c(5, 5, 4, 8), xpd = NA)\n\ncol_pts &lt;- adjustcolor(\"gray55\", alpha.f = 0.35)\ncol_pred &lt;- \"dodgerblue4\"\ncol_cf   &lt;- \"darkorange2\"\ncol_gap  &lt;- adjustcolor(col_cf, alpha.f = 0.12)\n\nplot(dataTS$Y,\n     col = col_pts, pch = 16, cex = 0.7,\n     xlim = c(1, nrow(dataTS)),\n     ylim = c(y_min - pad, y_max + pad),\n     xlab = \"Time (quarters)\", ylab = \"Prices\",\n     main = \"Interrupted Time Series: Impact of Russia–Ukraine Conflict\",\n     bty = \"l\")\n\n\nrect(T0, par(\"usr\")[3], nrow(dataTS), par(\"usr\")[4],\n     col = adjustcolor(\"gray95\", alpha.f = 0.8), border = NA)\n\n\nidx &lt;- T0:nrow(dataTS)\nx &lt;- idx\ny_lower &lt;- pmin(pred1[idx], pred2[idx])\ny_upper &lt;- pmax(pred1[idx], pred2[idx])\npolygon(c(x, rev(x)), c(y_lower, rev(y_upper)),\n        col = col_gap, border = NA)\n\n\nlines(1:nrow(dataTS), pred1, col = col_pred, lwd = 3)\nlines(T0:nrow(dataTS), pred2[T0:nrow(dataTS)], col = col_cf, lwd = 3, lty = 2)\n\n\nabline(v = T0, col = \"red3\", lty = 2, lwd = 2)\nabline(v = T1, col = \"forestgreen\", lty = 2, lwd = 2)\n\n\ntext(T0 + 2, y_max + 0.03*yr, \"Start of Conflict\", col = \"red3\", pos = 4)\ntext(T1 + 2, y_max - 0.05*yr, \"Delayed effect?\", col = \"forestgreen\", pos = 4)\n\nlegend(\"bottomright\", inset = 0.02,\n       legend = c(\"Observed\", \"Model prediction (with shock)\", \"Counterfactual (no shock)\", \"Estimated impact (gap)\"),\n       col    = c(\"gray55\", col_pred, col_cf, col_gap),\n       pch    = c(16, NA, NA, 15),\n       lty    = c(NA, 1, 2, NA),\n       lwd    = c(NA, 3, 3, NA),\n       pt.cex = c(0.7, NA, NA, 1.5),\n       bty = \"n\")"
  }
]